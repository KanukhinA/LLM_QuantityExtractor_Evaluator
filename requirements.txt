torch>=2.0.0
transformers>=4.50.0.dev0  # Требуется для поддержки Ministral-3 моделей с FP8 (рекомендуется установить из GitHub: pip install git+https://github.com/huggingface/transformers)
pandas>=1.5.0
openpyxl>=3.0.0
accelerate>=0.20.0
bitsandbytes>=0.41.0
google-genai>=0.2.0
huggingface-hub>=0.16.0
triton>=2.1.0  # Требуется для FP8 квантования (опционально, если недоступен - будет использована обычная загрузка)
langgraph>=0.2.0
outlines
mistral_common>=1.8.6
pyyaml>=6.0
gspread>=5.0.0
google-auth>=2.0.0

# Опционально: Flash Attention 2 для ускорения и экономии VRAM у локальных моделей.
# Установка: pip install flash-attn --no-build-isolation (нужны CUDA и совместимый компилятор; на Windows часто недоступен).
# flash-attn