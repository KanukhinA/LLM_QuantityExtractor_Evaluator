{
  "model_name": "gemma-3-27b-it",
  "timestamp": "20260124_174432",
  "analysis": "Привет! Как эксперт по оценке качества работы языковых моделей, я провел анализ результатов тестирования `gemma-3-27b-it` по задаче извлечения численных и количественных характеристик.\n\n**Резюме производительности:**\nМодель демонстрирует относительно хорошую производительность по извлечению \"массовой доли\", но значительно хуже справляется с блоком \"прочее\". Самая критичная проблема — это генерация невалидного JSON, что делает результат полностью непригодным для автоматического парсинга.\n\n---\n\n### 1. Характерные ошибки модели\n\n1.  **Невалидный JSON (критическая ошибка):** Самая серьезная и недопустимая ошибка. В примере модель оборвала вывод JSON на полпути (`{\"вещество\": \"Mg\", \"массовая доля`), что указывает на неполную генерацию ответа. Это означает, что даже если данные внутри частично верны, их невозможно использовать.\n2.  **Неполные ответы / Трассировка вывода:** Ошибка невалидного JSON почти наверняка вызвана обрывом генерации по лимиту токенов (`max_new_tokens`). Модель не успела достроить и закрыть все скобки JSON.\n3.  **Сниженная точность для \"прочих\" признаков:** Метрики показывают, что модель значительно хуже справляется с извлечением параметров, не относящихся к массовой доле (F1: 68.03% против 88.12%). Это может быть связано с:\n    *   **Сложностью правил:** Правила для \"прочего\" более разнообразны и требуют более глубокого контекстного понимания (например, различение \"масса нетто единицы\" от \"масса брутто\", вычисление \"1000 КГ+5%\").\n    *   **Разнообразие форматов:** \"Стандарт\" и \"марка\" имеют свои специфические форматы без единиц измерения, что может быть сложнее для модели, чем процентные доли.\n    *   **Пропуск или некорректная интерпретация правил:** Возможно, модель пропускает или неправильно применяет некоторые из подробных инструкций для \"прочего\".\n4.  **Сложность работы с длинными и плотными текстами:** Исходный текст для примера ошибки очень длинный и содержит множество похожих числовых данных, что может приводить к ошибкам в идентификации, отборе или связывании информации.\n\n---\n\n### 2. Причины ошибок парсинга JSON\n\nОсновной причиной ошибки парсинга JSON является **ограничение на максимальное количество генерируемых токенов (`max_new_tokens`)**.\n\n*   Промпт очень длинный (3768 символов). Это означает, что значительная часть контекстного окна модели уже занята инструкциями.\n*   Исходный текст для анализа в примере ошибки также очень длинный и насыщен информацией.\n*   Целевой JSON-вывод для такого сложного текста, содержащего множество параметров и массовых долей, может быть весьма объемным.\n\nС параметром `max_new_tokens: 512` модель, скорее всего, просто достигает этого лимита до того, как успевает полностью сформировать и закрыть JSON-объект. Это приводит к обрыву ответа на полуслове, делая JSON невалидным.\n\n---\n\n### 3. Причины ошибок в извлечении данных\n\n1.  **Лимит `max_new_tokens`:** Помимо обрыва JSON, недостаток токенов может приводить к тому, что модель не успевает извлечь *все* релевантные данные, особенно в конце длинного текста, или пропускает сложные конструкции.\n2.  **Длинный и детализированный промпт:** Хотя промпт содержит очень подробные инструкции, его длина (3768 символов) может приводить к тому, что модель \"забывает\" или уделяет меньше внимания инструкциям, которые находятся далеко от конца промпта или от места, где она должна применить эти инструкции. Особенно это касается сложных, многоуровневых правил, как в блоке \"прочее\".\n3.  **Сложность интерпретации контекста для \"прочих\" признаков:**\n    *   **Масса нетто vs брутто:** Правила типа \"МЕШКИ ПО 50 КГ\" -> \"масса нетто\" и \"масса с упаковкой\" -> \"масса брутто\" требуют тонкой интерпретации.\n    *   **Расчеты:** \"ПО 1000 КГ+5%\" требует арифметической операции.\n    *   **Приоритезация:** \"Если один и тот же параметр встречается дважды...\" требует логики выбора.\n    *   **Химические символы:** Замена названий элементов на символы (Кальций -> Ca) — это дополнительный шаг трансформации, который может быть источником ошибок.\n4.  **Разнообразие элементов в \"прочем\":** В отличие от относительно однородных \"массовых долей\" (вещество, доля), \"прочее\" включает в себя \"массу\", \"количество\", \"объем\", \"стандарт\", \"марку\", каждый со своими нюансами. Это увеличивает сложность задачи.\n\n---\n\n### 4. Рекомендации по улучшению промпта\n\nПромпт очень хорош в плане детализации и примеров, но его длина — это потенциальная проблема.\n\n1.  **Сокращение и Приоритизация:**\n    *   **Удалите избыточные примеры:** Если правило четко сформулировано, не всегда нужны многочисленные примеры его применения. Можно оставить 1-2 показательных.\n    *   **Сгруппируйте правила:** Постарайтесь сгруппировать связанные правила вместе и, возможно, использовать маркированные списки для повышения читаемости.\n    *   **Сформулируйте более лаконично:** Пересмотрите фразы, которые можно сократить без потери смысла.\n2.  **Подчеркните важность полного и валидного JSON:**\n    *   Перед блоком `4. Выводи строго JSON следующего вида...` добавьте очень сильное и четкое указание, например: \"**КРАЙНЕ ВАЖНО: Вывод должен быть ПОЛНОСТЬЮ валидным JSON-объектом, БЕЗ ОБРЫВОВ и с правильным закрытием всех скобок.**\".\n    *   Можно даже добавить пример, где показана закрывающая фигурная скобка `}`.\n3.  **Уточнение правил для \"прочего\":**\n    *   **Масса нетто/брутто:** Уточните различие, возможно, с более контрастными примерами.\n    *   **Расчеты:** Четко пропишите, что \"ПО 1000 КГ+5%\" означает расчет до \"1050\".\n    *   **Стандарт/Марка:** Подчеркните, что для этих признаков поле `единица` должно быть **отсутствующим** (не пустым, а именно отсутствующим), если это подразумевается (в примере JSON поля \"единица\" для \"стандарт\" и \"марка\" нет, что правильно, но в промпте написано \"или пусто для стандарта\" – это может быть двусмысленно). Лучше явно указать: \"для 'стандарт' и 'марка' единица измерения не указывается\".\n4.  **Размещение примера JSON:** Если модель не обучена на few-shot, то пример JSON в конце промпта (в пункте 4) может быть менее эффективным, чем если бы он был частью few-shot промпта с явным разделением `Prompt -> Text -> JSON_Example`. Для api_model это часто означает, что примеры должны быть в отдельных пользовательских/ассистентских сообщениях. Если это не few-shot, то пример хорошо иллюстрирует структуру, но занимает много места.\n\n---\n\n### 5. Рекомендации по настройке гиперпараметров\n\n1.  **`max_new_tokens`:**\n    *   **Это КЛЮЧЕВАЯ рекомендация.** Увеличьте этот параметр **значительно**. 512 токенов явно недостаточно.\n    *   Начните с `1024` или `2048`. Оцените типичную максимальную длину JSON-ответа для самых длинных входных текстов и установите `max_new_tokens` с запасом (например, +20-30% от максимальной). Это должно решить проблему с невалидным JSON из-за обрыва.\n2.  **`temperature`:**\n    *   Если модель делает мелкие синтаксические ошибки в JSON или \"фантазирует\" значения, попробуйте немного понизить `temperature` (например, до `0.1` или `0.3`). Это сделает ответы более детерминированными и менее склонными к ошибкам.\n    *   Если модель слишком консервативна и пропускает некоторые данные, можно попробовать немного повысить, но обычно для таких структурированных задач лучше низкая температура.\n3.  **Другие параметры (`top_p`, `top_k`):** Для данной задачи, как правило, меньше влияют на качество, чем `max_new_tokens` и `temperature`. Их можно подстраивать, если после изменения вышеуказанных параметров все еще есть проблемы с разнообразием или точностью.\n\n---\n\n### 6. Общие рекомендации по улучшению качества\n\n1.  **Пост-обработка и валидация:**\n    *   Внедрите строгую валидацию JSON на стороне клиента или в системе после получения ответа от модели. Если JSON невалиден, это сигнал к пересмотру промпта/гиперпараметров.\n    *   Для невалидных ответов можно попробовать *повторить запрос* к модели с добавлением инструкции \"Предыдущий ответ был невалидным JSON. Пожалуйста, убедитесь, что ваш ответ является полным и корректным JSON-объектом.\"\n2.  **Мониторинг длины вывода:** Отслеживайте фактическое количество сгенерированных токенов. Если оно часто приближается к `max_new_tokens`, это подтверждает, что лимит является проблемой.\n3.  **A/B тестирование промптов:** С такой сложной задачей и длинным промптом, стоит проводить итеративные эксперименты с различными версиями промпта, отслеживая метрики.\n4.  **Декомпозиция задачи (если предыдущие шаги не помогут):** Если \"прочее\" по-прежнему показывает низкие результаты, можно рассмотреть декомпозицию задачи на несколько этапов:\n    *   Этап 1: Извлечение всех числовых и количественных данных с их непосредственным контекстом.\n    *   Этап 2: Классификация этих данных (массовая доля, масса нетто, стандарт и т.д.).\n    *   Этап 3: Форматирование в целевой JSON.\n    Это может быть реализовано через несколько запросов к модели или через цепочку из модели и простых скриптов.\n\n**В заключение:** Основная проблема, судя по ошибке, кроется в ограничении `max_new_tokens`, которое прерывает генерацию ответа. Увеличение этого параметра должно быть первым и самым приоритетным шагом. После этого можно будет более предметно работать над улучшением точности извлечения данных для категории \"прочее\" путем доработки промпта.",
  "model_used": "gemini-2.5-flash",
  "parsing_errors_count": 1,
  "hyperparameters": {
    "max_new_tokens": 512,
    "model_name": "gemma-3-27b-it",
    "api_model": true
  },
  "system_info": {
    "api_model": true,
    "multi_agent_mode": null,
    "gpu_info": {
      "api": true
    },
    "gpu_memory_during_inference_gb": 0.0,
    "average_response_time_seconds": 8.700430445671081
  },
  "quality_metrics_summary": {
    "массовая доля": {
      "accuracy": 0.85244708994709,
      "precision": 0.8949416342412452,
      "recall": 0.8679245283018868,
      "f1": 0.8812260536398467
    },
    "прочее": {
      "accuracy": 0.5503737003737004,
      "precision": 0.7074468085106383,
      "recall": 0.6551724137931034,
      "f1": 0.6803069053708442
    }
  }
}