{
  "timestamp": "20251204_235850",
  "model_name": "mistralai/Ministral-3-3B-Reasoning-2512",
  "error": "Ошибка загрузки модели: No module named 'triton'",
  "error_traceback": "Traceback (most recent call last):\n  File \"C:\\D\\дзАрхив\\Магистратура\\Магистерская\\SmallLLMEvaluator\\model_evaluator.py\", line 146, in evaluate_model\n    model, tokenizer = load_model_func()\n                       ^^^^^^^^^^^^^^^^^\n  File \"C:\\D\\дзАрхив\\Магистратура\\Магистерская\\SmallLLMEvaluator\\model_loaders.py\", line 46, in load_ministral_3_3b_reasoning_2512\n    # Используем MistralCommonBackend для токенизатора\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 250, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3974, in from_pretrained\n    weight_conversions = get_model_conversion_mapping(model, key_mapping, hf_quantizer)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\conversion_mapping.py\", line 256, in get_model_conversion_mapping\n    weight_conversions.extend(hf_quantizer.get_weight_conversions())\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\quantizers\\quantizer_finegrained_fp8.py\", line 244, in get_weight_conversions\n    from ..integrations.finegrained_fp8 import Fp8Dequantize\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\integrations\\finegrained_fp8.py\", line 26, in <module>\n    import triton\nModuleNotFoundError: No module named 'triton'\n",
  "hyperparameters": {
    "max_new_tokens": 512,
    "do_sample": false,
    "torch_dtype": "bfloat16"
  }
}