{
  "model_name": "google/gemma-2-2b-it",
  "timestamp": "20260123_005803",
  "analysis": "## Отчет об оценке качества работы модели `google/gemma-2-2b-it`\n\n### Введение\nПроанализированы результаты тестирования модели `google/gemma-2-2b-it` на задаче извлечения численных и количественных характеристик из текстов о химических веществах и удобрениях. Модель использовалась с определенными гиперпараметрами и подробным промптом, требующим строгого JSON-формата вывода.\n\n### 1. Характерные ошибки модели\n\nОбщая картина производительности модели крайне низкая, о чем свидетельствуют метрики (F1-score 14.06% для \"массовой доли\" и 26.76% для \"прочего\"). Это указывает на фундаментальные проблемы в понимании задачи, следовании инструкциям и генерации требуемого формата.\n\nХарактерные ошибки включают:\n\n*   **Невалидный JSON / Обрыв генерации:** Наиболее частая и критическая проблема. Модель либо генерирует синтаксически некорректный JSON, либо обрывает вывод до его завершения. Это делает результаты непригодными для автоматической обработки.\n*   **Галлюцинации и ошибочные интерпретации:**\n    *   Приписывание некорректных химических элементов: Например, \"ВЛАГИ\" (влаги) интерпретируется как \"Na\" (натрий), \"НЕРАСТВ. В ВОДЕ ОСТАТКА\" (нерастворимый остаток) как \"Ca\" (кальций).\n    *   Создание несуществующих значений: Например, \"масса брутто\": 1050 кг, хотя в тексте нет явного источника для такого значения, и правило \"1000 КГ+5%\" не применимо.\n    *   Неправильное связывание чисел: Объединение процента и массы в один диапазон (`0.02 - 3000` для Ca).\n*   **Неспособность следовать сложным логическим правилам промпта:**\n    *   **Преобразование K в K2O:** Модель извлекает \"K\" вместо \"K2O\", несмотря на явное указание \"МАССОВАЯ ДОЛЯ K В ПЕРЕСЧЕТЕ НА К2О\" -> \"массовая доля K2O\".\n    *   **Различение \"массы нетто единицы\" и \"массы брутто\":** Модель ошибочно определяет \"масса нетто единицы\" как 30 кг (вес мешка), хотя по правилам промпта \"ПО 20 КГ\" должно быть массой нетто.\n    *   **Сложные расчеты:** Модель не справилась с интерпретацией \"1000 КГ+5%\".\n    *   **Семантика химических названий:** Неправильно интерпретирует \"АНТИСЛЕЖИВАТЕЛЯ (СУЛЬФОНАТА НАТРИЯ)\" просто как \"S\" (сера), игнорируя контекст.\n*   **Низкая полнота извлечения (Recall):** Метрики показывают, что модель пропускает значительную часть требуемых данных.\n*   **Низкая точность извлечения (Precision):** Когда модель что-то извлекает, это часто неверно или не соответствует заданным правилам.\n\n### 2. Причины ошибок парсинга JSON\n\nОсновными причинами ошибок парсинга JSON являются:\n\n*   **Ограниченные возможности модели (Model Capacity):** `gemma-2-2b-it` — это относительно небольшая модель (2 миллиарда параметров). Генерация длинных, сложных, вложенных JSON-структур с строгим соблюдением синтаксиса, типов данных и всех инструкций является очень сложной задачей для моделей такого размера. Они часто теряют \"нить\" форматирования или \"забывают\" о необходимости закрывать все скобки и кавычки, особенно при увеличении длины вывода.\n*   **Сложность JSON-структуры:** Требуемый JSON не является простым плоским объектом. Он содержит вложенные массивы объектов (`\"массовая доля\": [...]`), различные ключи для разных типов параметров (`\"масса\"`, `\"количество\"`, `\"значение\"`), что увеличивает когнитивную нагрузку на модель.\n*   **Длина промпта и текста:** Длинный и очень детализированный промпт (3768 символов) в сочетании с входным текстом для анализа увеличивает объем контекста. Небольшие модели могут испытывать трудности с поддержанием строгих инструкций, которые были в начале промпта, по мере генерации длинного ответа.\n*   **Строгое требование \"ОТВЕТ:\":** Если модель не начинает вывод строго со слова \"ОТВЕТ:\", за которым следует корректный JSON, или прерывает его, это сразу приводит к ошибке парсинга.\n\n### 3. Причины ошибок в извлечении данных\n\nПричины ошибок в извлечении данных тесно связаны с ограничениями модели и сложностью задачи:\n\n*   **Недостаточное понимание предметной области:** Модель не обладает глубоким знанием химии и удобрений, что приводит к неверной интерпретации терминов (например, \"влага\" как \"Na\", \"сульфонат натрия\" как \"S\").\n*   **Сложные и многослойные инструкции:** Промпт содержит множество нюансов и исключений (преобразование K в K2O, правила для \"массы нетто\" и \"массы брутто\", обработка диапазонов и операторов, расчеты типа \"1000 КГ+5%\"). Модель `gemma-2-2b-it` не справляется с таким объемом и сложностью логики. Она не может эффективно применять все эти правила одновременно и последовательно.\n*   **Отсутствие примеров (Few-shot learning):** Промпт является \"zero-shot\" (только инструкции). Для моделей меньшего размера и таких сложных задач добавление нескольких высококачественных примеров (текст + ожидаемый JSON) *внутри промпта* может значительно улучшить качество, демонстрируя модели, как именно применять правила и форматировать вывод.\n*   **Контекстная перегрузка:** Совокупность длинного промпта и текста для анализа заставляет модель \"забывать\" или неправильно применять детализированные инструкции, что приводит к пропускам или ошибочным извлечениям.\n\n### 4. Рекомендации по улучшению промпта\n\nПромпт очень детализирован и хорошо продуман, но для данной модели он слишком сложен.\n\n*   **Добавить Few-shot примеры:** Это, вероятно, самая эффективная мера. Включите 1-3 полных примера (текст для анализа и соответствующий ему **идеальный** JSON-вывод) прямо в промпт. Это поможет модели лучше понять требуемый формат и применение правил.\n*   **Упростить JSON-структуру (если возможно):** Рассмотрите возможность сделать JSON менее вложенным. Например, вместо `{\"массовая доля\": [...]}` и `{\"прочее\": [...]}` можно попробовать:\n    ```json\n    {\n      \"N_массовая_доля\": [26, 28],\n      \"P2O5_массовая_доля\": [20, null],\n      // ...\n      \"параметры_прочее\": [\n        {\"название\": \"масса нетто единицы\", \"значение\": 50, \"единица\": \"кг\"},\n        {\"название\": \"стандарт\", \"значение\": \"ТУ 2184-037-32496445-02\"}\n      ]\n    }\n    ```\n    Хотя текущая структура логична, она может быть слишком сложной для маленькой модели.\n*   **Разбить сложные инструкции на более мелкие/четкие пункты:** Например, правило про \"К в пересчете на К2О\" можно выделить и дать дополнительный пример. Правило про \"1000 КГ+5%\" также требует большей ясности или дополнительных примеров.\n*   **Явные негативные примеры:** Четко указать, чего *не* следует делать: \"Если массовая доля относится к 'влаге' или 'нерастворимому остатку', не связывайте это с химическим элементом.\"\n*   **Повторить важность JSON-формата и префикса \"ОТВЕТ:\":** Возможно, добавить в конец промпта еще раз инструкцию: \"ОБЯЗАТЕЛЬНО: Начинай вывод с 'ОТВЕТ:' и убедись, что JSON полностью валиден и закрыт.\"\n\n### 5. Рекомендации по настройке гиперпараметров\n\nТекущие гиперпараметры (`max_new_tokens`: 512, `do_sample`: false, `torch_dtype`: `bfloat16`) в целом подходят для задачи извлечения информации.\n\n*   **`max_new_tokens`:** 512 токенов может быть недостаточно, если генерируемый JSON очень большой. В примере #1 модель обрывает вывод. Рекомендуется увеличить до 768 или даже 1024, чтобы убедиться, что у модели достаточно \"пространства\" для полного и корректного завершения JSON-структуры.\n*   **`do_sample: false`:** Это правильный выбор для задач IE, поскольку обеспечивает детерминированный вывод (меньше \"креативности\", больше точности). Изменять не нужно.\n*   **`torch_dtype: bfloat16`:** Стандартно, изменять не требуется.\n*   **Модель:** **Главная рекомендация по гиперпараметрам — использовать значительно более мощную модель.** `gemma-2-2b-it` не справляется с данной задачей. Следует рассмотреть использование:\n    *   **Более крупные версии Gemma:** `gemma-2-9b-it`.\n    *   **Другие открытые модели:** `Llama 3 8B` или `70B`, `Mixtral 8x7B`.\n    *   **Проприетарные модели:** `GPT-3.5-Turbo` или `GPT-4` (если допустимо использование коммерческих решений), которые имеют значительно лучшие возможности по следованию инструкциям, генерации JSON и общему пониманию контекста.\n\n### 6. Общие рекомендации по улучшению качества\n\n*   **Смена модели:** Это наиболее критичный шаг. С текущей моделью `gemma-2-2b-it` добиться приемлемых результатов будет крайне сложно, если вообще возможно, даже с идеальным промптом.\n*   **Декомпозиция задачи:** Если использование небольшой модели является строгим требованием, рассмотрите возможность разбиения задачи на несколько этапов:\n    1.  **Извлечение сырых сущностей:** Модель извлекает все численные данные и их контекст в простой, менее структурированный формат (например, список пар \"ключ-значение\" без глубокой логики).\n    2.  **Постобработка/нормализация:** Отдельный скрипт (на Python, например) берет сырые извлеченные данные и применяет все сложные правила: преобразование K в K2O, расчеты \"1000 КГ+5%\", сопоставление с \"массой нетто/брутто\", валидацию и форматирование в окончательный JSON. Это позволяет использовать LLM для той части, где она сильна (гибкое извлечение), и правила/код для того, где нужна точность и строгая логика.\n*   **Fine-tuning (для малых моделей):** Если переход на более крупную модель невозможен, то для `gemma-2-2b` **обязательно потребуется дообучение (fine-tuning)** на большом и высококачественном датасете, содержащем примеры текстов и **корректных JSON-ответов**, соответствующих всем правилам промпта. Это позволит модели \"выучить\" конкретные правила извлечения и структуру JSON.\n*   **Валидация вывода:** Всегда выполняйте валидацию JSON-вывода. Если JSON невалиден, логируйте ошибку и, возможно, реализуйте механизм повторной попытки или перехвата. Также можно добавить семантическую валидацию (например, массовая доля должна быть от 0 до 100%).\n*   **Итеративный подход:** Постепенно усложняйте промпт, добавляя по одной инструкции или по одному типу правила за раз, и тестируйте модель на каждом шаге, чтобы выявить, на каком этапе возникают проблемы.\n\n### Заключение\n\nМодель `google/gemma-2-2b-it` продемонстрировала крайне низкую производительность и неспособность справиться с поставленной задачей извлечения информации с комплексными правилами и строгим JSON-форматом. Основные проблемы связаны с ее размером и ограничениями в понимании сложной логики, следовании многочисленным инструкциям и генерации синтаксически корректного, вложенного JSON.\n\n**Наиболее эффективные рекомендации:**\n1.  **Использовать более мощную языковую модель.**\n2.  **Добавить в промпт 1-3 высококачественных примера.**\n3.  Рассмотреть **декомпозицию задачи** на извлечение и последующую нормализацию/форматирование кодом, если смена модели невозможна.\n4.  Если использование малой модели является требованием, то **fine-tuning** с большим количеством размеченных данных будет необходим.",
  "model_used": "gemini-2.5-flash",
  "parsing_errors_count": 12,
  "hyperparameters": {
    "max_new_tokens": 512,
    "do_sample": false,
    "torch_dtype": "bfloat16"
  },
  "system_info": {
    "api_model": false,
    "multi_agent_mode": null,
    "gpu_info": {
      "cuda_available": true,
      "cuda_version": "12.1",
      "gpu_name": "NVIDIA GeForce RTX 4060 Laptop GPU",
      "gpu_memory_total_gb": 8.0,
      "gpu_memory_allocated_gb": 0.0,
      "gpu_memory_reserved_gb": 0.0,
      "gpu_name_detailed": "NVIDIA GeForce RTX 4060 Laptop GPU",
      "driver_version": "591.74"
    },
    "gpu_memory_during_inference_gb": 4.88,
    "average_response_time_seconds": 21.62690238714218
  },
  "quality_metrics_summary": {
    "массовая доля": {
      "accuracy": 0.07594633200693807,
      "precision": 0.09717868338557993,
      "recall": 0.2540983606557377,
      "f1": 0.1405895691609977
    },
    "прочее": {
      "accuracy": 0.1534798534798535,
      "precision": 0.18604651162790697,
      "recall": 0.47619047619047616,
      "f1": 0.26755852842809363
    }
  }
}