{
  "model_name": "gemma-3-27b-it",
  "timestamp": "20260121_221236",
  "analysis": "## Оценка работы модели gemma-3-27b-it и рекомендации по улучшению\n\n### Общий вывод\n\nМодель `gemma-3-27b-it` показывает хорошие результаты в извлечении \"Массовой доли\" (F1: 84.19%), что указывает на неплохое понимание этой категории данных и связанных с ней правил. Однако, производительность значительно падает для категории \"Прочее\" (F1: 59.95%). **Ключевой и критической проблемой, которая, вероятно, объясняет большинство ошибок, является обрезка промпта.** Модель получила только первые 2000 из 4115 символов промпта, что означает, что почти половина инструкций, включая важные уточнения и примеры, могла быть не видна модели. Отсутствие ошибок парсинга JSON является большим плюсом.\n\n---\n\n### 1. Характерные ошибки модели\n\n**По категории \"Массовая доля\":**\n*   **Высокий F1-score (84.19%)** говорит о том, что большинство случаев массовой доли извлекаются корректно. Ошибки, вероятно, носят более тонкий характер:\n    *   **Неточности в названии признака:** Возможно, не всегда строго соблюдается правило замены \"Кальций\" на \"Ca\", или \"K в пересчете на K2O\" на \"K2O\", если эти части промпта были обрезаны или интерпретированы неоднозначно.\n    *   **Некорректная обработка диапазонов/операторов:** Несмотря на явные инструкции, могли возникать ошибки в представлении [min, max], [min, null] или [null, max], особенно если эти правила находились за пределами 2000 символов.\n    *   **Пропуск сложных случаев:** Возможно, модель пропускает массовые доли, указанные в нестандартных форматах или требующие более глубокого контекстного анализа.\n\n**По категории \"Прочее\":**\n*   **Низкий F1-score (59.95%)** указывает на системные проблемы в этой категории.\n    *   **Низкий Recall (56.59%):** Модель часто пропускает \"прочие\" признаки. Это может быть связано с недостаточным пониманием определения этой категории, отсутствием достаточного количества примеров в видимой части промпта, или сложностью идентификации таких признаков в тексте. Вероятно, многие примеры и уточнения, связанные с \"прочими\" признаками (например, \"п/э мешки\", \"вагон\", \"стандарты (ТУ, ГОСТ)\"), были обрезаны.\n    *   **Более низкий Precision (63.74%) по сравнению с \"Массовой долей\":** Модель извлекает некоторые некорректные данные или ошибочно классифицирует их как \"прочие\" признаки. Это может происходить, если модель \"пытается\" найти численные данные, но не имеет достаточно четких правил или примеров для их правильной идентификации и извлечения в рамках этой широкой категории.\n    *   **Ошибки в форматировании/извлечении специфичных данных:** Проблемы с полным извлечением номеров стандартов (например, \"ГОСТ 123.456-78\") или идентификаторов, а также с нестандартными единицами измерения.\n\n---\n\n### 2. Причины ошибок парсинга JSON\n\n*   **Ошибок парсинга JSON не обнаружено.** Это отличный результат. Модель успешно соблюдает заданный формат вывода, что упрощает дальнейшую автоматизированную обработку данных.\n\n---\n\n### 3. Причины ошибок в извлечении данных\n\n1.  **Обрезка промпта (ключевая причина):** Модель получила только часть инструкций (2000 из 4115 символов). Это означает, что:\n    *   Многие детали, уточнения и примеры, особенно касающиеся категории \"Прочее\", могли быть недоступны модели.\n    *   Правила обработки сложных случаев (диапазоны, логические операторы, специфические правила именования) могли быть неполными или вовсе отсутствовать в видимой части промпта.\n    *   Заключительные и, возможно, суммирующие инструкции также были упущены.\n2.  **Широкое определение категории \"Прочее\":** Даже если бы промпт был полон, категория \"Прочее\" по своей природе сложнее для модели, так как она охватывает множество разнообразных численных характеристик. Требуется больше четких примеров и, возможно, более строгие правила для идентификации.\n3.  **Сложность правил извлечения:** Некоторые правила (например, приоритет данных из марки, преобразование K в K2O, обработка операторов \"не менее/не более\") требуют глубокого понимания контекста и строгой логики, что является вызовом для любой языковой модели без специализированного обучения.\n4.  **Ограничения модели:** Несмотря на размер, `gemma-3-27b-it` может испытывать трудности с точным извлечением высокоструктурированной, домен-специфичной информации из неструктурированного текста без дополнительного обучения на подобных задачах.\n\n---\n\n### 4. Рекомендации по улучшению промпта\n\n1.  **ОБЕСПЕЧИТЬ ПОЛНУЮ ПЕРЕДАЧУ ПРОМПТА:** Это самый важный шаг. Модель должна видеть все инструкции без исключения. Убедитесь, что нет технических ограничений на длину промпта при его передаче модели.\n2.  **Оптимизация длины промпта (если есть ограничения):** Если ограничение на длину промпта неизбежно, необходимо:\n    *   **Приоритезировать информацию:** Самые важные и критичные инструкции (особенно по основным категориям и формату вывода) должны быть в начале.\n    *   **Сократить избыточность:** Проверить, нет ли повторяющихся формулировок или менее важных деталей, которые можно убрать или обобщить.\n    *   **Конкретные примеры:** Вместо длинных описаний, иногда 1-2 хорошо подобранных примера \"входной текст -> ожидаемый JSON\" могут передать больше информации.\n3.  **Усиление инструкций для категории \"Прочее\":**\n    *   **Больше конкретных примеров:** Включить больше разнообразных примеров \"прочих\" признаков: `количество вагонов`, `количество п/э мешков`, `ТУ ХХХ-ХХХ`, `ГОСТ YYYY`, `серийный номер ZZZ`.\n    *   **Четкое определение границ:** По возможности, явно указать, что *не* следует извлекать, чтобы уменьшить ложноположительные срабатывания.\n    *   **Структурирование:** Разбить \"прочие\" признаки на подкатегории (например, \"Количество товара\", \"Стандарты\", \"Идентификаторы\") с отдельными инструкциями для каждой, если это не приведет к чрезмерному раздуванию промпта.\n4.  **Четкость формулировок:**\n    *   Убедиться, что все правила (особенно про диапазоны, `null` значения, преобразования `K` в `K2O`, приоритеты) сформулированы максимально однозначно.\n    *   Использовать жирный шрифт или выделение для ключевых терминов и правил.\n5.  **Few-shot Learning (если возможно по токенам):** Добавление 1-2 полных примера \"текст + ожидаемый JSON-вывод\" в промпт. Это значительно улучшает способность модели понимать желаемый формат и логику извлечения.\n\n---\n\n### 5. Рекомендации по настройке гиперпараметров\n\n*   **`max_new_tokens` (512):** Если модель не обрезает свой *собственный* вывод (а ошибок парсинга нет, что подтверждает это), то 512 токенов, вероятно, достаточно для вывода. Нет необходимости увеличивать, если это не приводит к усечению JSON-ответа.\n*   **`temperature` (не указано):** Для задач извлечения информации рекомендуется использовать низкое значение `temperature` (например, 0.1-0.3). Это делает генерацию модели более детерминированной и менее креативной, что критично для строгого соблюдения правил извлечения и формата JSON.\n*   **`top_p`, `top_k` (не указано):** Аналогично `temperature`, для задач извлечения рекомендуется использовать более консервативные значения, которые ограничивают разнообразие генерируемых токенов, способствуя точности и следованию инструкциям.\n\n---\n\n### 6. Общие рекомендации по улучшению качества\n\n1.  **Приоритет: Исправить проблему с обрезкой промпта.** Пока модель не видит полный промпт, любые другие улучшения будут неэффективны.\n2.  **Детальный анализ ошибок (ручной):** После исправления проблемы с промптом и, возможно, его доработки, проведите ручной анализ выборки ошибок для обеих категорий.\n    *   Для \"Массовой доли\": какие конкретно правила нарушаются? (Название, единица, диапазон?)\n    *   Для \"Прочее\": какие признаки чаще всего пропускаются (Recall)? Какие извлекаются ошибочно (Precision)?\n    Этот анализ поможет точнее настроить промпт.\n3.  **Использование нескольких примеров (Few-shot learning):** Включение 1-3 полных примеров (текст-запрос и желаемый JSON-ответ) в промпт является одним из самых эффективных способов улучшения качества для сложных задач извлечения.\n4.  **Пост-обработка (Post-processing):** Разработка скриптов для автоматической коррекции типичных ошибок после получения JSON-вывода от модели. Например, нормализация единиц измерения, проверка на валидность числовых диапазонов, приведение названий к единому виду.\n5.  **Дообучение (Fine-tuning) модели (долгосрочная перспектива):** Если задача критически важна и есть большой объем размеченных данных, специализированное дообучение модели на вашей задаче извлечения химических характеристик даст наилучшие результаты, так как модель адаптируется к специфике вашего домена и формату данных.\n6.  **Итеративный подход:** Процесс улучшения LLM — это итеративный цикл: изменение промпта -> тестирование -> анализ результатов -> повторение.",
  "model_used": "gemini-2.5-flash",
  "parsing_errors_count": 0,
  "hyperparameters": {
    "max_new_tokens": 512,
    "model_name": "gemma-3-27b-it",
    "api_model": true
  },
  "system_info": {
    "api_model": true,
    "multi_agent_mode": null,
    "gpu_info": {
      "api": true
    },
    "gpu_memory_during_inference_gb": 0.0,
    "average_response_time_seconds": 15.737084214687348
  },
  "quality_metrics_summary": {
    "массовая доля": {
      "accuracy": 0.8026334776334776,
      "precision": 0.8403041825095057,
      "recall": 0.8435114503816794,
      "f1": 0.8419047619047619
    },
    "прочее": {
      "accuracy": 0.48003589237766453,
      "precision": 0.6373626373626373,
      "recall": 0.5658536585365853,
      "f1": 0.5994832041343668
    }
  }
}