{
  "model_name": "Qwen/Qwen2.5-1.5B-Instruct",
  "timestamp": "20251204_011804",
  "analysis": "## Экспертная оценка качества работы модели Qwen/Qwen2.5-1.5B-Instruct\n\n**Общий вывод:**\nМодель Qwen/Qwen2.5-1.5B-Instruct демонстрирует крайне низкую производительность в задаче извлечения численных характеристик из текстов о химических веществах и удобрениях. Основные проблемы связаны с неспособностью следовать инструкциям по формату вывода (JSON), частыми галлюцинациями, а также пропусками релевантной информации. Метрики Precision и Recall находятся на недопустимо низком уровне для данной задачи.\n\n---\n\n### 1. Характерные ошибки модели\n\n*   **Полное игнорирование формата JSON:** Это самая критичная ошибка. В большинстве случаев модель либо выдает произвольный текст, либо генерирует Python-код, вместо запрошенного JSON. Это делает ответы непарсибельными и непригодными для автоматизированной обработки.\n*   **Галлюцинации и избыточное извлечение (низкая Precision):** Примеры ошибок качества показывают, что модель часто \"придумывает\" численные значения или сущности, которых нет в исходном тексте (например, \"Вещество Fe: предсказано [5.0, None], истина отсутствует\"). Это резко снижает точность извлечения.\n*   **Недостаточное извлечение (низкий Recall):** Низкие показатели Recall говорят о том, что модель пропускает значительную часть действительно присутствующих в тексте численных характеристик.\n*   **Несоблюдение специфических правил извлечения:** Модель, вероятно, не справляется с точными инструкциями промпта, такими как обработка диапазонов, операторов \"не менее\"/\"не более\", или заменой \"Кальций\" на \"Ca\", \"K в пересчете на K2O\" на \"K2O\".\n*   **Добавление постороннего текста:** Даже когда модель пытается сгенерировать JSON, она часто предваряет его фразами типа \"Ответ:\" или заключает в блоки кода (```json...```, ```python...```), что может помешать парсингу, если парсер ожидает чистый JSON-объект.\n\n### 2. Причины ошибок парсинга JSON\n\nОсновные причины ошибок парсинга JSON напрямую связаны с характером ошибок модели:\n\n*   **Негенерация JSON:** Самая частая причина. Модель просто не выдает JSON-формат, вместо этого генерируя обычный текст, который, естественно, не может быть спарсен как JSON (примеры #92, #78, #61, #32).\n*   **Генерация кода:** В некоторых случаях модель выдает фрагменты кода (например, Python-функцию для извлечения признаков), что является полным отклонением от инструкций.\n*   **Некорректное обрамление JSON:** Даже когда модель пытается выдать JSON, она может добавлять лишние символы (например, \"Ответ:\\n\\n```json\"), или не закрывать блок (обрезанные ответы с `...`), что делает JSON невалидным или требующим дополнительной пост-обработки.\n*   **Обрезанный вывод:** Примеры показывают, что JSON-вывод может быть неполным, что указывает либо на ограничение `max_new_tokens`, либо на внутреннюю ошибку модели, прекращающей генерацию.\n\n### 3. Причины ошибок в извлечении данных\n\n*   **Размер и мощность модели:** Qwen/Qwen2.5-1.5B-Instruct — это относительно небольшая модель (1.5 миллиарда параметров). Для такой сложной и детализированной задачи, требующей строгого следования множеству инструкций, генерации валидного структурированного вывода и избегания галлюцинаций, 1.5B часто недостаточно. Большие модели значительно лучше справляются с \"instruction following\" и \"JSON schema adherence\".\n*   **Сложность и объем промпта:** Промпт очень детализированный и длинный (более 3.7 тысяч символов). Для маленькой модели удержание в контексте и точное применение всех этих правил одновременно является сложной задачей. Модель может \"забывать\" или путать часть инструкций.\n*   **Отсутствие Few-Shot примеров:** Промпт является \"zero-shot\" инструкцией. Для задач извлечения информации с комплексным структурированным выводом (JSON) и множеством специфичных правил, добавление 1-3 качественных примеров \"вход-выход\" (текст и ожидаемый JSON) в промпт значительно улучшает производительность модели, особенно небольших размеров.\n*   **Неявные инструкции \"не галлюцинировать\":** Промпт явно не запрещает модели извлекать то, чего нет. LLM по своей природе могут \"додумывать\" информацию, если они не видят явного запрета или четких границ.\n\n### 4. Рекомендации по улучшению промпта\n\nУчитывая, что промпт достаточно детализирован, но модель не следует ему, основные изменения должны быть направлены на повышение его эффективности для модели:\n\n*   **Добавить Few-Shot примеры:** Это критически важно. Включите 1-3 полных примера в промпт, демонстрирующих, как должен выглядеть текст и соответствующий ему JSON-вывод. Примеры должны охватывать:\n    *   Успешное извлечение нескольких элементов.\n    *   Извлечение диапазона.\n    *   Случаи, когда некоторые данные отсутствуют (чтобы показать, что их не нужно включать).\n    *   Пример с конвертацией K в K2O.\n    *   Обязательно заключайте примеры JSON в ````json`...```` блоки.\n*   **Усилить акцент на формате JSON:**\n    *   Сделать инструкцию по JSON самой первой или выделить ее особо.\n    *   Добавить фразу вроде: \"**Твой ответ ДОЛЖЕН БЫТЬ ТОЛЬКО в формате JSON, заключенным в блок ```json...```.** Никакого другого текста, пояснений, кода или приветствий быть не должно.\"\n*   **Явно указать на запрет галлюцинаций:** Добавить пункт в \"Особые указания\":\n    *   \"**НЕ ИЗВЛЕКАЙ и НЕ ГЕНЕРИРУЙ численные характеристики, которых нет в исходном тексте.** Если информация отсутствует, не включай соответствующий признак в JSON.\"\n*   **Уточнить обработку \"не менее/не более\":** В промпте сказано \"не менее – [min, null]\", \"не более – [null, max]\". Убедитесь, что `null` является ожидаемым строковым значением или если ожидается `None` в Python, это также явно указано.\n*   **Проверить возможные сокращения промпта:** Хотя указано \"первые 2000 символов из 3763\", убедитесь, что вся полнота промпта подается модели. Если же часть промпта действительно обрезалась, это могло быть причиной значительного количества ошибок.\n\n### 5. Рекомендации по настройке гиперпараметров\n\nГиперпараметры в текущей ситуации являются вторичными, так как модель не следует базовым инструкциям. Однако, если после улучшения промпта все еще будут проблемы:\n\n*   **`max_new_tokens`:** Если JSON-ответы регулярно обрезаются (как видно по `...` в примерах), увеличьте `max_new_tokens` до 1024 или 2048, чтобы дать модели достаточно пространства для полного ответа.\n*   **`do_sample`:** Оставляем `false`. Для извлечения информации важна детерминированность.\n*   **`temperature`:** Неприменимо, так как `do_sample=false`.\n*   **`repetition_penalty`:** Если модель демонстрирует чрезмерные повторы в тексте, можно попробовать немного увеличить (например, до 1.1-1.2), но это маловероятно для данной задачи.\n\n### 6. Общие рекомендации по улучшению качества\n\n*   **Использовать более мощную модель:** Это наиболее эффективный и быстрый путь к значительному улучшению. Модель Qwen/Qwen2.5-1.5B-Instruct слишком мала для этой задачи. Рекомендуется попробовать:\n    *   **Qwen2.5-7B-Instruct**\n    *   **Qwen2.5-14B-Instruct**\n    *   В идеале, если позволяют ресурсы, рассмотреть **Qwen2.5-72B-Instruct**. Большие модели значительно лучше справляются с комплексным instruction following, генерацией валидного JSON и сокращением галлюцинаций.\n*   **Тонкая настройка (Fine-tuning):** Если даже более крупные модели не дают идеального результата, или задача очень специфична, тонкая настройка модели на вашем собственном наборе данных (текст + правильный JSON-вывод) будет самым эффективным методом. Это позволит модели \"научиться\" специфике ваших текстов и формату вывода.\n*   **Пост-обработка ответов:** Внедрить скрипт для пост-обработки, который:\n    *   Пытается очистить вывод от лишних префиксов (\"Ответ:\", ````json````) перед парсингом JSON.\n    *   Проверяет валидность JSON и, возможно, пытается исправить незначительные ошибки (например, незакрытые скобки, если JSON был обрезан).\n    *   Фильтрует извлеченные данные, чтобы убрать очевидные галлюцинации (если возможно определить их по каким-либо правилам).\n*   **Итеративный подход:** Начните с изменения промпта (добавление примеров, усиление инструкций по JSON и запрету галлюцинаций), затем протестируйте с более мощной моделью. Если этого недостаточно, рассмотрите тонкую настройку.",
  "model_used": "gemini-2.5-flash",
  "parsing_errors_count": 42,
  "quality_metrics_summary": {
    "массовая доля": {
      "precision": 0.027522935779816515,
      "recall": 0.02608695652173913,
      "f1": 0.02678571428571428
    },
    "прочее": {
      "precision": 0.09717868338557993,
      "recall": 0.17222222222222222,
      "f1": 0.12424849699398798
    }
  }
}