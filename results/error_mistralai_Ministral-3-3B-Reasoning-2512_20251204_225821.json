{
  "timestamp": "20251204_225821",
  "model_name": "mistralai/Ministral-3-3B-Reasoning-2512",
  "error": "Ошибка загрузки модели: 401 Client Error. (Request ID: Root=1-6931e7db-79d724377d8e9d5e3df79e1f;7e01afaf-6dad-4a4c-b379-862d38604c36)\n\nRepository Not Found for url: https://huggingface.co/api/models/mistralai/Ministral-3-3B-Reasoning-2512.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nUser Access Token \"RUDN QA\" is expired",
  "error_traceback": "Traceback (most recent call last):\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 402, in hf_raise_for_status\n    response.raise_for_status()\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/mistralai/Ministral-3-3B-Reasoning-2512\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\D\\дзАрхив\\Магистратура\\Магистерская\\SmallLLMEvaluator\\model_evaluator.py\", line 146, in evaluate_model\n    model, tokenizer = load_model_func()\n                       ^^^^^^^^^^^^^^^^^\n  File \"C:\\D\\дзАрхив\\Магистратура\\Магистерская\\SmallLLMEvaluator\\model_loaders.py\", line 33, in load_ministral_3_3b_reasoning_2512\n    tokenizer = AutoTokenizer.from_pretrained(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\", line 1156, in from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2113, in from_pretrained\n    return cls._from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2395, in _from_pretrained\n    tokenizer = cls._patch_mistral_regex(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2438, in _patch_mistral_regex\n    if _is_local or is_base_mistral(pretrained_model_name_or_path):\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2432, in is_base_mistral\n    model = model_info(model_id)\n            ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\huggingface_hub\\hf_api.py\", line 2661, in model_info\n    hf_raise_for_status(r)\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py\", line 452, in hf_raise_for_status\n    raise _format(RepositoryNotFoundError, message, response) from e\nhuggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-6931e7db-79d724377d8e9d5e3df79e1f;7e01afaf-6dad-4a4c-b379-862d38604c36)\n\nRepository Not Found for url: https://huggingface.co/api/models/mistralai/Ministral-3-3B-Reasoning-2512.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nUser Access Token \"RUDN QA\" is expired\n",
  "hyperparameters": {
    "max_new_tokens": 512,
    "do_sample": false,
    "torch_dtype": "bfloat16"
  }
}