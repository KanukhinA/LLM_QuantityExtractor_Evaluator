{
  "model_name": "gemma-3-12b-it",
  "timestamp": "20260123_005317",
  "analysis": "Как эксперт по оценке качества работы языковых моделей, я проанализировал предоставленные результаты тестирования модели `gemma-3-12b-it` для задачи извлечения численных и количественных характеристик.\n\n---\n\n### Анализ результатов тестирования `gemma-3-12b-it`\n\n#### 1. Характерные ошибки модели\n\nНаиболее критичная и очевидная ошибка, продемонстрированная в примере, — **неспособность сгенерировать валидный JSON-объект**. Модель выдает лишь начальную часть JSON-строки (`{\"веще`) и обрезает вывод. Это делает весь результат непригодным для автоматического парсинга и использования.\n\nПомимо этой критической ошибки, метрики качества указывают на следующие проблемы:\n\n*   **Низкая полнота (Recall):**\n    *   `массовая доля`: Recall 72.04%\n    *   `прочее`: Recall 62.38%\n    Это означает, что модель пропускает значительную часть численных характеристик, которые должны быть извлечены из текста. Она не находит все необходимые данные.\n*   **Средняя точность (Precision):**\n    *   `массовая доля`: Precision 57.58%\n    *   `прочее`: Precision 70.00%\n    Когда модель извлекает данные, она делает это правильно примерно в 60-70% случаев. Это означает, что есть существенный процент некорректно извлеченных, классифицированных или отформатированных данных.\n*   **Общий низкий F1-показатель (около 64-66%):** Указывает на то, что модель нуждается в значительном улучшении как в части обнаружения, так и в части правильного извлечения и форматирования данных.\n\n#### 2. Причины ошибок парсинга JSON\n\nОсновной причиной ошибки \"Невалидный JSON. Ответ: {\"веще\" является, с высокой степенью вероятности, **преждевременное обрезание ответа модели из-за достижения лимита `max_new_tokens`**.\n\n*   Модель начинает генерировать JSON-структуру (что видно по `{\"веще`), но не успевает ее завершить до того, как будет достигнут `max_new_tokens: 512`.\n*   Предоставленный промпт очень детализирован и, вместе с анализируемым текстом, занимает значительное количество токенов во входном контексте. При этом сам JSON-выход для сложного текста может быть довольно объемным. Для примера ошибки (текст #95), который содержит множество массовых долей и несколько жидких удобрений с разными параметрами, 512 новых токенов явно недостаточно для генерации полного и корректного JSON.\n*   Дополнительная причина: Модель также игнорирует инструкцию \"Выводи json результат **только после слова ОТВЕТ:**\". Она начинает генерацию JSON напрямую, что может быть связано как с недостатком токенов, так и с общим нестрогим следованием инструкциям.\n\n#### 3. Причины ошибок в извлечении данных\n\n*   **Сложность и объем промпта:** Промпт очень подробный, содержит множество правил, исключений и специфических инструкций по форматированию (диапазоны, операторы, пересчеты, именование признаков, приоритеты). Для модели `gemma-3-12b-it` (даже в режиме `it` - instruct-tuned) такое количество нюансов может быть сложным для одновременного отслеживания и применения ко всему тексту.\n*   **Длинный входной текст:** Пример текста для анализа (особенно \"Пример 1\") очень длинный и содержит множество разных сущностей и числовых значений. Модели могут \"терять\" контекст или испытывать трудности с обработкой длинных последовательностей, особенно если нужные данные находятся в середине или конце текста.\n*   **Нюансы форматирования и именования:**\n    *   Преобразование названий элементов (`Калий` -> `K`, `Магний` -> `Mg`).\n    *   Специфические правила для массовых долей (`K в пересчете на К2О` -> `массовая доля K2O`).\n    *   Различение `массы нетто` и `массы брутто` в зависимости от контекста (`МЕШКИ ПО 50 КГ` vs `МАССОЙ БРУТТО ДО 10КГ`).\n    *   Обработка диапазонов и логических операторов (`не менее`, `не более`).\n    Эти инструкции требуют глубокого понимания семантики и контекста, что может быть сложным для LLM без дополнительного обучения на подобных задачах.\n*   **Приоритет конкретных значений над общими условиями:** Правило \"Если один и тот же параметр встречается дважды... указывай только конкретное значение\" требует сложной логики обнаружения дубликатов и выбора приоритетного значения.\n\n#### 4. Рекомендации по улучшению промпта\n\n1.  **Увеличить количество Few-Shot примеров:** Добавить 1-3 полных примера (текст для анализа + ожидаемый JSON-вывод), охватывающих различные типы извлечений (массовые доли, диапазоны, стандарты, массы), непосредственно в промпт. Это значительно улучшит способность модели следовать инструкциям и формату.\n2.  **Усилить инструкцию по JSON-формату:** Несмотря на то, что она уже строгая, можно попробовать:\n    *   Переместить блок с примером JSON выше в промпте, чтобы модель видела его раньше.\n    *   После инструкции \"Выводи json результат **только после слова ОТВЕТ:**\" можно добавить \"И НИЧЕГО БОЛЕЕ. JSON должен быть полностью валидным.\"\n3.  **Разбить сложные правила на более мелкие шаги (для повышения Clarity):** Для особо сложных инструкций (например, про \"массу нетто/брутто\" или \"K в пересчете на K2O\") можно добавить мини-примеры или уточнения прямо в тексте промпта, иллюстрирующие эти правила.\n4.  **Подумать о \"Chain-of-Thought\" (CoT) промптинге:** Для очень сложной логики можно попробовать добавить в промпт запрос на пошаговое рассуждение, хотя это может увеличить объем вывода и, следовательно, расход токенов. Например: \"Сначала определи все числовые характеристики. Затем для каждой характеристики определи ее название, значение и единицу измерения, следуя всем правилам ниже. В конце сформируй JSON.\" (Это может быть излишним, если будутFew-Shot примеры).\n\n#### 5. Рекомендации по настройке гиперпараметров\n\n1.  **`max_new_tokens`:** **Это критически важно.** Увеличьте этот параметр **значительно**. Для текста, который может генерировать большой JSON, 512 токенов явно недостаточно. Рекомендуется начать с 1024 или даже 2048, чтобы исключить обрезание ответа как причину невалидного JSON.\n2.  **`temperature`:** Установите более низкое значение (например, 0.1-0.3). Это сделает модель более детерминированной и сфокусированной на строгом следовании инструкциям, уменьшая вероятность \"галлюцинаций\" или отступлений от заданного формата.\n3.  **`top_p`, `top_k` (если доступны):** Также рекомендуется настроить их таким образом, чтобы уменьшить случайность выборки токенов, что способствует более стабильному и предсказуемому выводу, особенно для структурированных данных.\n\n#### 6. Общие рекомендации по улучшению качества\n\n1.  **Создание качественного датасета для дообучения (Fine-tuning):** Для такой специфичной и сложной задачи с жесткими требованиями к формату вывода, **самым эффективным методом улучшения будет дообучение (fine-tuning) модели** на большом количестве размеченных данных. Это позволит модели усвоить конкретные паттерны извлечения и форматирования, а также специфическую терминологию.\n2.  **Итеративная разработка промпта:** Вносите изменения в промпт по одному и тестируйте их влияние. Это поможет понять, какие инструкции наиболее важны и как модель на них реагирует.\n3.  **Предварительная обработка текста:** Если тексты содержат много шума, опечаток или нестандартного форматирования, рассмотрите возможность их предварительной очистки перед подачей в модель.\n4.  **Постобработка и валидация:** Внедрите слой постобработки, который будет валидировать полученный JSON. Если JSON невалиден, попытайтесь автоматически исправить распространенные ошибки (например, добавить закрывающие скобки, если ответ обрезан) или сигнализировать о необходимости ручной проверки. Это сделает систему более устойчивой, даже если модель иногда ошибается.\n5.  **Разбиение задачи (если возможно):** Для очень длинных текстов, содержащих множество независимых блоков информации, можно рассмотреть возможность разбиения исходного текста на более мелкие фрагменты и их последовательную обработку, а затем агрегацию результатов. Однако, это может усложнить обработку правил, требующих глобального контекста (например, \"указывать только конкретное значение\").\n\n---\n\n**Вывод:**\nМодель `gemma-3-12b-it` демонстрирует потенциал, но сталкивается с серьезными проблемами в следовании сложным многосоставным инструкциям и, в особенности, в соблюдении жесткого формата вывода JSON. Устранение ошибки обрезания вывода путем увеличения `max_new_tokens` является первоочередной задачей. В долгосрочной перспективе, для достижения высоких метрик качества, необходимо инвестировать в дообучение модели на специфическом датасете.",
  "model_used": "gemini-2.5-flash",
  "parsing_errors_count": 1,
  "hyperparameters": {
    "max_new_tokens": 512,
    "model_name": "gemma-3-12b-it",
    "api_model": true
  },
  "system_info": {
    "api_model": true,
    "multi_agent_mode": null,
    "gpu_info": {
      "api": true
    },
    "gpu_memory_during_inference_gb": 0.0,
    "average_response_time_seconds": 10.036599555015563
  },
  "quality_metrics_summary": {
    "массовая доля": {
      "accuracy": 0.4307453416149068,
      "precision": 0.5757575757575758,
      "recall": 0.7203791469194313,
      "f1": 0.64
    },
    "прочее": {
      "accuracy": 0.5310204752512444,
      "precision": 0.7,
      "recall": 0.6237623762376238,
      "f1": 0.6596858638743456
    }
  }
}