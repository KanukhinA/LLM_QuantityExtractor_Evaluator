{
  "timestamp": "20251204_231644",
  "model_name": "mistralai/Ministral-3-3B-Reasoning-2512",
  "error": "Ошибка загрузки модели: 'ministral3'",
  "error_traceback": "Traceback (most recent call last):\n  File \"C:\\D\\дзАрхив\\Магистратура\\Магистерская\\SmallLLMEvaluator\\model_evaluator.py\", line 146, in evaluate_model\n    model, tokenizer = load_model_func()\n                       ^^^^^^^^^^^^^^^^^\n  File \"C:\\D\\дзАрхив\\Магистратура\\Магистерская\\SmallLLMEvaluator\\model_loaders.py\", line 48, in load_ministral_3_3b_reasoning_2512\n    model = AutoModelForCausalLM.from_pretrained(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 549, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 1372, in from_pretrained\n    return config_class.from_dict(config_dict, **unused_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\configuration_utils.py\", line 808, in from_dict\n    config = cls(**config_dict)\n             ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\models\\mistral3\\configuration_mistral3.py\", line 113, in __init__\n    text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n                  ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\.conda\\envs\\PyTorch\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 1048, in __getitem__\n    raise KeyError(key)\nKeyError: 'ministral3'\n",
  "hyperparameters": {
    "max_new_tokens": 512,
    "do_sample": false,
    "torch_dtype": "bfloat16"
  }
}