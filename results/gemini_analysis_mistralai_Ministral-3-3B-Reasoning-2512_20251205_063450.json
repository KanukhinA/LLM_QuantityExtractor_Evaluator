{
  "model_name": "mistralai/Ministral-3-3B-Reasoning-2512",
  "timestamp": "20251205_063450",
  "analysis": "Предоставленные результаты тестирования модели `mistralai/Ministral-3-3B-Reasoning-2512` на задаче извлечения численных и количественных характеристик из текстов о химических веществах и удобрениях демонстрируют серьезные проблемы как с форматированием вывода, так и с точностью извлечения данных.\n\n## 1. Характерные ошибки модели\n\nОсновные проблемы, выявленные по результатам тестирования:\n\n*   **Массовые ошибки парсинга JSON (95 из X текстов):** Это самая критичная проблема. Модель почти в 100% случаев (судя по 5 примерам и высокому F1) не может выдать валидный JSON. Вместо этого она генерирует:\n    *   Пустые ответы.\n    *   Длинные, повторяющиеся фрагменты текста, которые часто являются либо эхо-ответами на части входного текста, либо генерацией сходных по структуре, но некорректных данных (например, многократное повторение ГОСТов, масс, объемов).\n    *   Неструктурированный текст, который не соответствует формату JSON.\n*   **Чрезвычайно низкий Recall (для \"массовая доля\" 3.70%, для \"прочее\" 6.07%):** Модель практически не находит и не извлекает требуемые данные. Большинство ошибок качества сводятся к тому, что параметр \"предсказано отсутствует\", хотя истинное значение присутствует в тексте. Это указывает на фундаментальную неспособность модели идентифицировать целевые сущности.\n*   **Низкий Precision (для \"массовая доля\" 41.67%, для \"прочее\" 59.09%):** Даже когда модель пытается что-то извлечь, она часто делает это неверно. В сочетании с низким Recall это говорит о том, что модель очень плохо справляется с поставленной задачей.\n*   **Повторяющиеся ответы (hallucinations/loops):** Как видно из примеров ошибок парсинга JSON, модель часто зацикливается, генерируя одни и те же фразы или схожие числовые значения многократно, заполняя `max_new_tokens` бессмысленным текстом.\n\n## 2. Причины ошибок парсинга JSON\n\nОсновной причиной ошибок парсинга JSON является **отсутствие явного указания в промпте о необходимости вывода в формате JSON, а также отсутствие примера такого вывода**.\n\n*   **Отсутствие явного запроса на JSON:** Промпт подробно описывает *что* извлекать и *как* форматировать *отдельные значения* (диапазоны, единицы измерения, название признака), но нигде не говорит, что *весь вывод* должен быть JSON-объектом или массивом объектов. Модель, особенно такая компактная, без явного указания и примера будет генерировать текст в наиболее привычной для нее свободной форме.\n*   **Отсутствие примера JSON-структуры:** Без примера модель не знает, какой именно JSON-структуры вы ожидаете (например, массив объектов, один большой объект с вложенными массивами, какие ключи использовать).\n*   **Ограниченная способность к строгому форматированию:** Модели такого размера (3B) могут испытывать трудности с генерированием идеально структурированного вывода, особенно если задача по извлечению данных уже сложна.\n*   **`max_new_tokens` и `do_sample=false`:** При `do_sample=false` модель всегда выбирает наиболее вероятный следующий токен. Если в какой-то момент она \"сбивается\" с пути генерации JSON, она может застрять в повторяющемся паттерне. `max_new_tokens=512` позволяет ей генерировать достаточно много повторяющегося текста, прежде чем остановиться.\n\n## 3. Причины ошибок в извлечении данных\n\nНизкий Recall и Precision напрямую связаны с тем, что модель не понимает формат вывода, но также могут быть вызваны следующими причинами:\n\n*   **Приоритет JSON-форматирования над извлечением:** Если модель не может понять, как сгенерировать JSON, она не будет тратить ресурсы на точное извлечение данных в соответствии со всеми сложными правилами, потому что она не знает, куда эти данные поместить. Она просто генерирует \"какой-то\" текст.\n*   **Сложность промпта для небольшой модели:** Промпт очень детализирован и содержит много специфичных правил:\n    *   Обработка диапазонов и логических операторов (`[min, max]`, `[min, null]`, `[null, max]`).\n    *   Замена названий элементов на символы (Кальций -> Ca).\n    *   Преобразование \"K в пересчете на К2О\" в \"K2O\".\n    *   Обработка данных из марки продукта.\n    *   Рекомендации по маленьким буквам.\n    Все эти инструкции требуют достаточно высокого уровня \"понимания\" и \"рассуждения\", что может быть затруднительно для 3B-модели.\n*   **\"Министрал\" (3B) может быть недостаточно \"разумным\":** Несмотря на \"Reasoning\" в названии, модель такого размера может просто не иметь достаточной емкости для обработки стольких сложных правил и их последовательного применения.\n*   **Обрезка промпта:** Окончание промпта \"конкр\" обрезано. Возможно, там была важная инструкция, которую модель не получила. Это может привести к неопределенности в ее поведении.\n\n## 4. Рекомендации по улучшению промпта\n\n**Ключевая рекомендация: Включить явное требование и пример JSON-структуры.**\n\n1.  **Явное указание на JSON-вывод:**\n    *   Начните промпт с четкого указания: \"Твоя задача — извлечь все численные характеристики из текста и представить их в виде **JSON-массива объектов**. Каждый объект должен содержать поля `название_признака`, `значение` и `единица_измерения`.\"\n    *   Или, если вы хотите более сложную структуру: \"Представь извлеченные данные в виде **JSON-объекта** со следующими ключами: `массовые_доли` (массив объектов) и `прочие_параметры` (массив объектов).\"\n2.  **Предоставьте пример JSON-вывода (few-shot learning):** Это наиболее эффективный способ показать модели ожидаемый формат.\n    *   Включите 1-3 полных примера ввода текста и соответствующего *правильно сформированного JSON-вывода* непосредственно в промпт.\n    *   Пример:\n        ```\n        Текст: Массовая доля N не менее 10%, P2O5 20-22%. Стандарт: ГОСТ 12345-2023. Масса нетто 50 кг.\n        JSON-вывод:\n        ```json\n        [\n          {\n            \"название_признака\": \"массовая доля N\",\n            \"значение\": [10.0, null],\n            \"единица_измерения\": \"%\"\n          },\n          {\n            \"название_признака\": \"массовая доля P2O5\",\n            \"значение\": [20.0, 22.0],\n            \"единица_измерения\": \"%\"\n          },\n          {\n            \"название_признака\": \"стандарт\",\n            \"значение\": \"ГОСТ 12345-2023\",\n            \"единица_измерения\": null\n          },\n          {\n            \"название_признака\": \"масса нетто\",\n            \"значение\": 50.0,\n            \"единица_измерения\": \"кг\"\n          }\n        ]\n        ```\n        (Обратите внимание на использование `null` и `float` значений)\n3.  **Уточнение правил форматирования:**\n    *   Четко пропишите, что `null` должен использоваться как `null` (без кавычек) в JSON для отсутствующих значений в диапазонах.\n    *   Подчеркните, что десятичные числа должны быть с точкой, не запятой.\n    *   Укажите, что названия признаков должны быть в `snake_case` или `camelCase` если это важно для парсинга, или просто убедитесь, что они консистентны в примере.\n4.  **Упрощение или реструктуризация сложных правил:** Если модель продолжит ошибаться, возможно, некоторые сложные правила (например, преобразование \"Кальций -> Ca\") стоит вынести в отдельные шаги или упростить.\n5.  **Завершить промпт:** Убедитесь, что промпт не обрывается (\"конкр\").\n\n## 5. Рекомендации по настройке гиперпараметров\n\n*   **`repetition_penalty`:** Это самый важный гиперпараметр для решения проблемы с повторяющимся текстом. Увеличьте `repetition_penalty` до значения 1.2 или 1.5. Это значительно снизит вероятность того, что модель будет генерировать одни и те же токены снова и снова.\n*   **`max_new_tokens`:** После введения явного JSON-формата и `repetition_penalty`, возможно, потребуется скорректировать `max_new_tokens`. Если JSON-вывод ожидается длинным, 512 может быть недостаточно. Если же он достаточно компактен, то текущее значение приемлемо.\n*   **`do_sample`:** Пока оставьте `do_sample: false`, чтобы обеспечить детерминированные результаты для отладки. Как только модель начнет стабильно генерировать валидный JSON, можно попробовать `do_sample: true` с очень низким `temperature` (например, 0.1-0.3) для небольшого разнообразия, если это необходимо.\n\n## 6. Общие рекомендации по улучшению качества\n\n1.  **Приоритет №1: Стабильный JSON-вывод:** Достижение стабильного, валидного JSON-вывода должно быть вашей первоочередной задачей. Без этого никакие метрики извлечения данных не имеют смысла. Используйте предложенные улучшения промпта и `repetition_penalty`.\n2.  **Few-shot Learning:** Для моделей меньшего размера (как эта 3B) включение 1-3 высококачественных, полных примеров \"ввод текста -> ожидаемый JSON-вывод\" в промпт часто дает гораздо лучшие результаты, чем долгие и сложные инструкции.\n3.  **Рассмотреть другую модель:** `Ministral-3-3B-Reasoning-2512` может быть недостаточно мощной для такой сложной и строго структурированной задачи. Если есть возможность, попробуйте более крупные и/или более специализированные модели, например:\n    *   **Mistral-7B-Instruct-v0.2/v0.3:** Более крупные модели Mistral имеют значительно лучшую способность следовать инструкциям и генерировать JSON.\n    *   **Модели, специально обученные для извлечения информации или JSON-генерации:** Существуют модели, которые лучше справляются с этой задачей.\n    *   Или даже `GPT-3.5`/`GPT-4` для сравнения и понимания потенциала.\n4.  **Финтюнинг (Fine-tuning):** Если даже с лучшим промптом и более крупными открытыми моделями результаты не будут удовлетворительными, то лучшим решением будет дообучение (fine-tuning) модели на вашем наборе данных. Это научит модель точному формату вывода и нюансам извлечения специфичных для вашей задачи данных.\n5.  **Разбиение задачи:** Если извлечение очень сложная и многоэтапная задача, рассмотрите возможность разбиения ее на более мелкие подзадачи (например, сначала извлечь все числа и их контекст, затем на отдельном шаге классифицировать и отформатировать их). Это можно сделать либо последовательными запросами к LLM, либо комбинируя LLM с традиционными методами.\n6.  **Убедитесь, что текст помещается в контекстное окно:** Поскольку промпт уже достаточно длинный (2000 из 3763 символов), убедитесь, что весь входной текст вместе с промптом не превышает максимальное контекстное окно модели. Укорочение промпта (если возможно без потери информации) может помочь.\n\nНачните с обязательного добавления явного запроса на JSON-вывод и примера в промпт, а также настройки `repetition_penalty`. Это должно решить основную проблему с парсингом JSON и дать более осмысленные результаты для дальнейшей оптимизации.",
  "model_used": "gemini-2.5-flash",
  "parsing_errors_count": 95,
  "quality_metrics_summary": {
    "массовая доля": {
      "precision": 0.4166666666666667,
      "recall": 0.037037037037037035,
      "f1": 0.06802721088435373
    },
    "прочее": {
      "precision": 0.5909090909090909,
      "recall": 0.06074766355140187,
      "f1": 0.11016949152542373
    }
  }
}