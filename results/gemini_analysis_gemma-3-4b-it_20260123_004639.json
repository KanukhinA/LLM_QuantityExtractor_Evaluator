{
  "model_name": "gemma-3-4b-it",
  "timestamp": "20260123_004639",
  "analysis": "Как эксперт по оценке качества работы языковых моделей, я провел анализ предоставленных результатов тестирования модели `gemma-3-4b-it`.\n\n**Резюме результатов:**\nМодель демонстрирует крайне низкие метрики качества для извлечения численных характеристик, особенно для \"массовой доли\", и неудовлетворительные для \"прочих\" параметров. Главная проблема — невалидный JSON, что свидетельствует о фундаментальных сбоях в форматировании ответа.\n\n---\n\n### 1. Характерные ошибки модели\n\n1.  **Невалидный/Неполный JSON (доминирующая проблема):** Это наиболее частая и критическая ошибка. Модель либо обрывает генерацию JSON посредине (Примеры 1 и 2), либо выдает абсолютно неструктурированный, неполный фрагмент (Пример 3), который не может быть распарсен. Это приводит к нулевому или крайне низкому скорингу для таких ответов.\n2.  **Несоблюдение инструкций по именованию и формату:** Даже в тех случаях, когда JSON начинал генерироваться корректно, видны отклонения от предписанных правил:\n    *   Например, в Примере 2 модель использует \"общий азот\" вместо \"N\", \"р2о5\" вместо \"P2O5\", \"к2о\" вместо \"K2O\", \"вода\" вместо \"H2O\", хотя в промпте четко указано: \"Если в названии признака есть название химического элемента (например Кальций) - замени его на обозначение химического элемента - Ca.\" и \"Если указано например \"МАССОВАЯ ДОЛЯ K В ПЕРЕСЧЕТЕ НА К2О\" - то правильным извлекаемым признаком будет \"массовая доля K2O\".\"\n    *   Капитализация (например, \"р2о5\" вместо \"P2O5\") также не соответствует примерам промпта, хотя инструкция \"Названия признаков пиши с маленькой буквы\" относится к *названиям признаков* (например \"массовая доля Zn\"), а не к химическим обозначениям.\n3.  **Потенциальное \"галлюцинирование\" или вывод отсутствующих данных:** В Примере 1 модель включает `{\"вещество\": \"NH4+\", \"массовая доля\": [null, 2]}`. Если в исходном тексте для этого примера не было информации об `NH4+`, то это является нарушением инструкции \"Если признака в тексте нет - не извлекай его и не выводи его в json.\" (Однако, это также может быть следствием того, что пример JSON в промпте включает NH4+, и модель пытается имитировать структуру).\n4.  **Сложности с длинными и комплексными текстами:** Пример 3 с очень длинным и разнообразным текстом показал полный сбой в генерации, что указывает на трудности модели с обработкой большого объема информации в рамках контекстного окна и поддержанием сложного формата.\n\n### 2. Причины ошибок парсинга JSON\n\nОсновной причиной ошибок парсинга JSON, очевидно, является **ограничение `max_new_tokens` (512)**. В Примерах 1 и 2 видно, что модель начинает генерировать правильный JSON, но затем резко обрывается. Длина промпта (3768 символов) сама по себе значительна, и если ожидаемый JSON-ответ тоже длинный (что вероятно для богатых информацией текстов), то 512 новых токенов крайне недостаточно для его завершения. Модель просто достигает лимита и прекращает генерацию, оставляя невалидный JSON.\n\nДля Примера 3, где ответ обрывается на `[0.001`, причина может быть в сочетании слишком длинного входного текста, длинного промпта и ограниченного `max_new_tokens`, что приводит к потере контекста или внутреннему сбою модели при попытке обработки такого объема информации, не давая даже начать структурированный ответ.\n\n### 3. Причины ошибок в извлечении данных\n\n1.  **Недостаточная длина генерации (`max_new_tokens`):** Поскольку большинство ответов невалидны, трудно адекватно оценить качество самого извлечения. Низкие метрики могут быть **прямым следствием невалидного JSON**, а не исключительно плохим качеством извлечения. Модель могла бы извлечь данные, но не смогла их полностью выдать.\n2.  **Сложность и детализация промпта для модели 4B:** Несмотря на то, что промпт очень подробный и хорошо структурирован, для относительно небольшой модели `gemma-3-4b-it` следовать такому количеству специфических и часто конкурирующих инструкций (например, правила именования, обработки диапазонов, приоритеты) может быть очень сложно, особенно когда промпт занимает значительную часть контекстного окна.\n3.  **Ограничения модели:** `gemma-3-4b-it` — это меньшая модель. Хотя она и instruction-tuned, ее способность к сложному логическому рассуждению, точному следованию множеству мелких деталей и поддержанию сложной структуры вывода для высокоточной NER задачи может быть ограничена по сравнению с более крупными моделями.\n4.  **Влияние примеров в промпте:** Наличие `NH4+` в примере JSON в промпте могло заставить модель включить его в свой ответ, даже если его не было в исходном тексте, что является ошибкой извлечения.\n\n### 4. Рекомендации по улучшению промпта\n\nПромпт сам по себе очень хорошо детализирован, но его длина и наличие многочисленных правил могут перегружать модель.\n\n1.  **Оптимизация структуры и длины промпта:**\n    *   **Перенести пример JSON в Few-shot формат:** Вместо встраивания примеров внутрь инструкций, используйте несколько полных пар \"Текст для анализа -> Ожидаемый JSON\" *перед* вашим основным \"Текст для анализа\" для текущего запроса. Это позволяет модели учиться на конкретных случаях без необходимости парсить длинные описания.\n    *   **Сократить избыточность:** Просмотрите промпт на предмет фраз, которые можно сократить без потери смысла.\n    *   **Разделение инструкций:** Можно попробовать сгруппировать инструкции более логично или выделить наиболее критичные, чтобы они были лучше усвоены.\n2.  **Усиление инструкций по форматированию:**\n    *   **Явное требование валидности JSON:** В конце промпта добавьте очень строгое напоминание: \"КРАЙНЕ ВАЖНО: Весь вывод должен быть *строго* валидным JSON-объектом.\"\n    *   **Повторение правил именования:** Еще раз подчеркните правила преобразования химических названий в символы (`N`, `P2O5`, `K2O`, `Ca`) и регистр.\n3.  **Уточнение обработки отсутствующих данных:** Если проблема с `NH4+` действительно является \"галлюцинацией\", явно повторите: \"Не извлекай и не выводи в JSON никаких признаков, которые *явно* не указаны в предоставленном тексте, даже если они присутствуют в примерах.\"\n\n### 5. Рекомендации по настройке гиперпараметров\n\n1.  **`max_new_tokens`:** **Это самая критическая настройка.** Немедленно увеличьте этот параметр. Для сложных текстов и детализированных JSON ответов 512 токенов недостаточно. Необходимо установить значение, которое гарантированно позволит модели завершить JSON-объект. Рекомендуется начать с 2048 или даже 4096, а затем уменьшить, если выяснится, что большинство ответов короче.\n2.  **`temperature`:** Если модель иногда выдает несвязный текст или начинает \"фантазировать\" (как в Примере 3), попробуйте немного понизить `temperature` (например, до 0.5-0.7). Это сделает ответы более детерминированными и менее \"креативными\", что желательно для задачи извлечения данных.\n3.  **`top_p` / `top_k`:** Обычно для таких задач можно оставить значения по умолчанию или немного понизить `top_p` (например, до 0.8-0.9), чтобы сузить выборку слов и снизить вероятность несвязного вывода.\n\n### 6. Общие рекомендации по улучшению качества\n\n1.  **Few-shot Learning (Обучение на примерах):** Вместо того чтобы описывать все правила в промпте, предоставьте модели 3-5 *качественных примеров* \"входной текст -> идеальный JSON-вывод\" *перед* целевым текстом. Это значительно улучшает способность модели следовать формату и правилам, особенно для небольших моделей. Примеры должны быть разнообразными и охватывать разные аспекты правил. Это может потребовать сокращения инструктивной части промпта, чтобы не превышать общее контекстное окно.\n2.  **Пост-обработка:** Внедрите программный слой пост-обработки, который попытается \"починить\" невалидный JSON, если это возможно. Например, если JSON обрезан, можно попытаться добавить закрывающие скобки, если структура очевидна. Это не решает проблему генерации, но может спасти часть данных и повысить практическую пригодность модели.\n3.  **Тестирование контекстного окна:** Убедитесь, что суммарная длина промпта (включая инструкции и few-shot примеры) + длина входного текста + ожидаемая максимальная длина ответа не превышает максимальное контекстное окно модели.\n4.  **Использование более мощной модели:** Если после всех вышеперечисленных оптимизаций (особенно увеличения `max_new_tokens` и внедрения few-shot примеров) `gemma-3-4b-it` все еще показывает неудовлетворительные результаты, это может быть признаком того, что для такой сложной и точной задачи требуется более крупная и способная модель (например, Llama 3 8B, Gemma 7B, Mistral Large или другие модели, специально оптимизированные для извлечения информации).\n5.  **Fine-tuning:** Для задач, требующих высокой точности и специфического форматирования, лучшим решением часто является fine-tuning модели на большом, размеченном наборе данных \"текст -> правильный JSON\". Это позволит модели гораздо лучше \"учить\" тонкости формата и правил извлечения.",
  "model_used": "gemini-2.5-flash",
  "parsing_errors_count": 3,
  "hyperparameters": {
    "max_new_tokens": 512,
    "model_name": "gemma-3-4b-it",
    "api_model": true
  },
  "system_info": {
    "api_model": true,
    "multi_agent_mode": null,
    "gpu_info": {
      "api": true
    },
    "gpu_memory_during_inference_gb": 0.0,
    "average_response_time_seconds": 6.251798439025879
  },
  "quality_metrics_summary": {
    "массовая доля": {
      "accuracy": 0.1155,
      "precision": 0.11935483870967742,
      "recall": 0.19680851063829788,
      "f1": 0.14859437751004018
    },
    "прочее": {
      "accuracy": 0.3401281933196827,
      "precision": 0.4329501915708812,
      "recall": 0.5621890547263682,
      "f1": 0.4891774891774892
    }
  }
}