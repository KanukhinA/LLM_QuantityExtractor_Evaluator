{
  "model_name": "gemma-3-27b-it",
  "timestamp": "20260125_134527",
  "analysis": "Отлично, я провел анализ предоставленных результатов тестирования модели `gemma-3-27b-it`. Вот мои выводы и рекомендации:\n\n---\n\n### Анализ результатов тестирования модели gemma-3-27b-it\n\n**Модель:** gemma-3-27b-it\n**Гиперпараметры:** `max_new_tokens: 512`, `model_name: gemma-3-27b-it`, `api_model: true`\n**Промпт:** Подробный, 3768 символов, с четкими инструкциями по извлечению и форматированию JSON.\n\n---\n\n### 1. Характерные ошибки модели\n\n1.  **Невалидный/незавершенный JSON (основная проблема):** Самая критичная ошибка, явно продемонстрированная в примере #95, где модель обрывает вывод JSON на полуслове. Это указывает на то, что модель не успевает сгенерировать полный ответ в рамках выделенных токенов или сталкивается с внутренними проблемами при генерации длинных структурированных ответов. Незавершенный JSON делает весь вывод непригодным для автоматической обработки.\n2.  **Низкая полнота (Recall), особенно для категории \"прочее\":**\n    *   `Массовая доля`: Recall 75.87% – модель пропускает около четверти массовых долей.\n    *   `Прочее`: Recall 59.91% – модель пропускает более 40% элементов в категории \"прочее\". Это может быть связано с большой вариативностью формулировок для \"прочих\" параметров (стандарты, марки, различные виды масс, количества) и сложностью применения всех специфических правил, описанных в промпте.\n3.  **Потенциальные ошибки в применении сложных правил (предположение):** Хотя метрики `Precision` для \"массовой доли\" достаточно высоки (88.85%), а для \"прочее\" заметно ниже (69.74%), низкий `Recall` и общий F1-показатель (81.85% и 64.45% соответственно) позволяют предположить, что модель испытывает трудности с точным применением всех 15+ \"особых указаний\". Например:\n    *   Определение \"масса нетто\" vs \"масса брутто\".\n    *   Преобразование \"K\" в \"K2O\".\n    *   Обработка диапазонов и логических операторов `[min, null]`, `[null, max]`.\n    *   Игнорирование общих условий в пользу конкретных значений.\n    *   Выполнение арифметических операций (\"1000 КГ+5%\").\n\n---\n\n### 2. Причины ошибок парсинга JSON\n\nОсновной причиной ошибок парсинга JSON, как видно из примера #95, является **обрыв генерации ответа**. Это почти наверняка связано с ограничением `max_new_tokens: 512`.\n\n*   Длинный промпт (3768 символов) + длинный входной текст (как в примере #95, который содержит множество сущностей) + потенциально очень длинный JSON-ответ = превышение лимита токенов для генерации.\n*   Даже если модель не достигает общего лимита контекста, она может завершить генерацию после `max_new_tokens` *выходных* токенов, независимо от того, закончен ли JSON.\n*   Сам JSON-формат, особенно с вложенными структурами и длинными списками объектов (как в примере #95, где много веществ), может легко превысить 512 токенов.\n\n---\n\n### 3. Причины ошибок в извлечении данных\n\n1.  **Сложность и объем промпта:** Промпт очень детализирован и содержит множество специфических правил и исключений. Чем больше правил, тем выше вероятность, что модель пропустит или неправильно применит одно из них. Длина промпта также \"занимает\" часть контекстного окна.\n2.  **Сложность обработки \"прочих\" категорий:** Категория \"прочее\" имеет более разнообразные типы сущностей и более сложные логические правила для их извлечения и наименования (`масса нетто единицы`, `масса брутто`, `стандарт`, `марка`). Это подтверждается значительно более низкими метриками для этой категории.\n3.  **Недостаток примеров:** Несмотря на подробное описание, один JSON-пример в промпте может быть недостаточным, чтобы модель научилась справляться со всеми нюансами и разнообразием входных данных, особенно для категории \"прочее\", где вариативность выше.\n4.  **Длинные входные тексты:** Как в примере #95, очень длинные тексты могут вызывать \"long-context syndrome\", когда модель хуже обрабатывает информацию, расположенную в середине или конце текста.\n\n---\n\n### 4. Рекомендации по улучшению промпта\n\n1.  **Сокращение и структурирование промпта (если возможно):**\n    *   Подумайте, можно ли объединить некоторые инструкции или сделать их более лаконичными без потери смысла.\n    *   Используйте более четкие заголовки и списки для улучшения читаемости промпта моделью.\n2.  **Добавление нескольких примеров (few-shot learning):** Предоставьте 2-3 **разнообразных** примера полного входного текста с соответствующими корректными JSON-выводами, демонстрирующими как успешное извлечение, так и применение сложных правил (например, для \"масса брутто/нетто\", для K2O, для диапазонов). Это значительно улучшает способность модели следовать формату и логике.\n3.  **Больше конкретики для \"прочих\" параметров:** Если для \"прочих\" есть очень специфические и часто встречающиеся паттерны (например, для ТУ/ГОСТ), можно добавить их в примеры или явно упомянуть.\n4.  **Усиление инструкций по завершению JSON:** Можно добавить фразу вроде \"Убедись, что JSON полностью завершен и валиден.\" перед `ОТВЕТ:`. Хотя это не гарантирует решения проблемы с `max_new_tokens`, иногда помогает модели \"помнить\" о задаче.\n\n---\n\n### 5. Рекомендации по настройке гиперпараметров\n\n1.  **Увеличить `max_new_tokens` (КРИТИЧЕСКИ ВАЖНО):** Это первоочередная мера. Текущие 512 токенов явно недостаточно для генерации подробного JSON из длинных текстов. Установите значение как минимум в 2048, а лучше в 4096 или даже больше, если позволяют ограничения API и бюджет. Это должно решить проблему с обрезанным JSON.\n2.  **Настроить `temperature`:**\n    *   Начните с низкого значения `temperature` (например, `0.1` или `0.2`). Это сделает вывод модели более детерминированным и предсказуемым, что крайне важно для генерации структурированного JSON.\n    *   Если низкая температура приводит к ухудшению Recall (модель становится слишком консервативной и пропускает данные), можно постепенно увеличивать ее до `0.5`, но не выше, чтобы избежать \"галлюцинаций\" и ошибок форматирования.\n3.  **Настроить `top_p` и `top_k` (если доступны):** Для структурированного вывода обычно рекомендуются следующие значения:\n    *   `top_p` около `0.9` (или `0.95`).\n    *   `top_k` около `40-50`.\n    Эти параметры также помогают контролировать случайность и сосредоточенность генерации.\n\n---\n\n### 6. Общие рекомендации по улучшению качества\n\n1.  **Пост-обработка вывода:**\n    *   **Валидация JSON:** Внедрите обязательный этап проверки синтаксиса JSON. Если JSON невалиден, попробуйте восстановить его (например, добавьте закрывающие скобки, если он просто обрезан) или отметьте для ручного анализа.\n    *   **Проверка типов данных:** Убедитесь, что извлеченные числовые значения действительно являются числами, а не текстом, и что единицы измерения соответствуют ожидаемым.\n    *   **Исправление распространенных ошибок:** Создайте правила пост-обработки для автоматического исправления часто встречающихся ошибок (например, если модель ошиблась в названии признака на одну букву).\n2.  **Итеративное улучшение промпта:** Регулярно анализируйте ошибки модели на новых данных. Если появляются новые типы ошибок, уточняйте промпт, добавляя конкретные инструкции или примеры для их предотвращения.\n3.  **Fine-tuning (если необходимо и возможно):** Если после всех вышеперечисленных шагов качество все еще не устраивает, рассмотрите возможность дообучения (fine-tuning) модели на большом объеме данных, размеченных в соответствии с вашими требованиями. Это самый эффективный способ привить модели специфическое поведение и улучшить ее понимание предметной области и формата вывода.\n4.  **Разделение задач (если входные тексты очень длинные):** Для очень длинных текстов, которые могут вызывать проблемы с контекстом, можно рассмотреть стратегию разделения текста на более мелкие, перекрывающиеся сегменты, обработку каждого сегмента и последующую агрегацию результатов. Это сложнее, но может быть необходимо для сверхдлинных документов.\n\n---\n\n**Вывод:** Основной проблемой, судя по всему, является ограничение на количество генерируемых токенов, что приводит к обрыву JSON-ответа. Увеличение `max_new_tokens` и тщательная настройка `temperature` в сочетании с добавлением нескольких примеров в промпт должны значительно улучшить производительность модели.",
  "model_used": "gemini-2.5-flash",
  "parsing_errors_count": 1,
  "hyperparameters": {
    "max_new_tokens": 512,
    "model_name": "gemma-3-27b-it",
    "api_model": true
  },
  "system_info": {
    "api_model": true,
    "multi_agent_mode": null,
    "gpu_info": {
      "api": true
    },
    "gpu_memory_during_inference_gb": 0.0,
    "average_response_time_seconds": 9.4349959731102
  },
  "quality_metrics_summary": {
    "массовая доля": {
      "accuracy": 0.8517464295659785,
      "precision": 0.8884758364312267,
      "recall": 0.7587301587301587,
      "f1": 0.8184931506849316
    },
    "прочее": {
      "accuracy": 0.5374518999518999,
      "precision": 0.6974358974358974,
      "recall": 0.5991189427312775,
      "f1": 0.6445497630331753
    }
  }
}