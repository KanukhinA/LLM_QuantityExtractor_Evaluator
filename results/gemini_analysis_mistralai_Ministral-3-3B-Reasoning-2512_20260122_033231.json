{
  "model_name": "mistralai/Ministral-3-3B-Reasoning-2512",
  "timestamp": "20260122_033231",
  "analysis": "## Экспертная оценка качества работы модели mistralai/Ministral-3-3B-Reasoning-2512\n\n### Общий анализ\n\nМодель демонстрирует крайне низкое качество выполнения поставленной задачи. Метрики Precision, Recall и F1-score находятся на неприемлемо низком уровне (12-48%), что говорит о практически полном провале в извлечении данных. Наиболее критичная проблема — тотальный отказ модели от генерации валидного JSON-формата, что делает всю ее работу бесполезной для автоматизированной обработки.\n\n### 1. Характерные ошибки модели\n\n1.  **Полное игнорирование JSON-формата:** Это доминирующая и наиболее серьезная ошибка. Модель либо генерирует произвольный текст (часто повторяя части входного текста или генерируя бессмысленные фразы), либо выдает пустой ответ, либо генерирует неполные и синтаксически некорректные фрагменты, которые не являются валидным JSON.\n    *   Пример 1: Модель выдала повторяющийся текст вместо JSON.\n    *   Пример 2: Модель выдала пустую строку.\n    *   Пример 3: Модель повторила исходный текст, затем оборвала его.\n2.  **Игнорирование инструкции по форматированию вывода:** Модель не следует инструкции \"Выводи json результат **только после слова ОТВЕТ:**\". Зачастую она либо не выводит ничего, либо выводит некорректный текст сразу.\n3.  **Отсутствие извлечения данных:** Из-за невозможности сгенерировать правильный формат, модель фактически не извлекает ни одного требуемого числового признака, даже если они очевидно присутствуют в тексте (например, стандарт ТУ в примере 1 или массовая доля K2O в примере 3).\n4.  **Галлюцинации и повторения:** В некоторых случаях модель начинает \"галлюцинировать\", выдавая повторяющиеся фрагменты или несвязанный текст, как в примере 1 и 6.\n\n### 2. Причины ошибок парсинга JSON\n\nОсновная причина ошибок парсинга JSON заключается в том, что модель *вообще не пытается* генерировать JSON или генерирует его настолько плохо, что он невалиден. Это происходит по нескольким причинам:\n\n1.  **Размер и возможности модели (mistralai/Ministral-3-3B-Reasoning-2512):** Модель размером 3 миллиарда параметров, несмотря на \"Reasoning\" в названии, является относительно небольшой. Для выполнения сложных задач, требующих строгого форматирования вывода по заданной схеме, такие модели часто испытывают трудности. Они могут хорошо понимать смысл, но не справляться с жесткими ограничениями по структуре.\n2.  **Длина и сложность промпта (3768 символов):** Промпт очень длинный и содержит множество детализированных инструкций, исключений и примеров. Для небольшой модели это создает огромную когнитивную нагрузку. Модель, вероятно, \"забывает\" или не может удержать в контексте все правила, особенно те, что касаются строгого JSON-формата, которые расположены ближе к концу промпта.\n3.  **Отсутствие few-shot примеров:** В промпте дан *один* пример желаемого JSON-формата, но нет примеров \"входной текст -> ожидаемый JSON-вывод\". Few-shot learning критически важен для обучения модели генерировать конкретный формат, особенно такой сложный и вложенный, как здесь. Модель не видит, как применять все правила на практике в различных сценариях.\n4.  **Перегрузка информацией:** Промпт пытается одновременно решить множество задач: найти признаки, определить название, значение, единицу измерения, обработать диапазоны, операторы, специальные случаи марки, стандартов, химических элементов, масс с упаковкой, дублирующихся параметров, а затем еще и вывести все это в строго определенном JSON. Это слишком много для одной инструкции для данной модели.\n\n### 3. Причины ошибок в извлечении данных\n\nПричины ошибок в извлечении данных напрямую вытекают из невозможности модели соблюдать JSON-формат:\n\n1.  **Каскадный сбой:** Если модель не может справиться с основной задачей форматирования вывода, то задача извлечения данных становится второстепенной и нереализуемой. Некорректный или отсутствующий JSON означает, что никакие данные не были извлечены в требуемом виде.\n2.  **Недостаточная \"глубина\" понимания промпта:** Длинный и сложный промпт может привести к тому, что модель не сможет эффективно выделить ключевые сущности и их числовые значения, даже если она их \"видит\" в тексте. Она теряется в объеме инструкций.\n\n### 4. Рекомендации по улучшению промпта\n\n1.  **Добавить Few-Shot Примеры:** Это **самая важная** рекомендация. Предоставьте 3-5 пар \"Входной текст\" -> \"Ожидаемый JSON-вывод\" перед основным текстом для анализа. Примеры должны охватывать различные сценарии:\n    *   Текст с несколькими массовыми долями и прочими параметрами (включая диапазоны и \"не менее/не более\").\n    *   Текст только со стандартом или маркой.\n    *   Текст, где ничего не найдено (ожидаемый пустой JSON: `{\"массовая доля\": [], \"прочее\": []}`).\n    *   Пример, включающий специфические правила (например, пересчет K на K2O, массу брутто/нетто).\n2.  **Сократить и упростить промпт:**\n    *   Удалить избыточные слова и фразы.\n    *   Сгруппировать связанные инструкции.\n    *   Возможно, некоторые наименее частые и сложные правила перенести в раздел \"Примечаний\" или рассмотреть возможность их обработки после извлечения сырых данных.\n3.  **Усилить инструкцию по JSON-формату:** Сделать ее более заметной и повторяющейся. Например, использовать выделение текста, добавить фразу \"Крайне важно, что вывод должен быть *только* в формате JSON\".\n4.  **Использовать четкие разделители:** Разделить промпт на логические блоки с помощью таких разделителей, как `---ИНСТРУКЦИИ---`, `---ПРИМЕРЫ---`, `---ТЕКСТ ДЛЯ АНАЛИЗА---`, `---ОТВЕТ---`. Это поможет модели лучше структурировать входную информацию.\n5.  **Облегчить задачу по преобразованию:** Некоторые правила, такие как замена \"Кальций\" на \"Ca\" или \"МАССОВАЯ ДОЛЯ K В ПЕРЕСЧЕТЕ НА К2О\" на \"массовая доля K2O\", могут быть сложными для модели в сочетании с извлечением и форматированием. Рассмотрите возможность выполнения этих преобразований *после* извлечения сырых данных с помощью пост-обработки.\n\n### 5. Рекомендации по настройке гиперпараметров\n\nТекущие гиперпараметры:\n*   `max_new_tokens`: 512 – Достаточно для ожидаемого JSON.\n*   `do_sample`: `false` – Хорошо для детерминированной оценки.\n*   `torch_dtype`: `bfloat16` – Стандартный тип данных.\n\nНа данном этапе изменение этих гиперпараметров не решит корневую проблему невалидного JSON. Проблема гораздо глубже, чем простое изменение поведения при генерации. Если бы JSON генерировался, но был бы слишком коротким, тогда можно было бы увеличить `max_new_tokens`. Однако, пока модель неспособна генерировать JSON вообще, эти настройки второстепенны.\n\n### 6. Общие рекомендации по улучшению качества\n\n1.  **Смена модели:** Модель `mistralai/Ministral-3-3B-Reasoning-2512` с большой долей вероятности недостаточно мощна для данной задачи с такими строгими требованиями к формату.\n    *   **Рассмотрите более крупные модели:** Например, Mistral-7B-Instruct-v0.2/v0.3, Mixtral-8x7B-Instruct-v0.1, Llama-3-8B-Instruct или другие модели, специально оптимизированные для следования инструкциям и генерации структурированных данных.\n    *   **Модели с поддержкой Function Calling/Tools:** Если вы используете API (например, OpenAI, Anthropic), их модели часто имеют встроенные механизмы Function Calling, которые значительно улучшают генерацию валидного JSON по заданной схеме.\n2.  **Fine-tuning (дообучение модели):** Если задача критична и объем данных для обучения достаточно большой, дообучение специализированной модели на большом наборе пар \"Текст -> Правильный JSON\" даст наилучшие результаты. Это научит модель *конкретно* вашему формату и логике извлечения.\n3.  **Гибридный подход:** Разделите задачу на этапы:\n    *   **Этап 1 (LLM):** Использование LLM для *извлечения сырых данных* из текста в менее строгом формате (например, список пар \"название признака: значение [единица]\"). Это может быть проще для модели, чем генерация сложного JSON.\n    *   **Этап 2 (Программный код):** Напишите код (Python и т.д.), который берет сырые данные, извлеченные LLM, и программно форматирует их в требуемый JSON-структуру, обрабатывая диапазоны, логические операторы и т.д. Это перекладывает бремя точного форматирования на детерминированный код.\n4.  **Итеративная разработка промпта:**\n    *   Начните с максимально простого промпта, который просит только извлечь, например, \"массовую долю N\" и вывести ее значение.\n    *   Постепенно добавляйте сложность: сначала один тип \"прочего\" параметра, затем диапазоны, затем другие химические элементы, затем весь JSON.\n    *   На каждом шаге проверяйте, справляется ли модель, и корректируйте промпт.\n\nВ текущем состоянии модель `mistralai/Ministral-3-3B-Reasoning-2512` не справляется с поставленной задачей. Наиболее эффективным путем будет либо смена модели на более мощную, либо применение few-shot примеров в промпте, либо комбинация этих подходов с переработкой промпта.",
  "model_used": "gemini-2.5-flash",
  "parsing_errors_count": 82,
  "hyperparameters": {
    "max_new_tokens": 512,
    "do_sample": false,
    "torch_dtype": "bfloat16"
  },
  "system_info": {
    "api_model": false,
    "multi_agent_mode": null,
    "gpu_info": {
      "cuda_available": true,
      "cuda_version": "12.1",
      "gpu_name": "NVIDIA GeForce RTX 4060 Laptop GPU",
      "gpu_memory_total_gb": 8.0,
      "gpu_memory_allocated_gb": 0.0,
      "gpu_memory_reserved_gb": 0.0,
      "gpu_name_detailed": "NVIDIA GeForce RTX 4060 Laptop GPU",
      "driver_version": "591.74"
    },
    "gpu_memory_during_inference_gb": 5.88,
    "average_response_time_seconds": 156.43333882570266
  },
  "quality_metrics_summary": {
    "массовая доля": {
      "accuracy": 0.09212885154061624,
      "precision": 0.4166666666666667,
      "recall": 0.12,
      "f1": 0.1863354037267081
    },
    "прочее": {
      "accuracy": 0.09166164356037773,
      "precision": 0.4857142857142857,
      "recall": 0.16267942583732056,
      "f1": 0.24372759856630824
    }
  }
}