"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏
"""
import os
import sys
import json
import glob
from model_evaluator import ModelEvaluator
from config import GEMINI_API_KEY
from utils import find_dataset_path

def main():
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python reevaluate.py <–ø—É—Ç—å_–∫_csv_—Ñ–∞–π–ª—É_—Å_—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏> [–∏–º—è_–º–æ–¥–µ–ª–∏] [--gemini]")
        print("\n–ü—Ä–∏–º–µ—Ä:")
        print("  python reevaluate.py results/results_google_gemma-2-2b-it_20251203_123456.csv")
        print("  python reevaluate.py results/results_google_gemma-2-2b-it_20251203_123456.csv 'google/gemma-2-2b-it'")
        print("  python reevaluate.py results/results_google_gemma-2-2b-it_20251203_123456.csv 'google/gemma-2-2b-it' --gemini")
        sys.exit(1)
    
    results_csv_path = sys.argv[1]
    model_name = sys.argv[2] if len(sys.argv) > 2 and not sys.argv[2].startswith('--') else None
    use_gemini = '--gemini' in sys.argv
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
    if not os.path.exists(results_csv_path):
        print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {results_csv_path}")
        sys.exit(1)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_path = find_dataset_path()
    if not os.path.exists(dataset_path):
        print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_path}")
        sys.exit(1)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_dir = os.path.dirname(results_csv_path) or "results"
    
    try:
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫—É
        result = ModelEvaluator.reevaluate_from_file(
            results_csv_path=results_csv_path,
            dataset_path=dataset_path,
            output_dir=output_dir,
            model_name=model_name,
            use_gemini_analysis=use_gemini,
            gemini_api_key=GEMINI_API_KEY if use_gemini else None
        )
        
        print(f"\n‚úÖ –ü–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ Gemini –≤ JSON, –µ—Å–ª–∏ –æ–Ω –±—ã–ª –≤—ã–ø–æ–ª–Ω–µ–Ω
        gemini_analysis = result.get("gemini_analysis")
        if gemini_analysis and gemini_analysis.get("status") == "success":
            from datetime import datetime
            import json
            timestamp = result.get("timestamp", datetime.now().strftime("%H%M%S"))
            from model_evaluator import sanitize_filename
            model_name_safe = sanitize_filename(result.get("model_name", "unknown"))
            
            analysis_text = gemini_analysis.get("analysis", "")
            quality_metrics = result.get("quality_metrics", {})
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —Å–∏—Å—Ç–µ–º–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –º–µ—Ç—Ä–∏–∫
            hyperparameters = result.get("hyperparameters", {})
            system_info = None
            multi_agent_mode = None
            prompt_full_text = None
            prompt_info = None
            prompt_template = None
            
            # –ò—â–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª –º–µ—Ç—Ä–∏–∫ (–±–µ–∑ _reevaluated)
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤ —Å—Ç–∞—Ä–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ (–ø–ª–æ—Å–∫–æ–π) –∏ –Ω–æ–≤–æ–π (–≤–ª–æ–∂–µ–Ω–Ω–æ–π)
            model_name_for_pattern = sanitize_filename(result.get("model_name", "unknown"))
            csv_dir = os.path.dirname(results_csv_path)
            
            # –ò—â–µ–º –≤ —Å—Ç–∞—Ä–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ (–ø–ª–æ—Å–∫–æ–π)
            metrics_file_pattern = os.path.join(csv_dir, f"metrics_{model_name_for_pattern}_*.json")
            metrics_files = glob.glob(metrics_file_pattern)
            # –ò—Å–∫–ª—é—á–∞–µ–º —Ñ–∞–π–ª—ã —Å _reevaluated
            original_metrics_files = [f for f in metrics_files if "_reevaluated" not in f]
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –≤ –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –ø–∞–ø–æ–∫ (model_key/prompt_name/)
            if not original_metrics_files:
                for root, dirs, files in os.walk(csv_dir):
                    for file in files:
                        if file.startswith("metrics_") and file.endswith(".json") and "_reevaluated" not in file:
                            file_path = os.path.join(root, file)
                            original_metrics_files.append(file_path)
            
            if original_metrics_files:
                try:
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª –º–µ—Ç—Ä–∏–∫
                    with open(original_metrics_files[-1], 'r', encoding='utf-8') as f:
                        original_metrics = json.load(f)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                    hyperparameters = original_metrics.get("hyperparameters", hyperparameters)
                    gpu_info = original_metrics.get("gpu_info", {})
                    average_response_time = original_metrics.get("average_response_time_seconds", None)
                    gpu_memory_during_inference = original_metrics.get("gpu_memory_during_inference_gb", None)
                    api_model = original_metrics.get("api_model", False)
                    multi_agent_mode = original_metrics.get("multi_agent_mode")
                    prompt_full_text = original_metrics.get("prompt_full_text")
                    prompt_info = original_metrics.get("prompt_info")
                    prompt_template = original_metrics.get("prompt_template")
                    
                    system_info = {
                        "api_model": api_model,
                        "multi_agent_mode": multi_agent_mode,
                        "gpu_info": gpu_info,
                        "gpu_memory_during_inference_gb": gpu_memory_during_inference,
                        "average_response_time_seconds": average_response_time,
                        "source": "from_original_metrics_file"
                    }
                except Exception as e:
                    print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –º–µ—Ç—Ä–∏–∫: {e}")
            
            # –ï—Å–ª–∏ —Å–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –±—ã–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —Å–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é
            if system_info is None:
                system_info = {
                    "note": "system_info_not_available_for_reevaluation",
                    "source": "reevaluated_from_file"
                }
            
            analysis_data = {
                "model_name": result.get("model_name"),
                "timestamp": timestamp,
                "reevaluated_from": results_csv_path,
                "analysis": analysis_text,
                "model_used": gemini_analysis.get("model_used", "gemini-2.5-flash"),
                "parsing_errors_count": len(result.get("parsing_errors", [])),
                "hyperparameters": hyperparameters,
                "prompts": {
                    "prompt_template": prompt_template,
                    "prompt_full_text": prompt_full_text,
                    "prompt_info": prompt_info,
                    "note": "prompts_loaded_from_metrics_file" if prompt_full_text else "prompts_not_available_for_reevaluation"
                },
                "system_info": system_info,
                "quality_metrics_summary": {
                    "–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è": {
                        "accuracy": quality_metrics.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {}).get('accuracy', 0) if quality_metrics else 0,
                        "precision": quality_metrics.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {}).get('precision', 0) if quality_metrics else 0,
                        "recall": quality_metrics.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {}).get('recall', 0) if quality_metrics else 0,
                        "f1": quality_metrics.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {}).get('f1', 0) if quality_metrics else 0
                    },
                    "–ø—Ä–æ—á–µ–µ": {
                        "accuracy": quality_metrics.get('–ø—Ä–æ—á–µ–µ', {}).get('accuracy', 0) if quality_metrics else 0,
                        "precision": quality_metrics.get('–ø—Ä–æ—á–µ–µ', {}).get('precision', 0) if quality_metrics else 0,
                        "recall": quality_metrics.get('–ø—Ä–æ—á–µ–µ', {}).get('recall', 0) if quality_metrics else 0,
                        "f1": quality_metrics.get('–ø—Ä–æ—á–µ–µ', {}).get('f1', 0) if quality_metrics else 0
                    }
                } if quality_metrics else None
            }
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫ (—Ç–∞–∫–∞—è –∂–µ, –∫–∞–∫ –≤ reevaluate_from_file)
            from config import PROMPT_TEMPLATE_NAME
            from model_evaluator import sanitize_filename
            import re
            
            model_key = result.get("model_key")
            if not model_key:
                # –£–±–∏—Ä–∞–µ–º –¥–∞—Ç—É –∏–∑ model_name_safe, –µ—Å–ª–∏ –æ–Ω–∞ —Ç–∞–º –µ—Å—Ç—å (—Ñ–æ—Ä–º–∞—Ç: name_YYYYMMDD)
                model_key = model_name_safe
                # –£–±–∏—Ä–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ _20260123 –∏–ª–∏ _20260123_123456
                model_key = re.sub(r'_\d{8}(_\d{6})?$', '', model_key)
                if not model_key:  # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥–∞—Ç—ã –Ω–∏—á–µ–≥–æ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å
                    model_key = model_name_safe
            else:
                model_key = sanitize_filename(model_key)
            
            prompt_template_name = result.get("prompt_template") or prompt_template
            # –ï—Å–ª–∏ –≤ —Å—Ç–∞—Ä–æ–º —Ñ–∞–π–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ (build_prompt3), –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ PROMPT_TEMPLATE_NAME
            if prompt_template_name == "build_prompt3" or prompt_template_name == "build_prompt":
                prompt_template_name = PROMPT_TEMPLATE_NAME
            
            if multi_agent_mode:
                prompt_folder_name = sanitize_filename(multi_agent_mode)
            elif prompt_template_name:
                prompt_folder_name = sanitize_filename(prompt_template_name)
            else:
                prompt_folder_name = sanitize_filename(PROMPT_TEMPLATE_NAME)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π –∏–∑ config
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫
            model_dir = os.path.join(output_dir, model_key)
            prompt_dir = os.path.join(model_dir, prompt_folder_name)
            os.makedirs(prompt_dir, exist_ok=True)
            
            analysis_path = os.path.join(prompt_dir, f"gemini_analysis_{timestamp}_reevaluated.json")
            
            with open(analysis_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2)
            print(f"üíæ –ê–Ω–∞–ª–∏–∑ Gemini —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ JSON: {analysis_path}\n")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()

