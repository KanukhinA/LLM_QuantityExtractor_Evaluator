"""
–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π LLM
"""
import torch
import gc
import time
import pandas as pd
import json
from datetime import datetime
from typing import Dict, Any, List, Optional, Callable
import os

from utils import build_prompt3, parse_json_safe, is_valid_json, extract_json_from_response
from metrics import calculate_quality_metrics
from gpu_info import get_gpu_info, get_gpu_memory_usage


class ModelEvaluator:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ LLM –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ
    """
    
    def __init__(self, 
                 dataset_path: str,
                 ground_truth_path: Optional[str] = None,
                 output_dir: str = "results"):
        """
        Args:
            dataset_path: –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É (Excel —Ñ–∞–π–ª)
            ground_truth_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        self.dataset_path = dataset_path
        self.ground_truth_path = ground_truth_path
        self.output_dir = output_dir
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        os.makedirs(output_dir, exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑: {dataset_path}")
        self.df_full = pd.read_excel(dataset_path)
        print(f"   ‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.df_full)} —Å—Ç—Ä–æ–∫, {len(self.df_full.columns)} –∫–æ–ª–æ–Ω–æ–∫")
        print(f"   üìã –ö–æ–ª–æ–Ω–∫–∏: {', '.join(self.df_full.columns.tolist()[:5])}{'...' if len(self.df_full.columns) > 5 else ''}")
        
        # –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω—É–∂–Ω—ã –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤
        self.df = self.df_full.drop(["json", "Unnamed: 0"], axis=1, errors='ignore')
        self.texts = self.df["text"].tolist()
        print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(self.texts)} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\n")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º ground truth –∏–∑ —Ç–æ–≥–æ –∂–µ —Ñ–∞–π–ª–∞ (–∫–æ–ª–æ–Ω–∫–∞ json_parsed)
        self.ground_truths = None
        if "json_parsed" in self.df_full.columns:
            try:
                # json_parsed —É–∂–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π
                self.ground_truths = []
                for j in self.df_full["json_parsed"]:
                    if isinstance(j, dict):
                        self.ground_truths.append(j)
                    elif isinstance(j, str):
                        self.ground_truths.append(parse_json_safe(j))
                    else:
                        self.ground_truths.append({})
                non_empty = sum(1 for gt in self.ground_truths if gt)
                print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.ground_truths)} ground truth –∑–Ω–∞—á–µ–Ω–∏–π –∏–∑ –∫–æ–ª–æ–Ω–∫–∏ json_parsed")
                print(f"      (–ù–µ–ø—É—Å—Ç—ã—Ö: {non_empty}, –ü—É—Å—Ç—ã—Ö: {len(self.ground_truths) - non_empty})\n")
            except Exception as e:
                print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å ground truth –∏–∑ json_parsed: {e}\n")
        elif ground_truth_path and os.path.exists(ground_truth_path):
            # Fallback: –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (—Å—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–±)
            try:
                print(f"   üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ ground truth –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {ground_truth_path}")
                gt_df = pd.read_excel(ground_truth_path)
                if "json" in gt_df.columns:
                    self.ground_truths = [parse_json_safe(str(j)) for j in gt_df["json"]]
                    print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.ground_truths)} ground truth –∑–Ω–∞—á–µ–Ω–∏–π –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n")
            except Exception as e:
                print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å ground truth: {e}\n")
        else:
            print(f"   ‚ö†Ô∏è Ground truth –Ω–µ –Ω–∞–π–¥–µ–Ω (–∫–æ–ª–æ–Ω–∫–∞ json_parsed –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç)\n")
    
    def clear_memory(self):
        """–û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏"""
        print("‚ôªÔ∏è –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ PyTorch...")
        global model, tokenizer
        try:
            del model
        except NameError:
            pass
        try:
            del tokenizer
        except NameError:
            pass
        gc.collect()
        torch.cuda.empty_cache()
        print("‚úÖ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞")
    
    def evaluate_model(self,
                      model_name: str,
                      load_model_func: Callable,
                      generate_func: Callable,
                      hyperparameters: Dict[str, Any],
                      prompt_template: str = None,
                      max_new_tokens: int = 1024,
                      num_retries: int = 2) -> Dict[str, Any]:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ
        
        Args:
            model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            load_model_func: —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ (–¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å (model, tokenizer))
            generate_func: —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (model, tokenizer, prompt) -> response_text
            hyperparameters: —Å–ª–æ–≤–∞—Ä—å —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            prompt_template: —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è build_prompt3)
            max_new_tokens: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
            num_retries: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        
        Returns:
            —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ü–µ–Ω–∫–∏
        """
        print(f"\n{'='*80}")
        print(f"üöÄ –ù–ê–ß–ê–õ–û –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ò")
        print(f"{'='*80}")
        print(f"üìå –ú–æ–¥–µ–ª—å: {model_name}")
        print(f"üìå –î–∞—Ç–∞—Å–µ—Ç: {len(self.texts)} —Ç–µ–∫—Å—Ç–æ–≤")
        print(f"üìå –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        for key, value in hyperparameters.items():
            print(f"   ‚Ä¢ {key}: {value}")
        print(f"{'='*80}\n")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        gpu_info_before = get_gpu_info()
        print(f"üìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û GPU (–¥–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏):")
        print(f"   ‚Ä¢ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {gpu_info_before.get('cuda_available', False)}")
        if gpu_info_before.get('cuda_available'):
            print(f"   ‚Ä¢ –ù–∞–∑–≤–∞–Ω–∏–µ GPU: {gpu_info_before.get('gpu_name', 'N/A')}")
            print(f"   ‚Ä¢ –í–µ—Ä—Å–∏—è CUDA: {gpu_info_before.get('cuda_version', 'N/A')}")
            print(f"   ‚Ä¢ –û–±—â–∞—è –ø–∞–º—è—Ç—å: {gpu_info_before.get('gpu_memory_total_gb', 0):.2f} GB")
            print(f"   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {gpu_info_before.get('gpu_memory_allocated_gb', 0):.2f} GB")
        print()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        print(f"üì¶ –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò...")
        start_load = time.time()
        try:
            model, tokenizer = load_model_func()
            load_time = time.time() - start_load
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥ ({load_time/60:.2f} –º–∏–Ω—É—Ç)")
        except Exception as e:
            import traceback
            error_details = str(e)
            full_traceback = traceback.format_exc()
            
            print(f"\n{'='*80}")
            print(f"–û–®–ò–ë–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò")
            print(f"{'='*80}")
            print(f"–û—à–∏–±–∫–∞: {error_details}")
            print(f"\n–ü–æ–ª–Ω—ã–π traceback:")
            print(f"{'‚îÄ'*80}")
            print(full_traceback)
            print(f"{'‚îÄ'*80}")
            print(f"–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏ —Ç–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –æ—Ç—á—ë—Ç–µ")
            print(f"{'='*80}\n")
            
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏
            self.clear_memory()
            
            return {
                "status": "error",
                "error": f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {error_details}",
                "error_traceback": full_traceback
            }
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
        gpu_info_after = get_gpu_info()
        memory_after_load = get_gpu_memory_usage()
        print(f"üìä –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û GPU (–ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏):")
        print(f"   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {memory_after_load['allocated']:.2f} GB")
        print(f"   ‚Ä¢ –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {memory_after_load['reserved']:.2f} GB")
        print(f"   ‚Ä¢ –î–æ—Å—Ç—É–ø–Ω–æ –ø–∞–º—è—Ç–∏: {memory_after_load['total'] - memory_after_load['allocated']:.2f} GB")
        print()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–º–ø—Ç
        if prompt_template is None:
            prompt_template = build_prompt3
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ
        results = []
        parsing_errors = []
        times = []
        memory_samples = []  # –î–ª—è —Å–±–æ—Ä–∞ –∏–∑–º–µ—Ä–µ–Ω–∏–π –ø–∞–º—è—Ç–∏ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        total_start_time = time.time()
        
        model.eval()
        
        print(f"üîÑ –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–¢–ê–°–ï–¢–ê")
        print(f"{'='*80}")
        print(f"–í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(self.texts)}")
        print(f"{'='*80}\n")
        
        for i, text in enumerate(self.texts):
            prompt = prompt_template(text)
            response_text = ""
            error_msg = None
            
            # –ü–æ–ø—ã—Ç–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            for attempt in range(num_retries):
                try:
                    start_time = time.time()
                    # –ü–µ—Ä–µ–¥–∞–µ–º repetition_penalty –∏–∑ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –µ—Å–ª–∏ –µ—Å—Ç—å
                    repetition_penalty = hyperparameters.get("repetition_penalty")
                    if repetition_penalty is not None:
                        response_text = generate_func(model, tokenizer, prompt, max_new_tokens, repetition_penalty=repetition_penalty)
                    else:
                        response_text = generate_func(model, tokenizer, prompt, max_new_tokens)
                    elapsed = time.time() - start_time
                    times.append(elapsed)
                    
                    # –ò–∑–º–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞)
                    memory_sample = get_gpu_memory_usage()
                    memory_samples.append(memory_sample["allocated"])
                    break
                except Exception as e:
                    error_msg = str(e)
                    print(f"  ‚ö†Ô∏è [{i+1}/{len(self.texts)}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{num_retries}): {error_msg[:100]}")
                    if attempt < num_retries - 1:
                        time.sleep(4 + attempt * 2)
                    else:
                        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ
                        import traceback
                        parsing_errors.append(f"–¢–µ–∫—Å—Ç #{i}: –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ {num_retries} –ø–æ–ø—ã—Ç–æ–∫. –û—à–∏–±–∫–∞: {error_msg}. Traceback: {traceback.format_exc()[:200]}")
            
            if not response_text:
                print(f"  ‚ùå [{i+1}/{len(self.texts)}] –û—Ç–≤–µ—Ç –Ω–µ –ø–æ–ª—É—á–µ–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫")
                if error_msg:
                    print(f"     –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {error_msg[:200]}")
                parsing_errors.append(f"–¢–µ–∫—Å—Ç #{i}: –Ω–µ –ø–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç. –û—à–∏–±–∫–∞: {error_msg if error_msg else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞'}")
                results.append({
                    "text": text,
                    "json": "",
                    "json_parsed": {},
                    "is_valid": False
                })
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON
            json_part = extract_json_from_response(response_text)
            parsed_json = parse_json_safe(json_part)
            is_valid = is_valid_json(json_part)
            
            if not is_valid:
                parsing_errors.append(f"–¢–µ–∫—Å—Ç #{i}: –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON. –û—Ç–≤–µ—Ç: {json_part}")
            
            results.append({
                "text": text,
                "json": json_part,
                "json_parsed": parsed_json,
                "is_valid": is_valid
            })
            
            # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            elapsed_total = time.time() - total_start_time
            avg_time = sum(times) / len(times) if times else 0
            progress_pct = ((i + 1) / len(self.texts)) * 100
            remaining = len(self.texts) - (i + 1)
            eta_seconds = avg_time * remaining if avg_time > 0 else 0
            eta_minutes = eta_seconds / 60
            
            valid_count = sum(1 for r in results if r["is_valid"])
            invalid_count = (i + 1) - valid_count
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è
            if eta_minutes < 1:
                eta_str = f"{eta_seconds:.0f} —Å–µ–∫"
            else:
                eta_str = f"{eta_minutes:.1f} –º–∏–Ω"
            
            # –í—ã–≤–æ–¥–∏–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π —Å—Ç–∞—Ç—É—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            status_line = (
                f"  [{i + 1}/{len(self.texts)}] "
                f"‚úì: {valid_count} ‚úó: {invalid_count} | "
                f"–°–∫–æ—Ä–æ—Å—Ç—å: {avg_time:.2f}—Å/–æ—Ç–≤–µ—Ç | "
                f"–û—Å—Ç–∞–ª–æ—Å—å: ~{eta_str}"
            )
            print(f"\r{status_line}", end="", flush=True)
            
            # –ü–æ–¥—Ä–æ–±–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 —Ç–µ–∫—Å—Ç–æ–≤ –∏–ª–∏ –≤ –∫–æ–Ω—Ü–µ
            if (i + 1) % 10 == 0 or (i + 1) == len(self.texts):
                print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
                print(f"     üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
                print(f"        ‚Ä¢ –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress_pct:.1f}% ({i + 1}/{len(self.texts)})")
                print(f"        ‚Ä¢ –í–∞–ª–∏–¥–Ω—ã—Ö JSON: {valid_count} | –ù–µ–≤–∞–ª–∏–¥–Ω—ã—Ö: {invalid_count}")
                print(f"        ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {avg_time:.3f} —Å–µ–∫/–æ—Ç–≤–µ—Ç")
                print(f"        ‚Ä¢ –ü—Ä–æ—à–ª–æ –≤—Ä–µ–º–µ–Ω–∏: {elapsed_total/60:.1f} –º–∏–Ω | –û—Å—Ç–∞–ª–æ—Å—å: ~{eta_minutes:.1f} –º–∏–Ω")
                print()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        total_time = time.time() - total_start_time
        print(f"\n{'='*80}")
        print(f"üìä –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö")
        print(f"{'='*80}\n")
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö JSON
        invalid_count = sum(1 for r in results if not r["is_valid"])
        valid_count = len(results) - invalid_count
        parsing_error_rate = invalid_count / len(results) if results else 0.0
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        avg_speed = sum(times) / len(times) if times else 0.0
        min_time = min(times) if times else 0.0
        max_time = max(times) if times else 0.0
        total_inference_time = sum(times)
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –≤—Å–µ—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        if memory_samples:
            memory_during_inference_avg = sum(memory_samples) / len(memory_samples)
            memory_during_inference_max = max(memory_samples)
            memory_during_inference_min = min(memory_samples)
        else:
            # Fallback: –∏–∑–º–µ—Ä—è–µ–º —Å–µ–π—á–∞—Å, –µ—Å–ª–∏ –Ω–µ –±—ã–ª–æ –∏–∑–º–µ—Ä–µ–Ω–∏–π
            current_memory = get_gpu_memory_usage()
            memory_during_inference_avg = current_memory["allocated"]
            memory_during_inference_max = current_memory["allocated"]
            memory_during_inference_min = current_memory["allocated"]
        
        # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–µ
        memory_during_inference = {"allocated": memory_during_inference_avg}
        
        print(f"‚è±Ô∏è  –í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø:")
        print(f"   ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time/60:.2f} –º–∏–Ω—É—Ç ({total_time:.2f} —Å–µ–∫—É–Ω–¥)")
        print(f"   ‚Ä¢ –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {total_inference_time/60:.2f} –º–∏–Ω—É—Ç")
        print(f"   ‚Ä¢ –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {load_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞: {avg_speed:.3f} —Å–µ–∫/–æ—Ç–≤–µ—Ç")
        print(f"   ‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {min_time:.3f} —Å–µ–∫")
        print(f"   ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {max_time:.3f} —Å–µ–∫")
        print()
        
        # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–º–ø—Ç–µ
        print(f"üìù –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ù–´–ô –ü–†–û–ú–ü–¢:")
        print(f"   ‚Ä¢ –®–∞–±–ª–æ–Ω: {prompt_template.__name__ if hasattr(prompt_template, '__name__') else str(prompt_template)}")
        print(f"   ‚Ä¢ –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞ (–ø—Ä–∏–º–µ—Ä —Å –ø–µ—Ä–≤—ã–º —Ç–µ–∫—Å—Ç–æ–º):")
        print(f"{'‚îÄ'*80}")
        example_text = self.texts[0] if self.texts else "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞"
        full_prompt_example = prompt_template(example_text)
        # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–º–ø—Ç —Å –æ—Ç—Å—Ç—É–ø–∞–º–∏ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        prompt_lines = full_prompt_example.split('\n')
        for line in prompt_lines[:30]:  # –ü–µ—Ä–≤—ã–µ 30 —Å—Ç—Ä–æ–∫, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –∫–æ–Ω—Å–æ–ª—å
            print(f"   {line}")
        if len(prompt_lines) > 30:
            print(f"   ... (–µ—â—ë {len(prompt_lines) - 30} —Å—Ç—Ä–æ–∫, –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –æ—Ç—á—ë—Ç–µ)")
        print(f"{'‚îÄ'*80}")
        print()
        
        print(f"üíæ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –ü–ê–ú–Ø–¢–ò:")
        print(f"   ‚Ä¢ –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {memory_after_load['allocated']:.2f} GB")
        print(f"   ‚Ä¢ –í–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (—Å—Ä–µ–¥–Ω–µ–µ): {memory_during_inference_avg:.2f} GB")
        print(f"   ‚Ä¢ –í–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–º–∞–∫—Å–∏–º—É–º): {memory_during_inference_max:.2f} GB")
        print(f"   ‚Ä¢ –í–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–º–∏–Ω–∏–º—É–º): {memory_during_inference_min:.2f} GB")
        print(f"   ‚Ä¢ –ò–∑–º–µ–Ω–µ–Ω–∏–µ –æ—Ç –∑–∞–≥—Ä—É–∑–∫–∏: {memory_during_inference_avg - memory_after_load['allocated']:+.2f} GB")
        print()
        
        print(f"üìù –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–†–°–ò–ù–ì–ê JSON:")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(results)}")
        print(f"   ‚Ä¢ –í–∞–ª–∏–¥–Ω—ã—Ö JSON: {valid_count} ({100-parsing_error_rate*100:.1f}%)")
        print(f"   ‚Ä¢ –ù–µ–≤–∞–ª–∏–¥–Ω—ã—Ö JSON: {invalid_count} ({parsing_error_rate*100:.1f}%)")
        print(f"   ‚Ä¢ –û—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞: {len(parsing_errors)}")
        if parsing_errors:
            print(f"\n   üìã –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ ({len(parsing_errors)} –æ—à–∏–±–æ–∫):")
            print(f"   {'‚îÄ'*76}")
            for i, error in enumerate(parsing_errors, 1):
                # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏
                error_display = error[:200] + "..." if len(error) > 200 else error
                print(f"   {i}. {error_display}")
            print(f"   {'‚îÄ'*76}")
        print()
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å ground truth)
        quality_metrics = None
        if self.ground_truths and len(self.ground_truths) == len(results):
            try:
                print(f"üéØ –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö –ö–ê–ß–ï–°–¢–í–ê...")
                predictions = [r["json_parsed"] for r in results]
                quality_metrics = calculate_quality_metrics(predictions, self.ground_truths)
                
                mass_dolya = quality_metrics.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {})
                prochee = quality_metrics.get('–ø—Ä–æ—á–µ–µ', {})
                
                print(f"   ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã—á–∏—Å–ª–µ–Ω—ã:")
                print(f"   üìä –ì—Ä—É–ø–ø–∞ '–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è':")
                print(f"      ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {mass_dolya.get('—Å—Ä–µ–¥–Ω—è—è_—Ç–æ—á–Ω–æ—Å—Ç—å', 0):.2%}")
                print(f"      ‚Ä¢ Precision: {mass_dolya.get('precision', 0):.2%}")
                print(f"      ‚Ä¢ Recall: {mass_dolya.get('recall', 0):.2%}")
                print(f"      ‚Ä¢ F1-score: {mass_dolya.get('f1', 0):.2%}")
                print(f"      ‚Ä¢ TP: {mass_dolya.get('tp', 0)}, FP: {mass_dolya.get('fp', 0)}, FN: {mass_dolya.get('fn', 0)}")
                print(f"      ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–π: {mass_dolya.get('–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—Å—Ä–∞–≤–Ω–µ–Ω–∏–π', 0)}")
                print(f"      ‚Ä¢ –ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫: {len(mass_dolya.get('–æ—à–∏–±–∫–∏', []))}")
                print(f"   üìä –ì—Ä—É–ø–ø–∞ '–ø—Ä–æ—á–µ–µ':")
                print(f"      ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {prochee.get('—Å—Ä–µ–¥–Ω—è—è_—Ç–æ—á–Ω–æ—Å—Ç—å', 0):.2%}")
                print(f"      ‚Ä¢ Precision: {prochee.get('precision', 0):.2%}")
                print(f"      ‚Ä¢ Recall: {prochee.get('recall', 0):.2%}")
                print(f"      ‚Ä¢ F1-score: {prochee.get('f1', 0):.2%}")
                print(f"      ‚Ä¢ TP: {prochee.get('tp', 0)}, FP: {prochee.get('fp', 0)}, FN: {prochee.get('fn', 0)}")
                print(f"      ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–π: {prochee.get('–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—Å—Ä–∞–≤–Ω–µ–Ω–∏–π', 0)}")
                print(f"      ‚Ä¢ –ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫: {len(prochee.get('–æ—à–∏–±–∫–∏', []))}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
        else:
            print(f"   ‚ö†Ô∏è Ground truth –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–ª–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç –ø–æ —Ä–∞–∑–º–µ—Ä—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏")
            if not self.ground_truths:
                print(f"      (Ground truth –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ)")
            elif len(self.ground_truths) != len(results):
                print(f"      (–†–∞–∑–º–µ—Ä—ã –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: GT={len(self.ground_truths)}, Results={len(results)})")
        print()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –æ—Ç—á—ë—Ç
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∫–∞–∫ –ø—Ä–∏–º–µ—Ä
        example_text = self.texts[0] if self.texts else "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
        full_prompt_example = prompt_template(example_text)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        evaluation_result = {
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "model_name": model_name,
            "gpu_info": gpu_info_before,
            "gpu_memory_after_load_gb": memory_after_load["allocated"],
            "gpu_memory_during_inference_gb": memory_during_inference_avg,
            "gpu_memory_during_inference_max_gb": memory_during_inference_max,
            "gpu_memory_during_inference_min_gb": memory_during_inference_min,
            "average_response_time_seconds": avg_speed,
            "parsing_error_rate": parsing_error_rate,
            "parsing_errors_count": len(parsing_errors),
            "quality_metrics": quality_metrics,
            "hyperparameters": hyperparameters,
            "prompt_template": prompt_template.__name__ if hasattr(prompt_template, '__name__') else str(prompt_template),
            "prompt_full_text": full_prompt_example,
            "parsing_errors": parsing_errors,
            "total_samples": len(results),
            "valid_json_count": len(results) - invalid_count,
            "invalid_json_count": invalid_count
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í...")
        self._save_results(evaluation_result, results)
        
        print(f"\n{'='*80}")
        print(f"‚úÖ –û–¶–ï–ù–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        print(f"{'='*80}")
        print(f"üìå –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞:")
        print(f"   ‚Ä¢ –ú–æ–¥–µ–ª—å: {model_name}")
        print(f"   ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(results)}")
        print(f"   ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time/60:.2f} –º–∏–Ω—É—Ç")
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {avg_speed:.3f} —Å–µ–∫/–æ—Ç–≤–µ—Ç")
        print(f"   ‚Ä¢ –û—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞: {parsing_error_rate:.2%} ({invalid_count}/{len(results)})")
        print(f"   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ (—Å—Ä–µ–¥–Ω–µ–µ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞): {memory_during_inference_avg:.2f} GB")
        if quality_metrics:
            mass_acc = quality_metrics.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {}).get('—Å—Ä–µ–¥–Ω—è—è_—Ç–æ—á–Ω–æ—Å—Ç—å', 0)
            prochee_acc = quality_metrics.get('–ø—Ä–æ—á–µ–µ', {}).get('—Å—Ä–µ–¥–Ω—è—è_—Ç–æ—á–Ω–æ—Å—Ç—å', 0)
            print(f"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ '–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è': {mass_acc:.2%}")
            print(f"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ '–ø—Ä–æ—á–µ–µ': {prochee_acc:.2%}")
        print(f"{'='*80}\n")
        
        return evaluation_result
    
    def _save_results(self, evaluation_result: Dict[str, Any], results: List[Dict[str, Any]]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª—ã"""
        timestamp = evaluation_result["timestamp"]
        model_name_safe = evaluation_result["model_name"].replace("/", "_").replace("\\", "_")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        df_results = pd.DataFrame(results)
        csv_path = os.path.join(self.output_dir, f"results_{model_name_safe}_{timestamp}.csv")
        df_results.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"üíæ –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics_path = os.path.join(self.output_dir, f"metrics_{model_name_safe}_{timestamp}.json")
        import json
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation_result, f, ensure_ascii=False, indent=2)
        print(f"üíæ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_path}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â–∏–π —Ñ–∞–π–ª —Å–æ –≤—Å–µ–º–∏ –ø—Ä–æ–≥–æ–Ω–∞–º–∏
        summary_path = os.path.join(self.output_dir, "evaluation_summary.jsonl")
        with open(summary_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(evaluation_result, ensure_ascii=False) + '\n')
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –æ–±—â–∏–π —Ñ–∞–π–ª: {summary_path}")
    
    @staticmethod
    def reevaluate_from_file(
        results_csv_path: str,
        dataset_path: str,
        output_dir: str = "results",
        model_name: str = None
    ) -> Dict[str, Any]:
        """
        –ü–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ CSV —Ñ–∞–π–ª–∞ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏.
        
        Args:
            results_csv_path: –ø—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, results_model_name_timestamp.csv)
            dataset_path: –ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è ground truth
            output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            model_name: –∏–º—è –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None, –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞)
        
        Returns:
            —Å–ª–æ–≤–∞—Ä—å —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        print(f"\n{'='*80}")
        print(f"üîÑ –ü–ï–†–ï–û–¶–ï–ù–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò–ó –§–ê–ô–õ–ê")
        print(f"{'='*80}\n")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ CSV
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑: {results_csv_path}")
        if not os.path.exists(results_csv_path):
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {results_csv_path}")
        
        df_results = pd.read_csv(results_csv_path)
        print(f"   ‚Ä¢ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df_results)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_columns = ['text', 'json_parsed']
        missing_columns = [col for col in required_columns if col not in df_results.columns]
        if missing_columns:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º ground truth –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ ground truth –∏–∑: {dataset_path}")
        df_full = pd.read_excel(dataset_path)
        
        if "json_parsed" not in df_full.columns:
            raise ValueError("–í –¥–∞—Ç–∞—Å–µ—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'json_parsed' —Å ground truth")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º ground truth
        ground_truths = []
        for idx, row in df_full.iterrows():
            gt = row.get("json_parsed", {})
            if isinstance(gt, str):
                try:
                    gt = json.loads(gt)
                except:
                    gt = parse_json_safe(gt)
            ground_truths.append(gt if isinstance(gt, dict) else {})
        
        print(f"   ‚Ä¢ –ó–∞–≥—Ä—É–∂–µ–Ω–æ ground truth –∑–∞–ø–∏—Å–µ–π: {len(ground_truths)}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        predictions = []
        for idx, row in df_results.iterrows():
            pred = row.get("json_parsed", {})
            if isinstance(pred, str):
                try:
                    pred = json.loads(pred)
                except:
                    pred = parse_json_safe(pred)
            predictions.append(pred if isinstance(pred, dict) else {})
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
        if len(predictions) != len(ground_truths):
            print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ({len(predictions)}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º ground truth ({len(ground_truths)})")
            min_len = min(len(predictions), len(ground_truths))
            predictions = predictions[:min_len]
            ground_truths = ground_truths[:min_len]
            print(f"   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {min_len} –∑–∞–ø–∏—Å–µ–π")
        
        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        print(f"\nüìä –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö –ö–ê–ß–ï–°–¢–í–ê...")
        try:
            quality_metrics = calculate_quality_metrics(predictions, ground_truths)
            print(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—à–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω—ã")
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
            import traceback
            traceback.print_exc()
            quality_metrics = None
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø–∞—Ä—Å–∏–Ω–≥—É
        valid_count = sum(1 for p in predictions if p and isinstance(p, dict))
        invalid_count = len(predictions) - valid_count
        parsing_error_rate = invalid_count / len(predictions) if predictions else 0.0
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º parsing errors –∏–∑ CSV, –µ—Å–ª–∏ –µ—Å—Ç—å
        parsing_errors = []
        if "json" in df_results.columns:
            for idx, row in df_results.iterrows():
                json_str = row.get("json", "")
                is_valid = row.get("is_valid", False)
                if not is_valid and json_str:
                    parsing_errors.append(f"–¢–µ–∫—Å—Ç #{idx}: –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON. –û—Ç–≤–µ—Ç: {json_str}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ
        if model_name is None:
            filename = os.path.basename(results_csv_path)
            # –§–æ—Ä–º–∞—Ç: results_model_name_timestamp.csv
            parts = filename.replace("results_", "").replace(".csv", "").split("_")
            if len(parts) >= 2:
                # –ë–µ—Ä–µ–º –≤—Å–µ —á–∞—Å—Ç–∏ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π (timestamp)
                model_name = "_".join(parts[:-1])
            else:
                model_name = "unknown"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        evaluation_result = {
            "timestamp": timestamp,
            "model_name": model_name,
            "reevaluated_from": results_csv_path,
            "parsing_error_rate": parsing_error_rate,
            "parsing_errors_count": len(parsing_errors),
            "quality_metrics": quality_metrics,
            "parsing_errors": parsing_errors,
            "total_samples": len(predictions),
            "valid_json_count": valid_count,
            "invalid_json_count": invalid_count
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        print(f"\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –û–ë–ù–û–í–õ–ï–ù–ù–´–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í...")
        os.makedirs(output_dir, exist_ok=True)
        
        model_name_safe = model_name.replace("/", "_").replace("\\", "_")
        metrics_path = os.path.join(output_dir, f"metrics_{model_name_safe}_{timestamp}_reevaluated.json")
        
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation_result, f, ensure_ascii=False, indent=2)
        print(f"üíæ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_path}")
        
        # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
        print(f"\n{'='*80}")
        print(f"‚úÖ –ü–ï–†–ï–û–¶–ï–ù–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print(f"{'='*80}")
        print(f"üìå –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞:")
        print(f"   ‚Ä¢ –ú–æ–¥–µ–ª—å: {model_name}")
        print(f"   ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(predictions)}")
        print(f"   ‚Ä¢ –û—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞: {parsing_error_rate:.2%} ({invalid_count}/{len(predictions)})")
        if quality_metrics:
            mass_acc = quality_metrics.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {}).get('—Å—Ä–µ–¥–Ω—è—è_—Ç–æ—á–Ω–æ—Å—Ç—å', 0)
            prochee_acc = quality_metrics.get('–ø—Ä–æ—á–µ–µ', {}).get('—Å—Ä–µ–¥–Ω—è—è_—Ç–æ—á–Ω–æ—Å—Ç—å', 0)
            mass_f1 = quality_metrics.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {}).get('f1', 0)
            prochee_f1 = quality_metrics.get('–ø—Ä–æ—á–µ–µ', {}).get('f1', 0)
            print(f"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ '–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è': Accuracy={mass_acc:.2%}, F1={mass_f1:.2%}")
            print(f"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ '–ø—Ä–æ—á–µ–µ': Accuracy={prochee_acc:.2%}, F1={prochee_f1:.2%}")
        print(f"{'='*80}\n")
        
        return evaluation_result

