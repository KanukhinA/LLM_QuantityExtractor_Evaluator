"""
–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–∞–º–∏: –ø–æ–∏—Å–∫, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ, —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–∞–º–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö –∫–æ–¥–∞.
"""
import os
import json
import glob
import re
from typing import List, Optional, Dict, Any
import pandas as pd


class FileManager:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–∞–º–∏: –ø–æ–∏—Å–∫, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ, —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π.
    """
    
    def __init__(self, base_dir: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç FileManager.
        
        Args:
            base_dir: –±–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.base_dir = base_dir
    
    @staticmethod
    def sanitize_filename(name: str) -> str:
        """
        –°–∞–Ω–∏—Ç–∏–∑–∏—Ä—É–µ—Ç –∏–º—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞.
        –ó–∞–º–µ–Ω—è–µ—Ç –≤—Å–µ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è.
        
        Args:
            name: –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–º—è
            
        Returns:
            –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è –¥–ª—è —Ñ–∞–π–ª–∞
        """
        if not name:
            return "unknown"
        
        # –ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤ –≤ Windows –∏ Linux: < > : " / \ | ? *
        # –¢–∞–∫–∂–µ –∑–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –¥—Ä—É–≥–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        invalid_chars = r'[<>:"/\\|?*\s]'
        sanitized = re.sub(invalid_chars, '_', name)
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
        sanitized = re.sub(r'_+', '_', sanitized)
        # –£–¥–∞–ª—è–µ–º –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        sanitized = sanitized.strip('_')
        
        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏–∏ –∏–º—è –ø—É—Å—Ç–æ–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º "unknown"
        if not sanitized:
            return "unknown"
        
        return sanitized
    
    def ensure_directory(self, directory_path: str) -> None:
        """
        –°–æ–∑–¥–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
        
        Args:
            directory_path: –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        """
        if self.base_dir and not os.path.isabs(directory_path):
            directory_path = os.path.join(self.base_dir, directory_path)
        os.makedirs(directory_path, exist_ok=True)
    
    def build_path(self, *parts: str) -> str:
        """
        –°—Ç—Ä–æ–∏—Ç –ø—É—Ç—å –∏–∑ —á–∞—Å—Ç–µ–π.
        
        Args:
            *parts: —á–∞—Å—Ç–∏ –ø—É—Ç–∏
            
        Returns:
            –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –ø—É—Ç—å
        """
        path = os.path.join(*parts)
        if self.base_dir and not os.path.isabs(path):
            path = os.path.join(self.base_dir, path)
        return path
    
    def get_basename(self, file_path: str) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑–æ–≤–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –ø—É—Ç–∏.
        
        Args:
            file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            –±–∞–∑–æ–≤–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        """
        return os.path.basename(file_path)
    
    def get_dirname(self, file_path: str) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Ñ–∞–π–ª–∞ –∏–∑ –ø—É—Ç–∏.
        
        Args:
            file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ñ–∞–π–ª–∞
        """
        return os.path.dirname(file_path)
    
    def get_name_without_ext(self, file_path: str) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è.
        
        Args:
            file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        """
        basename = self.get_basename(file_path)
        return os.path.splitext(basename)[0]
    
    def file_exists(self, file_path: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞.
        
        Args:
            file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            True, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∏–Ω–∞—á–µ False
        """
        if self.base_dir and not os.path.isabs(file_path):
            file_path = os.path.join(self.base_dir, file_path)
        return os.path.exists(file_path) and os.path.isfile(file_path)
    
    def find_files(self, pattern: str, directory: Optional[str] = None, recursive: bool = False) -> List[str]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É.
        
        Args:
            pattern: –ø–∞—Ç—Ç–µ—Ä–Ω –ø–æ–∏—Å–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "*.json", "metrics_*.json")
            directory: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–∫—É—â–∞—è –∏–ª–∏ base_dir)
            recursive: –∏—Å–∫–∞—Ç—å —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
            
        Returns:
            —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        """
        if directory:
            search_pattern = self.build_path(directory, pattern)
        else:
            search_pattern = pattern
        
        if recursive:
            # –î–ª—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º **
            if not pattern.startswith("**"):
                search_pattern = self.build_path(directory or ".", "**", pattern)
        
        files = glob.glob(search_pattern, recursive=recursive)
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
        files.sort(key=os.path.getmtime, reverse=True)
        return files
    
    def save_csv(self, dataframe: pd.DataFrame, file_path: str, encoding: str = 'utf-8-sig', index: bool = False) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç DataFrame –≤ CSV —Ñ–∞–π–ª.
        
        Args:
            dataframe: DataFrame –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            encoding: –∫–æ–¥–∏—Ä–æ–≤–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'utf-8-sig')
            index: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –∏–Ω–¥–µ–∫—Å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)
        """
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        directory = self.get_dirname(file_path)
        if directory:
            self.ensure_directory(directory)
        
        dataframe.to_csv(file_path, index=index, encoding=encoding)
    
    def save_json(self, data: Dict[str, Any], file_path: str, encoding: str = 'utf-8', indent: int = 2, ensure_ascii: bool = False) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ JSON —Ñ–∞–π–ª.
        
        Args:
            data: –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (—Å–ª–æ–≤–∞—Ä—å)
            file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            encoding: –∫–æ–¥–∏—Ä–æ–≤–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'utf-8')
            indent: –æ—Ç—Å—Ç—É–ø –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2)
            ensure_ascii: —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –ª–∏ –Ω–µ-ASCII —Å–∏–º–≤–æ–ª—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)
        """
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        directory = self.get_dirname(file_path)
        if directory:
            self.ensure_directory(directory)
        
        with open(file_path, 'w', encoding=encoding) as f:
            json.dump(data, f, ensure_ascii=ensure_ascii, indent=indent)
    
    def load_json(self, file_path: str, encoding: str = 'utf-8') -> Optional[Dict[str, Any]]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞.
        
        Args:
            file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            encoding: –∫–æ–¥–∏—Ä–æ–≤–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'utf-8')
            
        Returns:
            –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ None, –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞
        """
        if not self.file_exists(file_path):
            return None
        
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                return json.load(f)
        except Exception:
            return None
    
    def save_text(self, content: str, file_path: str, encoding: str = 'utf-8') -> None:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ñ–∞–π–ª.
        
        Args:
            content: —Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            encoding: –∫–æ–¥–∏—Ä–æ–≤–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'utf-8')
        """
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        directory = self.get_dirname(file_path)
        if directory:
            self.ensure_directory(directory)
        
        with open(file_path, 'w', encoding=encoding) as f:
            f.write(content)
    
    def append_text(self, content: str, file_path: str, encoding: str = 'utf-8') -> None:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞.
        
        Args:
            content: —Ç–µ–∫—Å—Ç –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
            file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            encoding: –∫–æ–¥–∏—Ä–æ–≤–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'utf-8')
        """
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        directory = self.get_dirname(file_path)
        if directory:
            self.ensure_directory(directory)
        
        with open(file_path, 'a', encoding=encoding) as f:
            f.write(content)
    
    def save_evaluation_results(
        self,
        evaluation_result: Dict[str, Any],
        results: List[Dict[str, Any]],
        output_dir: str,
        timestamp: str
    ) -> Dict[str, str]:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –≤ —Ñ–∞–π–ª—ã.
        –ò–Ω–∫–∞–ø—Å—É–ª–∏—Ä—É–µ—Ç –≤—Å—é –ª–æ–≥–∏–∫—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: CSV, JSON –º–µ—Ç—Ä–∏–∫–∏, raw –º–µ—Ç—Ä–∏–∫–∏, –æ—à–∏–±–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞, summary.
        
        Args:
            evaluation_result: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ü–µ–Ω–∫–∏
            results: —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            output_dir: –±–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            timestamp: –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
            
        Returns:
            —Å–ª–æ–≤–∞—Ä—å —Å –ø—É—Ç—è–º–∏ –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
        """
        import re
        import json
        import pandas as pd
        
        saved_files = {}
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º model_key
        model_key = evaluation_result.get("model_key")
        if model_key:
            model_key = FileManager.sanitize_filename(model_key)
            model_key = re.sub(r'_\d{8}(_\d{4})?$', '', model_key)
        else:
            model_name = evaluation_result.get("model_name", "")
            model_key = re.sub(r'_\d{8}(_\d{4})?$', '', model_name)
            model_key = FileManager.sanitize_filename(model_key)
            if "_" in model_key:
                parts = model_key.split("_")
                non_date_parts = [p for p in parts if not re.match(r'^\d{8}(_\d{4})?$', p)]
                if non_date_parts:
                    if len(non_date_parts) > 1 and len(non_date_parts[-1]) <= 3:
                        model_key = non_date_parts[-2] if len(non_date_parts[-2]) > 3 else non_date_parts[-1]
                    else:
                        model_key = non_date_parts[-1]
        
        model_key = re.sub(r'_\d{8}(_\d{4})?$', '', model_key)
        if not model_key:
            model_key = "unknown_model"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–¥–ø–∞–ø–∫–∏ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        multi_agent_mode = evaluation_result.get("multi_agent_mode")
        prompt_template_name = evaluation_result.get("prompt_template", "unknown")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ structured_output –∏ outlines
        hyperparameters = evaluation_result.get("hyperparameters", {})
        structured_output = hyperparameters.get("structured_output", False)
        use_outlines = hyperparameters.get("use_outlines", False)
        
        if multi_agent_mode:
            prompt_folder_name = FileManager.sanitize_filename(multi_agent_mode)
        else:
            prompt_folder_name = FileManager.sanitize_filename(prompt_template_name)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–∂–∏–º–∞—Ö structured_output –∏ outlines –≤ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏
        mode_suffixes = []
        if use_outlines:
            mode_suffixes.append("outlines")
        elif structured_output:
            mode_suffixes.append("structured")
        
        if mode_suffixes:
            prompt_folder_name = f"{prompt_folder_name}_{'_'.join(mode_suffixes)}"
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫
        model_dir = self.build_path(output_dir, model_key)
        prompt_dir = self.build_path(model_dir, prompt_folder_name)
        self.ensure_directory(prompt_dir)
        
        model_name_for_file = FileManager.sanitize_filename(evaluation_result.get("model_name", "unknown"))
        
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        df_results = pd.DataFrame(results)
        csv_path = self.build_path(prompt_dir, f"results_{model_name_for_file}_{timestamp}.csv")
        self.save_csv(df_results, csv_path)
        saved_files["csv"] = csv_path
        print(f"üíæ –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}")
        
        # 2. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ JSON
        evaluation_result_for_json = {}
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è
        for key in ["timestamp", "model_name", "model_key", "interrupted", "total_count", "total_samples"]:
            if key in evaluation_result:
                evaluation_result_for_json[key] = evaluation_result[key]
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        for key in ["valid_json_count", "invalid_json_count", "parsing_error_rate", "parsing_errors_count", "validation_stats"]:
            if key in evaluation_result:
                evaluation_result_for_json[key] = evaluation_result[key]
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        if "quality_metrics" in evaluation_result:
            evaluation_result_for_json["quality_metrics"] = evaluation_result["quality_metrics"]
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–ø–µ—Ä–µ–¥ –æ—à–∏–±–∫–∞–º–∏)
        for key in ["gpu_info", "gpu_memory_after_load_gb", "gpu_memory_during_inference_gb", 
                    "gpu_memory_during_inference_max_gb", "gpu_memory_during_inference_min_gb",
                    "average_response_time_seconds", "api_model"]:
            if key in evaluation_result:
                evaluation_result_for_json[key] = evaluation_result[key]
        
        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—è (–∫—Ä–æ–º–µ raw_output_metrics, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ)
        for key in evaluation_result:
            if key not in evaluation_result_for_json and key != "raw_output_metrics":
                evaluation_result_for_json[key] = evaluation_result[key]
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –æ—à–∏–±–∫–∏
        quality_metrics_for_json = evaluation_result_for_json.get("quality_metrics")
        all_quality_errors = []
        if quality_metrics_for_json:
            for group in ["–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è", "–ø—Ä–æ—á–µ–µ"]:
                if group in quality_metrics_for_json:
                    group_errors = quality_metrics_for_json[group].get("–≤—Å–µ_–æ—à–∏–±–∫–∏", [])
                    for error in group_errors:
                        if isinstance(error, dict):
                            all_quality_errors.append(error)
                        else:
                            all_quality_errors.append({"error": str(error)})
                    quality_metrics_for_json[group].pop("–≤—Å–µ_–æ—à–∏–±–∫–∏", None)
                    quality_metrics_for_json[group].pop("–æ—à–∏–±–∫–∏", None)
        
        parsing_errors_list = evaluation_result_for_json.get("parsing_errors", [])
        all_errors = parsing_errors_list + all_quality_errors
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø–æ —Ç–µ–∫—Å—Ç–∞–º
        errors_by_text = {}
        for error in all_errors:
            if isinstance(error, dict):
                text_idx = error.get("text_index", 0)
                if text_idx not in errors_by_text:
                    errors_by_text[text_idx] = {
                        "text_index": text_idx,
                        "text": error.get("text", ""),
                        "response": error.get("response", ""),
                        "prompt": error.get("prompt", ""),
                        "errors": []
                    }
                if error.get("error"):
                    errors_by_text[text_idx]["errors"].append(error.get("error"))
                if error.get("text") and not errors_by_text[text_idx]["text"]:
                    errors_by_text[text_idx]["text"] = error.get("text")
                if error.get("response") and not errors_by_text[text_idx]["response"]:
                    errors_by_text[text_idx]["response"] = error.get("response")
                if error.get("prompt") and not errors_by_text[text_idx]["prompt"]:
                    errors_by_text[text_idx]["prompt"] = error.get("prompt")
        
        evaluation_result_for_json["–æ—à–∏–±–∫–∏"] = list(errors_by_text.values())
        
        # –£–¥–∞–ª—è–µ–º parsing_errors, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ —É–∂–µ –≤–∫–ª—é—á–µ–Ω—ã –≤ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–µ –ø–æ–ª–µ "–æ—à–∏–±–∫–∏"
        evaluation_result_for_json.pop("parsing_errors", None)
        
        metrics_path = self.build_path(prompt_dir, f"metrics_{model_name_for_file}_{timestamp}.json")
        self.save_json(evaluation_result_for_json, metrics_path)
        saved_files["metrics"] = metrics_path
        print(f"üíæ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_path}")
        print(f"   üìã –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {list(evaluation_result.get('hyperparameters', {}).keys())}")
        
        # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º raw –º–µ—Ç—Ä–∏–∫–∏
        raw_output_metrics = evaluation_result.get("raw_output_metrics")
        if raw_output_metrics:
            raw_metrics_path = self.build_path(prompt_dir, f"raw_metrics_{model_name_for_file}_{timestamp}.json")
            raw_metrics_for_json = {}
            
            if "validation" in raw_output_metrics:
                validation_data = raw_output_metrics["validation"]
                raw_metrics_for_json["validation"] = {
                    "valid_count": validation_data.get("valid_count", 0),
                    "invalid_count": validation_data.get("invalid_count", 0),
                    "validation_rate": validation_data.get("validation_rate", 0.0),
                    "validation_errors": validation_data.get("all_validation_errors", [])[:10] if "all_validation_errors" in validation_data else validation_data.get("validation_errors", [])[:10]
                }
            
            for group in ["–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è", "–ø—Ä–æ—á–µ–µ"]:
                if group in raw_output_metrics:
                    group_data = raw_output_metrics[group]
                    raw_metrics_for_json[group] = {
                        "accuracy": group_data.get("accuracy", 0.0),
                        "precision": group_data.get("precision", 0.0),
                        "recall": group_data.get("recall", 0.0),
                        "f1": group_data.get("f1", 0.0),
                        "tp": group_data.get("tp", 0),
                        "fp": group_data.get("fp", 0),
                        "fn": group_data.get("fn", 0),
                        "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—Å—Ä–∞–≤–Ω–µ–Ω–∏–π": group_data.get("–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—Å—Ä–∞–≤–Ω–µ–Ω–∏–π", group_data.get("tp", 0) + group_data.get("fp", 0) + group_data.get("fn", 0)),
                        "–æ—à–∏–±–∫–∏": group_data.get("–≤—Å–µ_–æ—à–∏–±–∫–∏", [])[:10] if "–≤—Å–µ_–æ—à–∏–±–∫–∏" in group_data else group_data.get("–æ—à–∏–±–∫–∏", [])[:10]
                    }
            
            try:
                self.save_json(raw_metrics_for_json, raw_metrics_path)
                saved_files["raw_metrics"] = raw_metrics_path
                print(f"üíæ Raw –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {raw_metrics_path}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ raw –º–µ—Ç—Ä–∏–∫: {e}")
        else:
            print(f"‚ö†Ô∏è Raw –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ evaluation_result, —Ñ–∞–π–ª raw_metrics –Ω–µ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω")
        
        # 4. –û–±–Ω–æ–≤–ª—è–µ–º summary —Ñ–∞–π–ª
        summary_path = self.build_path(output_dir, "evaluation_summary.jsonl")
        self.append_text(json.dumps(evaluation_result, ensure_ascii=False) + '\n', summary_path)
        saved_files["summary"] = summary_path
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –æ–±—â–∏–π —Ñ–∞–π–ª: {summary_path}")
        
        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—à–∏–±–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_metrics = evaluation_result.get("quality_metrics")
        if quality_metrics:
            errors_path = self.build_path(prompt_dir, f"quality_errors_{model_name_for_file}_{timestamp}.txt")
            error_content = f"–û—à–∏–±–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –º–æ–¥–µ–ª–∏: {evaluation_result['model_name']}\n"
            error_content += f"–î–∞—Ç–∞: {timestamp}\n"
            error_content += f"{'='*80}\n\n"
            
            for group_name, group_key in [("–ú–ê–°–°–û–í–ê–Ø –î–û–õ–Ø", "–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è"), ("–ü–†–û–ß–ï–ï", "–ø—Ä–æ—á–µ–µ")]:
                group = quality_metrics.get(group_key, {})
                errors = group.get('–≤—Å–µ_–æ—à–∏–±–∫–∏', group.get('–æ—à–∏–±–∫–∏', []))
                error_content += f"–û–®–ò–ë–ö–ò –ö–ê–ß–ï–°–¢–í–ê: {group_name}\n"
                if errors:
                    error_content += f"–í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {len(errors)}\n"
                    error_content += f"{'‚îÄ'*80}\n"
                    for i, error in enumerate(errors, 1):
                        error_content += f"{i}. {error}\n"
                    error_content += f"\n"
                else:
                    error_content += f"–û—à–∏–±–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.\n\n"
            
            self.save_text(error_content, errors_path)
            saved_files["quality_errors"] = errors_path
            print(f"üíæ –û—à–∏–±–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {errors_path}")
        
        return saved_files
    
    def save_reevaluation_results(
        self,
        evaluation_result: Dict[str, Any],
        results_csv_path: str,
        df_results: pd.DataFrame,
        predictions: List[Dict[str, Any]],
        quality_metrics: Optional[Dict[str, Any]],
        raw_output_metrics: Optional[Dict[str, Any]],
        timestamp: str,
        model_name: str
    ) -> Dict[str, str]:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∏ –≤ —Ñ–∞–π–ª—ã.
        –ò–Ω–∫–∞–ø—Å—É–ª–∏—Ä—É–µ—Ç –≤—Å—é –ª–æ–≥–∏–∫—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è reevaluation.
        
        Args:
            evaluation_result: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∏
            results_csv_path: –ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É CSV —Ñ–∞–π–ª—É
            df_results: DataFrame —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            predictions: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            quality_metrics: –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            raw_output_metrics: raw –º–µ—Ç—Ä–∏–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            timestamp: –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
            model_name: –∏–º—è –º–æ–¥–µ–ª–∏
            
        Returns:
            —Å–ª–æ–≤–∞—Ä—å —Å –ø—É—Ç—è–º–∏ –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
        """
        import json
        import pandas as pd
        
        saved_files = {}
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞–ø–∫—É –∏—Å—Ö–æ–¥–Ω–æ–≥–æ CSV —Ñ–∞–π–ª–∞
        csv_dir = self.get_dirname(os.path.abspath(results_csv_path))
        self.ensure_directory(csv_dir)
        
        csv_name_without_ext = self.get_name_without_ext(results_csv_path)
        model_name_for_file = FileManager.sanitize_filename(model_name)
        
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        results_for_csv = []
        for idx, row in df_results.iterrows():
            pred = predictions[idx] if idx < len(predictions) else {}
            result_row = {
                "text": row.get("text", ""),
                "json": row.get("json", ""),
                "json_parsed": json.dumps(pred, ensure_ascii=False) if pred else "",
                "is_valid": bool(pred and isinstance(pred, dict) and len(pred) > 0)
            }
            for col in ["raw_output", "raw_validation", "parsed_validation"]:
                if col in df_results.columns:
                    result_row[col] = row.get(col, "")
            results_for_csv.append(result_row)
        
        df_results_reevaluated = pd.DataFrame(results_for_csv)
        csv_path = self.build_path(csv_dir, f"{csv_name_without_ext}_reevaluated_{timestamp}.csv")
        self.save_csv(df_results_reevaluated, csv_path)
        saved_files["csv"] = csv_path
        print(f"üíæ –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}")
        
        # 2. –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª –º–µ—Ç—Ä–∏–∫
        metrics_file_pattern = "metrics_*.json"
        metrics_files = self.find_files(metrics_file_pattern, csv_dir)
        original_metrics_files = [f for f in metrics_files if "_reevaluated" not in f]
        
        if not original_metrics_files:
            parent_dir = self.get_dirname(csv_dir)
            if parent_dir and parent_dir != csv_dir:
                metrics_files = self.find_files(metrics_file_pattern, parent_dir, recursive=True)
                original_metrics_files = [f for f in metrics_files if "_reevaluated" not in f]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –º–µ—Ç—Ä–∏–∫
        if original_metrics_files:
            original_metrics = self.load_json(original_metrics_files[-1])
            if original_metrics:
                model_key = original_metrics.get("model_key")
                if model_key:
                    evaluation_result["model_key"] = model_key
        
        # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ JSON
        if csv_name_without_ext.startswith("results_"):
            metrics_base_name = csv_name_without_ext.replace("results_", "metrics_", 1)
        else:
            metrics_base_name = f"metrics_{csv_name_without_ext}"
        
        metrics_path = self.build_path(csv_dir, f"{metrics_base_name}_reevaluated_{timestamp}.json")
        
        evaluation_result_for_json = {}
        for key in ["timestamp", "model_name", "model_key", "interrupted", "total_count", "total_samples"]:
            if key in evaluation_result:
                evaluation_result_for_json[key] = evaluation_result[key]
        
        for key in ["valid_json_count", "invalid_json_count", "parsing_error_rate", "parsing_errors_count", "validation_stats"]:
            if key in evaluation_result:
                evaluation_result_for_json[key] = evaluation_result[key]
        
        if "quality_metrics" in evaluation_result:
            evaluation_result_for_json["quality_metrics"] = evaluation_result["quality_metrics"]
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–ø–µ—Ä–µ–¥ –æ—à–∏–±–∫–∞–º–∏)
        for key in ["gpu_info", "gpu_memory_after_load_gb", "gpu_memory_during_inference_gb", 
                    "gpu_memory_during_inference_max_gb", "gpu_memory_during_inference_min_gb",
                    "average_response_time_seconds", "api_model"]:
            if key in evaluation_result:
                evaluation_result_for_json[key] = evaluation_result[key]
        
        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—è (–∫—Ä–æ–º–µ raw_output_metrics, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ)
        for key in evaluation_result:
            if key not in evaluation_result_for_json and key != "raw_output_metrics":
                evaluation_result_for_json[key] = evaluation_result[key]
        
        quality_metrics_for_json = evaluation_result_for_json.get("quality_metrics")
        if quality_metrics_for_json:
            for group in ["–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è", "–ø—Ä–æ—á–µ–µ"]:
                if group in quality_metrics_for_json:
                    quality_metrics_for_json[group].pop("–≤—Å–µ_–æ—à–∏–±–∫–∏", None)
        
        self.save_json(evaluation_result_for_json, metrics_path)
        saved_files["metrics"] = metrics_path
        print(f"üíæ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_path}")
        
        # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º raw –º–µ—Ç—Ä–∏–∫–∏
        if raw_output_metrics:
            if csv_name_without_ext.startswith("results_"):
                raw_metrics_base_name = csv_name_without_ext.replace("results_", "raw_metrics_", 1)
            else:
                raw_metrics_base_name = f"raw_metrics_{csv_name_without_ext}"
            
            raw_metrics_path = self.build_path(csv_dir, f"{raw_metrics_base_name}_reevaluated_{timestamp}.json")
            raw_metrics_for_json = {}
            
            if "validation" in raw_output_metrics:
                validation_data = raw_output_metrics["validation"]
                raw_metrics_for_json["validation"] = {
                    "valid_count": validation_data.get("valid_count", 0),
                    "invalid_count": validation_data.get("invalid_count", 0),
                    "validation_rate": validation_data.get("validation_rate", 0.0),
                    "validation_errors": validation_data.get("all_validation_errors", [])[:10] if "all_validation_errors" in validation_data else validation_data.get("validation_errors", [])[:10]
                }
            
            for group in ["–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è", "–ø—Ä–æ—á–µ–µ"]:
                if group in raw_output_metrics:
                    group_data = raw_output_metrics[group]
                    raw_metrics_for_json[group] = {
                        "accuracy": group_data.get("accuracy", 0.0),
                        "precision": group_data.get("precision", 0.0),
                        "recall": group_data.get("recall", 0.0),
                        "f1": group_data.get("f1", 0.0),
                        "tp": group_data.get("tp", 0),
                        "fp": group_data.get("fp", 0),
                        "fn": group_data.get("fn", 0),
                        "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—Å—Ä–∞–≤–Ω–µ–Ω–∏–π": group_data.get("–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—Å—Ä–∞–≤–Ω–µ–Ω–∏–π", group_data.get("tp", 0) + group_data.get("fp", 0) + group_data.get("fn", 0)),
                        "–æ—à–∏–±–∫–∏": group_data.get("–≤—Å–µ_–æ—à–∏–±–∫–∏", [])[:10] if "–≤—Å–µ_–æ—à–∏–±–∫–∏" in group_data else group_data.get("–æ—à–∏–±–∫–∏", [])[:10]
                    }
            
            self.save_json(raw_metrics_for_json, raw_metrics_path)
            saved_files["raw_metrics"] = raw_metrics_path
            print(f"üíæ Raw –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {raw_metrics_path}")
        
        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—à–∏–±–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        if quality_metrics:
            if csv_name_without_ext.startswith("results_"):
                errors_base_name = csv_name_without_ext.replace("results_", "quality_errors_", 1)
            else:
                errors_base_name = f"quality_errors_{csv_name_without_ext}"
            
            errors_path = self.build_path(csv_dir, f"{errors_base_name}_reevaluated_{timestamp}.txt")
            error_content = f"–û—à–∏–±–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –º–æ–¥–µ–ª–∏: {model_name}\n"
            error_content += f"–î–∞—Ç–∞: {timestamp}\n"
            error_content += f"–ü–µ—Ä–µ–æ—Ü–µ–Ω–µ–Ω–æ –∏–∑: {results_csv_path}\n"
            error_content += f"{'='*80}\n\n"
            
            for group_name, group_key in [("–ú–ê–°–°–û–í–ê–Ø –î–û–õ–Ø", "–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è"), ("–ü–†–û–ß–ï–ï", "–ø—Ä–æ—á–µ–µ")]:
                group = quality_metrics.get(group_key, {})
                errors = group.get('–≤—Å–µ_–æ—à–∏–±–∫–∏', group.get('–æ—à–∏–±–∫–∏', []))
                error_content += f"–û–®–ò–ë–ö–ò –ö–ê–ß–ï–°–¢–í–ê: {group_name}\n"
                if errors:
                    error_content += f"–í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {len(errors)}\n"
                    error_content += f"{'‚îÄ'*80}\n"
                    for i, error in enumerate(errors, 1):
                        error_content += f"{i}. {error}\n"
                    error_content += f"\n"
                else:
                    error_content += f"–û—à–∏–±–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.\n\n"
            
            self.save_text(error_content, errors_path)
            saved_files["quality_errors"] = errors_path
            print(f"üíæ –û—à–∏–±–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {errors_path}")
        
        return saved_files