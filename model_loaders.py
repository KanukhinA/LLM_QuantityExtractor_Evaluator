"""
–§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
"""
import torch
import os
import warnings
import time
from transformers import AutoTokenizer, AutoModelForCausalLM, Gemma3ForCausalLM, AutoProcessor, AutoModelForSeq2SeqLM, AutoModelForImageTextToText, T5ForConditionalGeneration, T5Tokenizer
from typing import Tuple, Any, Optional
from config import HF_TOKEN, GEMINI_API_KEY, USE_FLASH_ATTENTION_2

# –ò–º–ø–æ—Ä—Ç –¥–ª—è API –º–æ–¥–µ–ª–µ–π
try:
    from google import genai
    GENAI_AVAILABLE = True
except ImportError:
    genai = None
    GENAI_AVAILABLE = False

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –Ω–µ—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –∫–ª—é—á–∞—Ö –≤ rope_parameters –¥–ª—è yarn
warnings.filterwarnings("ignore", message=".*Unrecognized keys in `rope_parameters`.*")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
HF_HUB_DOWNLOAD_TIMEOUT = int(os.environ.get("HF_HUB_DOWNLOAD_TIMEOUT", "300"))  # 5 –º–∏–Ω—É—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é


def _get_flash_attn_kwargs() -> dict:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç kwargs –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Flash Attention 2 –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏.
    –ï—Å–ª–∏ USE_FLASH_ATTENTION_2 –≤–∫–ª—é—á–µ–Ω –∏ –ø–∞–∫–µ—Ç flash-attn –¥–æ—Å—Ç—É–ø–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç
    {"attn_implementation": "flash_attention_2"}, –∏–Ω–∞—á–µ –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å.
    """
    if not USE_FLASH_ATTENTION_2:
        return {}
    try:
        import flash_attn  # noqa: F401  # type: ignore
        return {"attn_implementation": "flash_attention_2"}
    except ImportError:
        warnings.warn(
            "USE_FLASH_ATTENTION_2 –≤–∫–ª—é—á–µ–Ω, –Ω–æ flash-attn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
            "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install flash-attn --no-build-isolation (—Ç—Ä–µ–±—É–µ—Ç—Å—è CUDA)."
        )
        return {}


def _load_causal_4bit(
    model_name: str,
    model_class: type,
    hyperparameters: Optional[dict] = None,
    **from_pretrained_extra
) -> Tuple[Any, Any]:
    """
    –û–±—â–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ª—é–±–æ–π causal LM –≤ 4-bit (nf4) –ø–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—É torch_dtype.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—Å–µ–º–∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞–º–∏ –ø—Ä–∏ hyperparameters["torch_dtype"] in ("nf4", "4bit").
    """
    from transformers import BitsAndBytesConfig
    hp = hyperparameters or {}
    max_cpu_gb = hp.get("max_cpu_gb_4bit") or os.environ.get("MAX_CPU_GB_4BIT") or os.environ.get("GEMMA_27B_4BIT_MAX_CPU_GB") or "12"
    max_cpu_gb = int(max_cpu_gb)
    max_memory = {0: "80GiB", "cpu": f"{max_cpu_gb}GiB"}
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ {model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        token=HF_TOKEN,
        timeout=HF_HUB_DOWNLOAD_TIMEOUT,
        resume_download=from_pretrained_extra.pop("resume_download", True),
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name} (4-bit, torch_dtype=nf4, CPU –ª–∏–º–∏—Ç {max_cpu_gb} GB)...")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4",
    )
    model = model_class.from_pretrained(
        model_name,
        device_map="auto",
        quantization_config=quantization_config,
        token=HF_TOKEN,
        max_memory=max_memory,
        low_cpu_mem_usage=True,
        **from_pretrained_extra,
    )
    if hasattr(model, "eval"):
        model = model.eval()
    print(f"   –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ 4-bit (nf4)")
    return model, tokenizer


def load_gemma_3(model_name: str, vram_warning: Optional[str] = None, model_size_warning: Optional[str] = None, hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π Gemma 3 —á–µ—Ä–µ–∑ Gemma3ForCausalLM.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—Å–µ—Ö Gemma 3 –º–æ–¥–µ–ª–µ–π (1b, 4b, 12b, 27b).
    
    Args:
        model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ HuggingFace (–Ω–∞–ø—Ä–∏–º–µ—Ä, "google/gemma-3-4b-it")
        vram_warning: –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –∫ VRAM (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        model_size_warning: –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Ä–∞–∑–º–µ—Ä–µ –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    
    Returns:
        (model, tokenizer)
    """
    hp = hyperparameters or {}
    if hp.get("torch_dtype") in ("nf4", "4bit"):
        return _load_causal_4bit(model_name, Gemma3ForCausalLM, hyperparameters)
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ {model_name}...")
    if vram_warning:
        print(f"   ‚ö†Ô∏è –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: {vram_warning}")
    
    try:
        start_time = time.time()
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            token=HF_TOKEN,
            timeout=HF_HUB_DOWNLOAD_TIMEOUT,
            resume_download=True
        )
        elapsed = time.time() - start_time
        print(f"   ‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {elapsed:.1f}—Å")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        print(f"   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print(f"     - –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
        print(f"     - –ü—Ä–æ–±–ª–µ–º—ã —Å HuggingFace —Å–µ—Ä–≤–µ—Ä–∞–º–∏")
        print(f"     - –ù–µ–≤–µ—Ä–Ω—ã–π –∏–ª–∏ –∏—Å—Ç–µ–∫—à–∏–π HF_TOKEN")
        print(f"   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
        print(f"     - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
        print(f"     - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å HF_TOKEN –≤ config_secrets.py")
        print(f"     - –£–≤–µ–ª–∏—á–∏—Ç—å —Ç–∞–π–º–∞—É—Ç: set HF_HUB_DOWNLOAD_TIMEOUT=600")
        raise
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name} (Gemma3ForCausalLM)...")
    if model_size_warning:
        print(f"   ‚ö†Ô∏è {model_size_warning}")
    
    try:
        start_time = time.time()
        model = Gemma3ForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            token=HF_TOKEN,
            **_get_flash_attn_kwargs()
        ).eval()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º eval –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        elapsed = time.time() - start_time
        if elapsed > 60:
            print(f"   ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {elapsed:.1f}—Å ({elapsed/60:.1f} –º–∏–Ω—É—Ç)")
        else:
            print(f"   ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {elapsed:.1f}—Å")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print(f"     - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é (4-bit –∏–ª–∏ 8-bit) –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø–∞–º—è—Ç–∏")
        if "api" not in model_name:
            api_model = model_name.replace("google/", "").replace("-it", "-api")
            print(f"     - –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API –≤–µ—Ä—Å–∏–∏: {api_model}")
        print(f"     - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—É—é VRAM: python gpu_info.py")
        raise
    
    return model, tokenizer


def load_mistral_3(model_name: str, vram_warning: Optional[str] = None, hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π Mistral 3 —á–µ—Ä–µ–∑ Mistral3ForConditionalGeneration.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—Å–µ—Ö Mistral 3 –º–æ–¥–µ–ª–µ–π.
    
    –í–ê–ñ–ù–û: 
    - –¢—Ä–µ–±—É–µ—Ç—Å—è transformers>=4.50.0.dev0: pip install git+https://github.com/huggingface/transformers
    - –¢—Ä–µ–±—É–µ—Ç—Å—è mistral-common >= 1.8.6: pip install mistral-common --upgrade
    
    Args:
        model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ HuggingFace (–Ω–∞–ø—Ä–∏–º–µ—Ä, "mistralai/Ministral-3-8B-Instruct-2512")
        vram_warning: –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –∫ VRAM (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        hyperparameters: –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ; –ø—Ä–∏ torch_dtype "nf4"/"4bit" –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –≤ 4-bit
    
    Returns:
        (model, tokenizer)
    """
    hp = hyperparameters or {}
    if hp.get("torch_dtype") in ("nf4", "4bit"):
        from transformers import Mistral3ForConditionalGeneration
        return _load_causal_4bit(model_name, Mistral3ForConditionalGeneration, hyperparameters)
    from transformers import Mistral3ForConditionalGeneration, MistralCommonBackend
    
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ {model_name}...")
    if vram_warning:
        print(f"   ‚ö†Ô∏è {vram_warning}")
    
    try:
        start_time = time.time()
        tokenizer = MistralCommonBackend.from_pretrained(model_name, token=HF_TOKEN)
        elapsed = time.time() - start_time
        print(f"   ‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {elapsed:.1f}—Å")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        raise
    
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
    try:
        start_time = time.time()
        model = Mistral3ForConditionalGeneration.from_pretrained(
            model_name,
            device_map="auto",
            dtype=torch.bfloat16,
            token=HF_TOKEN,
            **_get_flash_attn_kwargs()
        )
        elapsed = time.time() - start_time
        if elapsed > 60:
            print(f"   ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {elapsed:.1f}—Å ({elapsed/60:.1f} –º–∏–Ω—É—Ç)")
        else:
            print(f"   ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {elapsed:.1f}—Å")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        print(f"   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        if vram_warning:
            print(f"     - –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ VRAM ({vram_warning})")
        print(f"     - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—É—é VRAM: python gpu_info.py")
        raise
    
    return model, tokenizer


def load_standard_model(model_name: str, dtype: Optional[str] = None, torch_dtype: Optional[str] = None, 
                        device_map: str = "auto", trust_remote_code: bool = True, hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ AutoTokenizer –∏ AutoModelForCausalLM.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ fallback, –∫–æ–≥–¥–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.
    
    –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—Å–æ–±—ã—Ö —Å–ª—É—á–∞–µ–≤:
    - –ú–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã (Gemma3ForCausalLM, Mistral3ForConditionalGeneration, T5ForConditionalGeneration)
    - –ú–æ–¥–µ–ª–∏ —Å –æ—Å–æ–±—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    - –ú–æ–¥–µ–ª–∏ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏ –æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –∫ VRAM
    
    –î–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (Qwen, Gemma 2, –∏ —Ç.–¥.) —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
    –í hyperparameters –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å torch_dtype: "nf4" –¥–ª—è 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –ª—é–±–æ–π –º–æ–¥–µ–ª–∏.
    
    Args:
        model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ HuggingFace
        dtype: —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "bfloat16", "float16")
        torch_dtype: —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è torch (–Ω–∞–ø—Ä–∏–º–µ—Ä, "auto", "bfloat16")
        device_map: –∫–∞—Ä—Ç–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤ ("auto", "cuda", –∏ —Ç.–¥.)
        trust_remote_code: –¥–æ–≤–µ—Ä—è—Ç—å –ª–∏ —É–¥–∞–ª–µ–Ω–Ω–æ–º—É –∫–æ–¥—É
        hyperparameters: –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ; –ø—Ä–∏ torch_dtype "nf4"/"4bit" –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –≤ 4-bit
    
    Returns:
        (model, tokenizer)
    """
    hp = hyperparameters or {}
    if hp.get("torch_dtype") in ("nf4", "4bit"):
        return _load_causal_4bit(model_name, AutoModelForCausalLM, hyperparameters)
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        token=HF_TOKEN
    )
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    model_kwargs = {
        "device_map": device_map,
        "token": HF_TOKEN,
        "trust_remote_code": trust_remote_code,
        **_get_flash_attn_kwargs()
    }
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º dtype/torch_dtype –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
    if torch_dtype:
        if torch_dtype == "auto":
            model_kwargs["torch_dtype"] = "auto"
        elif torch_dtype == "bfloat16":
            model_kwargs["torch_dtype"] = torch.bfloat16
        elif torch_dtype == "float16":
            model_kwargs["torch_dtype"] = torch.float16
    elif dtype:
        if dtype == "bfloat16":
            model_kwargs["dtype"] = torch.bfloat16
        elif dtype == "float16":
            model_kwargs["dtype"] = torch.float16
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        **model_kwargs
    )
    
    return model, tokenizer

def load_gemma_2_2b(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ google/gemma-2-2b-it (–ø—Ä–∏ torch_dtype nf4/4bit ‚Äî 4-bit)."""
    return load_standard_model("google/gemma-2-2b-it", dtype="bfloat16", device_map="cuda", hyperparameters=hyperparameters)


def load_ministral_3_3b_reasoning_2512(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ mistralai/Ministral-3-3B-Reasoning-2512 (–ø—Ä–∏ torch_dtype nf4/4bit ‚Äî 4-bit)."""
    return load_mistral_3("mistralai/Ministral-3-3B-Reasoning-2512", hyperparameters=hyperparameters)


def load_mistral_3_8b_instruct(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ mistralai/Ministral-3-8B-Instruct-2512 (–ø—Ä–∏ torch_dtype nf4/4bit ‚Äî 4-bit)."""
    return load_mistral_3("mistralai/Ministral-3-8B-Instruct-2512", vram_warning="–ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç ~16GB VRAM –¥–ª—è –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏", hyperparameters=hyperparameters)


def load_mistral_3_14b_instruct(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ mistralai/Ministral-3-14B-Instruct-2512 (–ø—Ä–∏ torch_dtype nf4/4bit ‚Äî 4-bit)."""
    return load_mistral_3("mistralai/Ministral-3-14B-Instruct-2512", vram_warning="–ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç ~28GB VRAM –¥–ª—è –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏", hyperparameters=hyperparameters)


def load_mistral_3_3b_reasoning(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ mistralai/Ministral-3-3B-Reasoning-2512 (–ø—Ä–∏ torch_dtype nf4/4bit ‚Äî 4-bit)."""
    return load_mistral_3("mistralai/Ministral-3-3B-Reasoning-2512", hyperparameters=hyperparameters)

def load_qwen_2_5_1_5b(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ Qwen/Qwen2.5-1.5B-Instruct (–ø—Ä–∏ torch_dtype nf4/4bit ‚Äî 4-bit)."""
    return load_standard_model("Qwen/Qwen2.5-1.5B-Instruct", dtype="float16", hyperparameters=hyperparameters)


def load_qwen_2_5_3b(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ Qwen/Qwen2.5-3B-Instruct (–ø—Ä–∏ torch_dtype nf4/4bit ‚Äî 4-bit)."""
    return load_standard_model("Qwen/Qwen2.5-3B-Instruct", dtype="bfloat16", hyperparameters=hyperparameters)


def load_qwen_2_5_4b(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ Qwen/Qwen2.5-4B-Instruct (–ø—Ä–∏ torch_dtype nf4/4bit ‚Äî 4-bit)."""
    return load_standard_model("Qwen/Qwen2.5-4B-Instruct", dtype="bfloat16", hyperparameters=hyperparameters)


def load_qwen_3_4b(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ Qwen/Qwen3-4B-Instruct-2507 (–ø—Ä–∏ torch_dtype nf4/4bit ‚Äî 4-bit)."""
    return load_standard_model("Qwen/Qwen3-4B-Instruct-2507", dtype="bfloat16", hyperparameters=hyperparameters)


def load_qwen_3_8b(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ Qwen/Qwen3-8B (–ø—Ä–∏ torch_dtype nf4/4bit ‚Äî 4-bit)."""
    return load_standard_model("Qwen/Qwen3-8B", torch_dtype="auto", hyperparameters=hyperparameters)


def load_qwen_3_32b(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ Qwen/Qwen3-32B (–ø—Ä–∏ torch_dtype nf4/4bit ‚Äî 4-bit)."""
    model_id = "Qwen/Qwen3-32B"
    hp = hyperparameters or {}
    if hp.get("torch_dtype") in ("nf4", "4bit"):
        return _load_causal_4bit(model_id, AutoModelForCausalLM, hyperparameters)
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ Qwen/Qwen3-32B...")
    print(f"   ‚ö†Ô∏è –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –æ–±—ä–µ–º VRAM (~64GB+ –¥–ª—è –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏)")
    print(f"   (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)")
    
    try:
        start_time = time.time()
        tokenizer = AutoTokenizer.from_pretrained(
            "Qwen/Qwen3-32B",
            token=HF_TOKEN,
            timeout=HF_HUB_DOWNLOAD_TIMEOUT,
            resume_download=True
        )
        elapsed = time.time() - start_time
        print(f"   ‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {elapsed:.1f}—Å")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        print(f"   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print(f"     - –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
        print(f"     - –ü—Ä–æ–±–ª–µ–º—ã —Å HuggingFace —Å–µ—Ä–≤–µ—Ä–∞–º–∏")
        print(f"     - –ù–µ–≤–µ—Ä–Ω—ã–π –∏–ª–∏ –∏—Å—Ç–µ–∫—à–∏–π HF_TOKEN")
        print(f"   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
        print(f"     - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
        print(f"     - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å HF_TOKEN –≤ config_secrets.py")
        print(f"     - –£–≤–µ–ª–∏—á–∏—Ç—å —Ç–∞–π–º–∞—É—Ç: set HF_HUB_DOWNLOAD_TIMEOUT=600")
        raise
    
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Qwen/Qwen3-32B...")
    print(f"   ‚ö†Ô∏è –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∏–∑-–∑–∞ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ (~32B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)")
    try:
        start_time = time.time()
        model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen3-32B",
            torch_dtype="auto",
            device_map="auto",
            token=HF_TOKEN,
            trust_remote_code=True,
            **_get_flash_attn_kwargs()
        )
        elapsed = time.time() - start_time
        print(f"   ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {elapsed:.1f}—Å ({elapsed/60:.1f} –º–∏–Ω—É—Ç)")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        print(f"   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print(f"     - –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ VRAM (–º–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç ~64GB+ –¥–ª—è –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏)")
        print(f"     - –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
        print(f"     - –ü—Ä–æ–±–ª–µ–º—ã —Å HuggingFace —Å–µ—Ä–≤–µ—Ä–∞–º–∏")
        print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print(f"     - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é (4-bit –∏–ª–∏ 8-bit) –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø–∞–º—è—Ç–∏")
        print(f"     - –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API –≤–µ—Ä—Å–∏–∏: qwen-3-32b-api")
        print(f"     - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—É—é VRAM: python gpu_info.py")
        raise
    
    return model, tokenizer



def load_codegemma_7b(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ google/codegemma-7b-it (–ø—Ä–∏ torch_dtype nf4/4bit ‚Äî 4-bit)."""
    model_id = "google/codegemma-7b-it"
    hp = hyperparameters or {}
    if hp.get("torch_dtype") in ("nf4", "4bit"):
        return _load_causal_4bit(model_id, AutoModelForCausalLM, hyperparameters)
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ {model_id}...")
    print(f"   ‚ö†Ô∏è –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: CodeGemma —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º")
    print(f"   (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)")
    
    try:
        start_time = time.time()
        tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            token=HF_TOKEN,
            timeout=HF_HUB_DOWNLOAD_TIMEOUT,
            resume_download=True
        )
        elapsed = time.time() - start_time
        print(f"   ‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {elapsed:.1f}—Å")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        print(f"   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print(f"     - –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
        print(f"     - –ü—Ä–æ–±–ª–µ–º—ã —Å HuggingFace —Å–µ—Ä–≤–µ—Ä–∞–º–∏")
        print(f"     - –ù–µ–≤–µ—Ä–Ω—ã–π –∏–ª–∏ –∏—Å—Ç–µ–∫—à–∏–π HF_TOKEN")
        print(f"   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
        print(f"     - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
        print(f"     - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å HF_TOKEN –≤ config_secrets.py")
        print(f"     - –£–≤–µ–ª–∏—á–∏—Ç—å —Ç–∞–π–º–∞—É—Ç: set HF_HUB_DOWNLOAD_TIMEOUT=600")
        raise
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_id}...")
    print(f"   ‚ö†Ô∏è –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è –∏–∑-–∑–∞ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ (~7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)")
    try:
        start_time = time.time()
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            token=HF_TOKEN,
            trust_remote_code=True,
            **_get_flash_attn_kwargs()
        )
        elapsed = time.time() - start_time
        print(f"   ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {elapsed:.1f}—Å ({elapsed/60:.1f} –º–∏–Ω—É—Ç)")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        print(f"   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print(f"     - –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ VRAM (–º–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç ~14GB –¥–ª—è –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏)")
        print(f"     - –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
        print(f"     - –ü—Ä–æ–±–ª–µ–º—ã —Å HuggingFace —Å–µ—Ä–≤–µ—Ä–∞–º–∏")
        print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print(f"     - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é (4-bit –∏–ª–∏ 8-bit) –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø–∞–º—è—Ç–∏")
        print(f"     - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—É—é VRAM: python gpu_info.py")
        raise
    
    return model, tokenizer


def generate_gemma(
    model, 
    tokenizer, 
    prompt: str, 
    max_new_tokens: int = 1024, 
    repetition_penalty: float = None,
    structured_output: bool = False,
    response_schema: Any = None,
    use_outlines: bool = False
) -> str:
    """
    –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è Gemma 3 –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
    
    Args:
        model: –º–æ–¥–µ–ª—å (Gemma3ForCausalLM)
        tokenizer: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        prompt: –ø—Ä–æ–º–ø—Ç
        max_new_tokens: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        repetition_penalty: —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è (–µ—Å–ª–∏ None, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
        structured_output: —Ñ–ª–∞–≥ –¥–ª—è structured output
        response_schema: —Å—Ö–µ–º–∞ –¥–ª—è structured output
        use_outlines: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ outlines –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ JSON
    """
    # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω outlines-—Ä–µ–∂–∏–º –¥–ª—è structured output ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º JSON –Ω–∞–ø—Ä—è–º—É—é –ø–æ —Å—Ö–µ–º–µ
    # –†–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö HF-–º–æ–¥–µ–ª–µ–π; –¥–ª—è API –º–æ–¥–µ–ª–µ–π outlines –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.
    if use_outlines and structured_output and response_schema is not None:
        try:
            import outlines  # type: ignore
            from outlines import generate  # type: ignore
        except Exception as e:
            raise ImportError(
                "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ outlines –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install outlines"
            ) from e

        try:
            # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º HF –º–æ–¥–µ–ª—å/—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ outlines model
            outlines_model = outlines.models.transformers.Transformers(model, tokenizer)
            generator = generate.json(outlines_model, response_schema)
            generated = generator(prompt)

            # Outlines –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å dict/list –ª–∏–±–æ —Å—Ç—Ä–æ–∫—É; –ø—Ä–∏–≤–æ–¥–∏–º –∫ JSON-—Å—Ç—Ä–æ–∫–µ
            if isinstance(generated, (dict, list)):
                import json as _json
                return _json.dumps(generated, ensure_ascii=False, indent=2)
            return str(generated).strip()
        except Exception as e:
            raise RuntimeError(f"Outlines –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}") from e
    
    # –î–ª—è Gemma 3 –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å Gemma3ForCausalLM
    is_gemma3 = isinstance(model, Gemma3ForCausalLM) or model.__class__.__name__ == 'Gemma3ForCausalLM'
    
    if is_gemma3 and hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template is not None:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è Gemma 3
        messages = [
            [
                {
                    "role": "user",
                    "content": [{"type": "text", "text": prompt}]
                },
            ],
        ]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        inputs_dict = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        )
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º device –∏ dtype –∫ —Ç–µ–Ω–∑–æ—Ä–∞–º –≤ —Å–ª–æ–≤–∞—Ä–µ
        # –í–ê–ñ–ù–û: input_ids –¥–æ–ª–∂–Ω—ã –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è Long (int64), –Ω–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏—Ö –≤ bfloat16
        device = next(model.parameters()).device
        inputs = {}
        for key, value in inputs_dict.items():
            if isinstance(value, torch.Tensor):
                if key == "input_ids":
                    # input_ids –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å Long (int64), —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ device
                    inputs[key] = value.to(device)
                else:
                    # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã (attention_mask –∏ —Ç.–¥.) –º–æ–≥—É—Ç –±—ã—Ç—å –≤ bfloat16
                    inputs[key] = value.to(device).to(torch.bfloat16)
            else:
                inputs[key] = value
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        generate_kwargs = {
            "max_new_tokens": max_new_tokens,
            "do_sample": False,
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º eos_token_id, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        if tokenizer.eos_token_id is not None:
            generate_kwargs["eos_token_id"] = tokenizer.eos_token_id
        
        # –î–æ–±–∞–≤–ª—è–µ–º repetition_penalty, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if repetition_penalty is not None:
            generate_kwargs["repetition_penalty"] = repetition_penalty
        
        with torch.inference_mode():
            outputs = model.generate(**inputs, **generate_kwargs)
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        outputs_decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        full_text = outputs_decoded[0] if outputs_decoded else ""
        
        # –ù—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã (–æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏)
        # –î–ª—è —ç—Ç–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç–¥–µ–ª—å–Ω–æ
        inputs_decoded = tokenizer.batch_decode(
            inputs['input_ids'] if isinstance(inputs, dict) else inputs, 
            skip_special_tokens=True
        )
        input_text = inputs_decoded[0] if inputs_decoded else ""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é —á–∞—Å—Ç—å –æ—Ç–≤–µ—Ç–∞
        if full_text.startswith(input_text):
            text = full_text[len(input_text):].strip()
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å input_text, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –¥—Ä—É–≥–∏–º —Å–ø–æ—Å–æ–±–æ–º
            # –ü—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏ –ø—É—Å—Ç—å –ø–∞—Ä—Å–µ—Ä —Ä–∞–∑–±–µ—Ä–µ—Ç—Å—è
            text = full_text
        
        return text
    
    else:
        # Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π Gemma –∏–ª–∏ –µ—Å–ª–∏ chat template –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å chat template, –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω
        if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template is not None:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∫ –¥–∏–∞–ª–æ–≥
            messages = [{"role": "user", "content": prompt}]
            formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        else:
            formatted_prompt = prompt
        
        input_ids = tokenizer(formatted_prompt, return_tensors="pt").input_ids.to(model.device)
        
        generate_kwargs = {
            "input_ids": input_ids,
            "max_new_tokens": max_new_tokens,
            "do_sample": False,
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º eos_token_id, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        if tokenizer.eos_token_id is not None:
            generate_kwargs["eos_token_id"] = tokenizer.eos_token_id
        
        # –î–æ–±–∞–≤–ª—è–µ–º repetition_penalty, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if repetition_penalty is not None:
            generate_kwargs["repetition_penalty"] = repetition_penalty
        
        with torch.no_grad():
            output_ids = model.generate(**generate_kwargs)
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã (–∏–≥–Ω–æ—Ä–∏—Ä—É—è –≤—Ö–æ–¥–Ω—ã–µ)
        input_length = input_ids.shape[1]
        generated_ids = output_ids[0][input_length:]
        text = tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        # –ï—Å–ª–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–∞–ª–æ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –ø—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å –æ—Ç–≤–µ—Ç
        if not text.strip():
            text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
            # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä prompt
            if text.startswith(formatted_prompt):
                text = text[len(formatted_prompt):].strip()
            elif text.startswith(prompt):
                text = text[len(prompt):].strip()
        
        return text.strip()


def load_phi_4_mini_instruct(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ microsoft/Phi-4-mini-instruct (–ø—Ä–∏ torch_dtype nf4/4bit ‚Äî 4-bit)."""
    model_id = "microsoft/Phi-4-mini-instruct"
    hp = hyperparameters or {}
    if hp.get("torch_dtype") in ("nf4", "4bit"):
        return _load_causal_4bit(model_id, AutoModelForCausalLM, hyperparameters)
    tokenizer = AutoTokenizer.from_pretrained(
        model_id,
        token=HF_TOKEN
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        dtype=torch.bfloat16,
        token=HF_TOKEN,
        trust_remote_code=True,
        **_get_flash_attn_kwargs()
    )
    return model, tokenizer


def load_t5gemma_2_1b_1b(hyperparameters: Optional[dict] = None) -> Tuple[Any, Any]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ google/t5gemma-2-1b-1b (–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å Image-Text-to-Text)
    
    –ú–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–±–æ—Ç—É —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, –Ω–æ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞—á
    –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –≤–≤–æ–¥.
    
    –°–º. –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é: https://huggingface.co/google/t5gemma-2-1b-1b
    """
    model_id = "google/t5gemma-2-1b-1b"
    
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ {model_id}...")
    print(f"   ‚ö†Ô∏è –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç XET –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤, –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è")
    try:
        processor = AutoProcessor.from_pretrained(
            model_id,
            token=HF_TOKEN,
            timeout=HF_HUB_DOWNLOAD_TIMEOUT,
            resume_download=True
        )
        print(f"   ‚úì –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
    except Exception as e:
        error_msg = str(e)
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞: {error_msg}")
        if "XET" in error_msg or "xet" in error_msg.lower() or "getaddrinfo failed" in error_msg:
            print(f"   üí° –ü—Ä–æ–±–ª–µ–º–∞ —Å XET —Å–µ—Ä–≤–∏—Å–æ–º –∏–ª–∏ —Å–µ—Ç—å—é:")
            print(f"      - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
            print(f"      - –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –ø—Ä–∏–Ω—è–ª–∏ –ª–∏—Ü–µ–Ω–∑–∏—é –º–æ–¥–µ–ª–∏ –Ω–∞ https://huggingface.co/{model_id}")
            print(f"      - –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é T5 –º–æ–¥–µ–ª—å")
        raise
    
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_id}...")
    print(f"   ‚ö†Ô∏è –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è –∏–∑-–∑–∞ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ (~2B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)")
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º AutoModelForImageTextToText –¥–ª—è T5Gemma 2 –º–æ–¥–µ–ª–µ–π
        model = AutoModelForImageTextToText.from_pretrained(
            model_id,
            device_map="auto",
            dtype=torch.bfloat16,
            token=HF_TOKEN,
            timeout=HF_HUB_DOWNLOAD_TIMEOUT,
            resume_download=True
        )
        print(f"   ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        error_msg = str(e)
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {error_msg}")
        if "XET" in error_msg or "xet" in error_msg.lower() or "getaddrinfo failed" in error_msg:
            print(f"   üí° –ü—Ä–æ–±–ª–µ–º–∞ —Å XET —Å–µ—Ä–≤–∏—Å–æ–º –∏–ª–∏ —Å–µ—Ç—å—é:")
            print(f"      - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
            print(f"      - –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –ø—Ä–∏–Ω—è–ª–∏ –ª–∏—Ü–µ–Ω–∑–∏—é –º–æ–¥–µ–ª–∏ –Ω–∞ https://huggingface.co/{model_id}")
            print(f"      - –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç XET (Git LFS —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ), —á—Ç–æ –º–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
            print(f"      - –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é T5 –º–æ–¥–µ–ª—å:")
            print(f"        * google/t5-v1_1-base")
            print(f"        * google/flan-t5-base")
            print(f"        * google/flan-t5-small")
        elif "pytorch_model.bin" in error_msg:
            print(f"   üí° –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é:")
            print(f"      - –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –ø—Ä–∏–Ω—è–ª–∏ –ª–∏—Ü–µ–Ω–∑–∏—é –º–æ–¥–µ–ª–∏ –Ω–∞ https://huggingface.co/{model_id}")
            print(f"      - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ HF_TOKEN —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ")
            print(f"      - –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å —Ç–∞–π–º–∞—É—Ç: set HF_HUB_DOWNLOAD_TIMEOUT=600")
        raise
    
    # –î–ª—è T5Gemma –º–æ–¥–µ–ª–µ–π –≤–æ–∑–≤—Ä–∞—â–∞–µ–º processor –∫–∞–∫ tokenizer (processor —Å–æ–¥–µ—Ä–∂–∏—Ç tokenizer)
    return model, processor


def generate_standard(
    model,
    tokenizer,
    prompt: str,
    max_new_tokens: int = 1024,
    repetition_penalty: float = None,
    structured_output: bool = False,
    response_schema: Any = None,
    use_outlines: bool = False,
) -> str:
    """
    –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π
    
    Args:
        model: –º–æ–¥–µ–ª—å
        tokenizer: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        prompt: –ø—Ä–æ–º–ø—Ç
        max_new_tokens: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        repetition_penalty: —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è (–µ—Å–ª–∏ None, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
    """
    # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω outlines-—Ä–µ–∂–∏–º –¥–ª—è structured output ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º JSON –Ω–∞–ø—Ä—è–º—É—é –ø–æ —Å—Ö–µ–º–µ
    # –†–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö HF-–º–æ–¥–µ–ª–µ–π; –¥–ª—è API –º–æ–¥–µ–ª–µ–π outlines –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.
    if use_outlines and structured_output and response_schema is not None:
        try:
            import outlines  # type: ignore
            from outlines import generate  # type: ignore
        except Exception as e:
            raise ImportError(
                "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ outlines –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install outlines"
            ) from e

        try:
            # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º HF –º–æ–¥–µ–ª—å/—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ outlines model
            outlines_model = outlines.models.transformers.Transformers(model, tokenizer)
            generator = generate.json(outlines_model, response_schema)
            generated = generator(prompt)

            # Outlines –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å dict/list –ª–∏–±–æ —Å—Ç—Ä–æ–∫—É; –ø—Ä–∏–≤–æ–¥–∏–º –∫ JSON-—Å—Ç—Ä–æ–∫–µ
            if isinstance(generated, (dict, list)):
                import json as _json
                return _json.dumps(generated, ensure_ascii=False, indent=2)
            return str(generated).strip()
        except Exception as e:
            raise RuntimeError(f"Outlines –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}") from e

    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
    
    generate_kwargs = {
        "input_ids": input_ids,
        "max_new_tokens": max_new_tokens,
        "do_sample": False,
        "use_cache": True,  # –í–∫–ª—é—á–∞–µ–º –∫—ç—à –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º eos_token_id, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
    if tokenizer.eos_token_id is not None:
        generate_kwargs["eos_token_id"] = tokenizer.eos_token_id
    
    # –î–æ–±–∞–≤–ª—è–µ–º repetition_penalty, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    if repetition_penalty is not None:
        generate_kwargs["repetition_penalty"] = repetition_penalty
    
    with torch.no_grad():
        try:
            output_ids = model.generate(**generate_kwargs)
        except AttributeError as e:
            if "from_legacy_cache" in str(e):
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ —Å–≤—è–∑–∞–Ω–∞ —Å –∫—ç—à–µ–º, –æ—Ç–∫–ª—é—á–∞–µ–º use_cache
                generate_kwargs["use_cache"] = False
                output_ids = model.generate(**generate_kwargs)
            else:
                raise
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã (–∏–≥–Ω–æ—Ä–∏—Ä—É—è –≤—Ö–æ–¥–Ω—ã–µ)
    input_length = input_ids.shape[1]
    generated_ids = output_ids[0][input_length:]
    text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    # –ï—Å–ª–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–∞–ª–æ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –ø—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å –æ—Ç–≤–µ—Ç
    if not text.strip():
        text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä prompt
        if text.startswith(prompt):
            text = text[len(prompt):].strip()
    
    return text.strip()


def generate_qwen(
    model, 
    tokenizer, 
    prompt: str, 
    max_new_tokens: int = 512, 
    repetition_penalty: float = None,
    structured_output: bool = False,
    response_schema: Any = None,
    use_outlines: bool = False
) -> str:
    """
    –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è Qwen —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Å—Ç–æ–ø-—Å—Ç—Ä–æ–∫–∞–º–∏
    
    Args:
        model: –º–æ–¥–µ–ª—å
        tokenizer: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        prompt: –ø—Ä–æ–º–ø—Ç
        max_new_tokens: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        repetition_penalty: —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è (–µ—Å–ª–∏ None, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
        structured_output: —Ñ–ª–∞–≥ –¥–ª—è structured output
        response_schema: —Å—Ö–µ–º–∞ –¥–ª—è structured output
        use_outlines: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ outlines –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ JSON
    """
    # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω outlines-—Ä–µ–∂–∏–º –¥–ª—è structured output ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º JSON –Ω–∞–ø—Ä—è–º—É—é –ø–æ —Å—Ö–µ–º–µ
    # –†–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö HF-–º–æ–¥–µ–ª–µ–π; –¥–ª—è API –º–æ–¥–µ–ª–µ–π outlines –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.
    if use_outlines and structured_output and response_schema is not None:
        try:
            import outlines  # type: ignore
            from outlines import generate  # type: ignore
        except Exception as e:
            raise ImportError(
                "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ outlines –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install outlines"
            ) from e

        try:
            # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º HF –º–æ–¥–µ–ª—å/—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ outlines model
            outlines_model = outlines.models.transformers.Transformers(model, tokenizer)
            generator = generate.json(outlines_model, response_schema)
            generated = generator(prompt)

            # Outlines –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å dict/list –ª–∏–±–æ —Å—Ç—Ä–æ–∫—É; –ø—Ä–∏–≤–æ–¥–∏–º –∫ JSON-—Å—Ç—Ä–æ–∫–µ
            if isinstance(generated, (dict, list)):
                import json as _json
                return _json.dumps(generated, ensure_ascii=False, indent=2)
            return str(generated).strip()
        except Exception as e:
            raise RuntimeError(f"Outlines –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}") from e
    
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
    
    generate_kwargs = {
        "input_ids": input_ids,
        "max_new_tokens": max_new_tokens,
        "do_sample": False,
        "eos_token_id": tokenizer.eos_token_id,
        "stop_strings": ["Human:", "Example"],
        "tokenizer": tokenizer
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º repetition_penalty, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    if repetition_penalty is not None:
        generate_kwargs["repetition_penalty"] = repetition_penalty
    
    with torch.no_grad():
        output_ids = model.generate(**generate_kwargs)
    
    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    
    # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä prompt
    if text.startswith(prompt):
        text = text[len(prompt):].strip()
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å—Ç—Ä–æ–∫–∏
    for s in ["Human:", "Example"]:
        if s in text:
            text = text.split(s)[0].strip()
    
    return text.strip()


def generate_t5(
    model, 
    tokenizer_or_processor, 
    prompt: str, 
    max_new_tokens: int = 1024, 
    repetition_penalty: float = None,
    structured_output: bool = False,
    response_schema: Any = None,
    use_outlines: bool = False
) -> str:
    """
    –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è T5/Seq2Seq –º–æ–¥–µ–ª–µ–π
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ processor (AutoProcessor), —Ç–∞–∫ –∏ tokenizer (T5Tokenizer)
    
    Args:
        model: –º–æ–¥–µ–ª—å (AutoModelForImageTextToText, AutoModelForSeq2SeqLM –∏–ª–∏ T5ForConditionalGeneration)
        tokenizer_or_processor: –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (AutoProcessor) –∏–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (T5Tokenizer)
        prompt: –ø—Ä–æ–º–ø—Ç
        max_new_tokens: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        repetition_penalty: —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è (–µ—Å–ª–∏ None, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
        structured_output: —Ñ–ª–∞–≥ –¥–ª—è structured output (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –¥–ª—è T5)
        response_schema: —Å—Ö–µ–º–∞ –¥–ª—è structured output (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –¥–ª—è T5)
        use_outlines: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ outlines (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –¥–ª—è T5)
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —ç—Ç–æ processor –∏–ª–∏ tokenizer
    # –î–ª—è T5Gemma processor —Ç—Ä–µ–±—É–µ—Ç —è–≤–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä text= –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞
    
    input_ids = None
    decoder = None
    
    try:
        # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ processor (–µ—Å–ª–∏ —ç—Ç–æ AutoProcessor –¥–ª—è T5Gemma)
        # –î–ª—è T5Gemma –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å text= –ø–∞—Ä–∞–º–µ—Ç—Ä
        if hasattr(tokenizer_or_processor, '__call__'):
            # –ü—Ä–æ–±—É–µ–º —Å —è–≤–Ω—ã–º text= –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º (–¥–ª—è T5Gemma)
            try:
                inputs = tokenizer_or_processor(text=prompt, return_tensors="pt")
                if inputs is not None and isinstance(inputs, dict) and 'input_ids' in inputs:
                    input_ids = inputs['input_ids'].to(model.device)
                    decoder = tokenizer_or_processor
            except (TypeError, ValueError):
                # –ï—Å–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ —Å text=, –ø—Ä–æ–±—É–µ–º –±–µ–∑ –Ω–µ–≥–æ
                try:
                    inputs = tokenizer_or_processor(prompt, return_tensors="pt")
                    if inputs is not None and isinstance(inputs, dict) and 'input_ids' in inputs:
                        input_ids = inputs['input_ids'].to(model.device)
                        decoder = tokenizer_or_processor
                except Exception:
                    pass
        
        # –ï—Å–ª–∏ processor –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ tokenizer
        if input_ids is None:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É –æ–±—ä–µ–∫—Ç–∞ –∞—Ç—Ä–∏–±—É—Ç tokenizer (processor –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å tokenizer)
            if hasattr(tokenizer_or_processor, 'tokenizer'):
                actual_tokenizer = tokenizer_or_processor.tokenizer
            else:
                actual_tokenizer = tokenizer_or_processor
            
            input_ids = actual_tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
            decoder = actual_tokenizer
            
    except Exception as e:
        # –ï—Å–ª–∏ –≤—Å–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ, –ø—Ä–æ–±—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–∞—Ä–∏–∞–Ω—Ç
        try:
            if hasattr(tokenizer_or_processor, 'tokenizer'):
                actual_tokenizer = tokenizer_or_processor.tokenizer
            else:
                actual_tokenizer = tokenizer_or_processor
            input_ids = actual_tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
            decoder = actual_tokenizer
        except Exception as e2:
            raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–æ–º–ø—Ç —Å processor/tokenizer: {e2}") from e2
    
    if input_ids is None:
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å input_ids –∏–∑ processor/tokenizer")
    
    generate_kwargs = {
        "input_ids": input_ids,
        "max_length": input_ids.shape[1] + max_new_tokens,  # T5 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç max_length –≤–º–µ—Å—Ç–æ max_new_tokens
        "do_sample": False,
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º decoder_start_token_id –¥–ª—è T5 –º–æ–¥–µ–ª–µ–π
    if decoder is not None:
        if hasattr(decoder, 'pad_token_id') and decoder.pad_token_id is not None:
            generate_kwargs["decoder_start_token_id"] = decoder.pad_token_id
        elif hasattr(decoder, 'tokenizer') and hasattr(decoder.tokenizer, 'pad_token_id'):
            if decoder.tokenizer.pad_token_id is not None:
                generate_kwargs["decoder_start_token_id"] = decoder.tokenizer.pad_token_id
    
    with torch.no_grad():
        output_ids = model.generate(**generate_kwargs)
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    if decoder is None:
        raise RuntimeError("Decoder –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ output_ids –Ω–µ None –∏ –Ω–µ –ø—É—Å—Ç–æ–π
    if output_ids is None or len(output_ids) == 0:
        raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –æ—Ç–≤–µ—Ç")
    
    # –î–ª—è processor –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å tokenizer –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
    if hasattr(decoder, 'decode'):
        text = decoder.decode(output_ids[0], skip_special_tokens=True)
    elif hasattr(decoder, 'tokenizer') and hasattr(decoder.tokenizer, 'decode'):
        text = decoder.tokenizer.decode(output_ids[0], skip_special_tokens=True)
    else:
        raise RuntimeError(f"Decoder {type(decoder)} –Ω–µ –∏–º–µ–µ—Ç –º–µ—Ç–æ–¥–∞ decode")
    
    # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä prompt, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
    if text.startswith(prompt):
        text = text[len(prompt):].strip()
    
    return text.strip()


def generate_qwen_3(
    model, 
    tokenizer, 
    prompt: str, 
    max_new_tokens: int = 32768, 
    repetition_penalty: float = None, 
    enable_thinking: bool = False,
    structured_output: bool = False,
    response_schema: Any = None,
    use_outlines: bool = False
) -> str:
    """
    –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è Qwen3 —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π thinking mode
    
    Args:
        model: –º–æ–¥–µ–ª—å
        tokenizer: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        prompt: –ø—Ä–æ–º–ø—Ç
        max_new_tokens: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 32768 –¥–ª—è Qwen3)
        repetition_penalty: —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è (–µ—Å–ª–∏ None, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
        enable_thinking: –≤–∫–ª—é—á–∏—Ç—å thinking mode (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)
        structured_output: —Ñ–ª–∞–≥ –¥–ª—è structured output (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –¥–ª—è Qwen3)
        response_schema: —Å—Ö–µ–º–∞ –¥–ª—è structured output (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –¥–ª—è Qwen3)
        use_outlines: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ outlines (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –¥–ª—è Qwen3)
    """
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è chat template
    messages = [
        {"role": "user", "content": prompt}
    ]
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template —Å thinking mode
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=enable_thinking
    )
    
    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    generate_kwargs = {
        **model_inputs,
        "max_new_tokens": max_new_tokens,
        "do_sample": False,
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º eos_token_id, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
    if tokenizer.eos_token_id is not None:
        generate_kwargs["eos_token_id"] = tokenizer.eos_token_id
    
    # –î–æ–±–∞–≤–ª—è–µ–º repetition_penalty, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    if repetition_penalty is not None:
        generate_kwargs["repetition_penalty"] = repetition_penalty
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º
    with torch.no_grad():
        generated_ids = model.generate(**generate_kwargs)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã (–æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏)
    input_length = model_inputs["input_ids"].shape[1]
    output_ids = generated_ids[0][input_length:].tolist()
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    text = tokenizer.decode(output_ids, skip_special_tokens=True)
    
    return text.strip()

