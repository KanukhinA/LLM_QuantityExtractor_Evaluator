"""
–ú–æ–¥—É–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è few-shot –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞
–Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ Dual-Level Introspective Uncertainty
"""
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
import json
import os
from collections import Counter
from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer
import warnings

def levenshtein_distance(s1: str, s2: str) -> int:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç—Ä–æ–∫–∞–º–∏.
    
    Args:
        s1: –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞
        s2: –≤—Ç–æ—Ä–∞—è —Å—Ç—Ä–æ–∫–∞
    
    Returns:
        –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–ø–µ—Ä–∞—Ü–∏–π –≤—Å—Ç–∞–≤–∫–∏,
        —É–¥–∞–ª–µ–Ω–∏—è –∏–ª–∏ –∑–∞–º–µ–Ω—ã —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è s1 –≤ s2)
    """
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    if len(s2) == 0:
        return len(s1)
    
    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    return previous_row[-1]

from utils import build_prompt3, parse_json_safe, is_valid_json, extract_json_from_response
from config import PROMPT_TEMPLATE_NAME


def cluster_russian_texts(
    df: pd.DataFrame,
    text_column: str,
    n_clusters: int = 100,
    model_name: str = "intfloat/multilingual-e5-large"
) -> pd.DataFrame:
    """
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ—Ç —Å—Ç—Ä–æ–∫–∏ –∏–∑ —Å—Ç–æ–ª–±—Ü–∞ `text_column` –≤ DataFrame `df`.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π DataFrame + –¥–æ–±–∞–≤–ª–µ–Ω–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ '–ö–ª–∞—Å—Ç–µ—Ä'.
    
    Args:
        df: DataFrame —Å —Ç–µ–∫—Å—Ç–∞–º–∏
        text_column: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–∞–º–∏
        n_clusters: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    
    Returns:
        DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π '–ö–ª–∞—Å—Ç–µ—Ä'
    """
    if text_column not in df.columns:
        raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{text_column}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ DataFrame")
    
    print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_name}...")
    model = SentenceTransformer(model_name)
    
    print(f"   –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã...")
    texts = df[text_column].astype(str).tolist()
    embeddings = model.encode(texts, normalize_embeddings=True, show_progress_bar=True)
    
    print(f"   –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(embeddings)
    
    df_result = df.copy()
    df_result["–ö–ª–∞—Å—Ç–µ—Ä"] = clusters
    df_result = df_result.sort_values("–ö–ª–∞—Å—Ç–µ—Ä").reset_index(drop=True)
    
    print(f"   ‚úì –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return df_result


def filter_unlabeled_texts(
    unlabeled_df: pd.DataFrame,
    labeled_df: pd.DataFrame,
    text_column: str = "text"
) -> pd.DataFrame:
    """
    –§–∏–ª—å—Ç—Ä—É–µ—Ç –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã, —É–±–∏—Ä–∞—è —Ç–µ, —á—Ç–æ —É–∂–µ –µ—Å—Ç—å –≤ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.
    
    Args:
        unlabeled_df: DataFrame —Å –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏
        labeled_df: DataFrame —Å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏
        text_column: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–∞–º–∏
    
    Returns:
        –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame —Å –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏
    """
    if text_column not in unlabeled_df.columns:
        raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{text_column}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ unlabeled_df")
    if text_column not in labeled_df.columns:
        raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{text_column}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ labeled_df")
    
    labeled_texts = set(labeled_df[text_column].astype(str).str.strip())
    unlabeled_texts = unlabeled_df[text_column].astype(str).str.strip()
    
    mask = ~unlabeled_texts.isin(labeled_texts)
    filtered_df = unlabeled_df[mask].reset_index(drop=True)
    
    print(f"   –ò—Å—Ö–æ–¥–Ω–æ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {len(unlabeled_df)}")
    print(f"   –†–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {len(labeled_df)}")
    print(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {len(filtered_df)}")
    
    return filtered_df


def extract_structured_output(json_obj: Any) -> set:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON –≤ –≤–∏–¥–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ (type, text) –∫–æ—Ä—Ç–µ–∂–µ–π.
    
    Args:
        json_obj: —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç
    
    Returns:
        –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –∫–æ—Ä—Ç–µ–∂–µ–π (type, text)
    """
    result = set()
    
    if not isinstance(json_obj, dict):
        return result
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞—Å—Å–æ–≤—ã–µ –¥–æ–ª–∏
    if "–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è" in json_obj:
        mass_dolya = json_obj["–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è"]
        if isinstance(mass_dolya, list):
            for item in mass_dolya:
                if isinstance(item, dict):
                    substance = item.get("–≤–µ—â–µ—Å—Ç–≤–æ", "")
                    value = item.get("–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è", "")
                    if substance:
                        result.add(("–º–∞—Å—Å–æ–≤–∞—è_–¥–æ–ª—è", f"{substance}:{value}"))
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–æ—á–µ–µ
    if "–ø—Ä–æ—á–µ–µ" in json_obj:
        prochee = json_obj["–ø—Ä–æ—á–µ–µ"]
        if isinstance(prochee, list):
            for item in prochee:
                if isinstance(item, dict):
                    param = item.get("–ø–∞—Ä–∞–º–µ—Ç—Ä", "")
                    value = item.get("–∑–Ω–∞—á–µ–Ω–∏–µ") or item.get("–º–∞—Å—Å–∞") or item.get("–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ") or item.get("–æ–±—ä–µ–º")
                    if param:
                        result.add(("–ø—Ä–æ—á–µ–µ", f"{param}:{value}"))
    
    return result


def calculate_generation_disagreement(
    responses: List[str]
) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç Generation Disagreement (ùí∞_d) –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ–ø–∞—Ä–Ω–æ–≥–æ 
    —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –º–µ–∂–¥—É k —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏.
    
    –§–æ—Ä–º—É–ª–∞: ùí∞_d(s_i) = 2/(k(k-1)) * Œ£ Levenshtein(‚Ñ≥_Œ∏^j(s_i), ‚Ñ≥_Œ∏^l(s_i))
    –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä 1 ‚â§ j < l ‚â§ k
    
    Args:
        responses: —Å–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏ (k –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤)
    
    Returns:
        Generation Disagreement (—Å—Ä–µ–¥–Ω–µ–µ –ø–æ–ø–∞—Ä–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞)
    """
    k = len(responses)
    if k < 2:
        return 0.0
    
    # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–ø–∞—Ä–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞
    distances = []
    for i in range(k):
        for j in range(i + 1, k):
            dist = levenshtein_distance(responses[i], responses[j])
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Å—Ç—Ä–æ–∫ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏—è –≤ [0, 1]
            max_len = max(len(responses[i]), len(responses[j]), 1)
            normalized_dist = dist / max_len
            distances.append(normalized_dist)
    
    # –°—Ä–µ–¥–Ω–µ–µ –ø–æ–ø–∞—Ä–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ —Å–æ–≥–ª–∞—Å–Ω–æ —Ñ–æ—Ä–º—É–ª–µ (2)
    # –§–æ—Ä–º—É–ª–∞: 2/(k(k-1)) * Œ£ distances, –Ω–æ —Ç–∞–∫ –∫–∞–∫ –º—ã —É–∂–µ —É—Å—Ä–µ–¥–Ω—è–µ–º, –ø—Ä–æ—Å—Ç–æ –±–µ—Ä–µ–º mean
    avg_distance = np.mean(distances) if distances else 0.0
    
    return avg_distance


def calculate_format_uncertainty(
    responses: List[str],
    parser_func: callable = parse_json_safe
) -> Tuple[float, float]:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç format-level uncertainty (R_fail + Structural Disagreement).
    
    Args:
        responses: —Å–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏ (k –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤)
        parser_func: —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON
    
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (R_fail, structural_disagreement)
    """
    k = len(responses)
    if k == 0:
        return 1.0, 1.0
    
    # –í—ã—á–∏—Å–ª—è–µ–º R_fail (parsing failure rate)
    failed_count = 0
    parsed_outputs = []
    
    for response in responses:
        json_part = extract_json_from_response(response)
        parsed = parser_func(json_part)
        is_valid = is_valid_json(json_part)
        
        if not is_valid or parsed is None:
            failed_count += 1
        else:
            parsed_outputs.append(parsed)
    
    R_fail = failed_count / k
    
    # –í—ã—á–∏—Å–ª—è–µ–º Structural Disagreement (–¥–ª—è —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö)
    if len(parsed_outputs) < 2:
        structural_disagreement = 1.0 if R_fail > 0 else 0.0
    else:
        # –ò–∑–º–µ—Ä—è–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (—Ä–∞–∑–Ω—ã–µ –∫–ª—é—á–∏, —Ä–∞–∑–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–∏—Å–∫–æ–≤)
        structures = []
        for parsed in parsed_outputs:
            if isinstance(parsed, dict):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É: –Ω–∞–±–æ—Ä –∫–ª—é—á–µ–π –∏ –¥–ª–∏–Ω—ã —Å–ø–∏—Å–∫–æ–≤
                keys = set(parsed.keys())
                list_lengths = {}
                for key, value in parsed.items():
                    if isinstance(value, list):
                        list_lengths[key] = len(value)
                structures.append((keys, list_lengths))
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä
        if len(structures) == 0:
            structural_disagreement = 1.0
        else:
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ–ø–∞—Ä–Ω–æ
            disagreements = []
            for i in range(len(structures)):
                for j in range(i + 1, len(structures)):
                    keys_i, lengths_i = structures[i]
                    keys_j, lengths_j = structures[j]
                    
                    # –†–∞–∑–ª–∏—á–∏–µ –≤ –∫–ª—é—á–∞—Ö
                    key_diff = len(keys_i.symmetric_difference(keys_j)) / max(len(keys_i.union(keys_j)), 1)
                    
                    # –†–∞–∑–ª–∏—á–∏–µ –≤ –¥–ª–∏–Ω–∞—Ö —Å–ø–∏—Å–∫–æ–≤
                    all_keys = set(lengths_i.keys()) | set(lengths_j.keys())
                    length_diff = 0.0
                    if all_keys:
                        for key in all_keys:
                            len_i = lengths_i.get(key, 0)
                            len_j = lengths_j.get(key, 0)
                            if len_i != len_j:
                                length_diff += 1.0
                        length_diff /= len(all_keys)
                    
                    disagreement = (key_diff + length_diff) / 2.0
                    disagreements.append(disagreement)
            
            structural_disagreement = np.mean(disagreements) if disagreements else 0.0
    
    return R_fail, structural_disagreement


def calculate_content_uncertainty(
    responses: List[str],
    parser_func: callable = parse_json_safe
) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç content-level uncertainty –Ω–∞ –æ—Å–Ω–æ–≤–µ Jaccard similarity.
    
    Args:
        responses: —Å–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏ (k –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤)
        parser_func: —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON
    
    Returns:
        Content-level uncertainty (1 - average Jaccard similarity)
    """
    # –ü–∞—Ä—Å–∏–º –≤—Å–µ –æ—Ç–≤–µ—Ç—ã
    parsed_outputs = []
    for response in responses:
        json_part = extract_json_from_response(response)
        parsed = parser_func(json_part)
        if parsed is not None and isinstance(parsed, dict):
            parsed_outputs.append(parsed)
    
    k_prime = len(parsed_outputs)
    if k_prime < 2:
        return 1.0  # –í—ã—Å–æ–∫–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å, –µ—Å–ª–∏ –Ω–µ—Ç —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
    extracted_sets = []
    for parsed in parsed_outputs:
        extracted = extract_structured_output(parsed)
        extracted_sets.append(extracted)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–ø–∞—Ä–Ω—É—é Jaccard similarity
    jaccard_similarities = []
    for i in range(len(extracted_sets)):
        for j in range(i + 1, len(extracted_sets)):
            set_i = extracted_sets[i]
            set_j = extracted_sets[j]
            
            if len(set_i) == 0 and len(set_j) == 0:
                similarity = 1.0
            elif len(set_i) == 0 or len(set_j) == 0:
                similarity = 0.0
            else:
                intersection = len(set_i & set_j)
                union = len(set_i | set_j)
                similarity = intersection / union if union > 0 else 0.0
            
            jaccard_similarities.append(similarity)
    
    avg_jaccard = np.mean(jaccard_similarities) if jaccard_similarities else 0.0
    content_uncertainty = 1.0 - avg_jaccard
    
    return content_uncertainty


def generate_multiple_responses(
    text: str,
    generate_func: callable,
    model: Any,
    tokenizer: Any,
    max_new_tokens: int = 1024,
    k: int = 3,
    hyperparameters: Dict[str, Any] = None,
    temperature: float = 0.7,
    top_p: float = 0.95
) -> List[str]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç k –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º sampling.
    
    Args:
        text: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        generate_func: —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        model: –º–æ–¥–µ–ª—å
        tokenizer: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        max_new_tokens: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3)
        hyperparameters: –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        temperature: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è sampling (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.7)
        top_p: top_p –¥–ª—è sampling (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.95)
    
    Returns:
        –°–ø–∏—Å–æ–∫ –∏–∑ k –æ—Ç–≤–µ—Ç–æ–≤
    """
    import torch
    prompt = build_prompt3(text)
    responses = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ API –º–æ–¥–µ–ª—å—é
    is_api_model = hyperparameters and hyperparameters.get("api_model", False)
    
    for i in range(k):
        try:
            if is_api_model:
                # –î–ª—è API –º–æ–¥–µ–ª–µ–π –ø–µ—Ä–µ–¥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã sampling –Ω–∞–ø—Ä—è–º—É—é
                if "model_name" in hyperparameters:
                    # –ü–µ—Ä–µ–¥–∞–µ–º temperature –∏ top_p –¥–ª—è API –º–æ–¥–µ–ª–µ–π
                    try:
                        response = generate_func(
                            model, tokenizer, prompt, max_new_tokens, 
                            model_name=hyperparameters["model_name"],
                            temperature=temperature,
                            top_p=top_p if temperature > 0 else None
                        )
                    except TypeError:
                        # –ï—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —ç—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑ –Ω–∏—Ö
                        response = generate_func(model, tokenizer, prompt, max_new_tokens, 
                                                model_name=hyperparameters["model_name"])
                else:
                    try:
                        response = generate_func(
                            model, tokenizer, prompt, max_new_tokens,
                            temperature=temperature,
                            top_p=top_p if temperature > 0 else None
                        )
                    except TypeError:
                        # –ï—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —ç—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑ –Ω–∏—Ö
                        response = generate_func(model, tokenizer, prompt, max_new_tokens)
            else:
                # –î–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–µ –æ–±—Ä–∞—â–µ–Ω–∏–µ –∫ model.generate()
                # —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ sampling
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ chat template (–¥–ª—è Gemma 3 –∏ –ø–æ–¥–æ–±–Ω—ã—Ö)
                from transformers import Gemma3ForCausalLM
                is_gemma3 = isinstance(model, Gemma3ForCausalLM) or model.__class__.__name__ == 'Gemma3ForCausalLM'
                
                if is_gemma3 and hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template is not None:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è Gemma 3
                    messages = [[{"role": "user", "content": [{"type": "text", "text": prompt}]}]]
                    inputs_dict = tokenizer.apply_chat_template(
                        messages,
                        add_generation_prompt=True,
                        tokenize=True,
                        return_dict=True,
                        return_tensors="pt",
                    )
                    device = next(model.parameters()).device
                    inputs = {}
                    for key, value in inputs_dict.items():
                        if isinstance(value, torch.Tensor):
                            if key == "input_ids":
                                inputs[key] = value.to(device)
                            else:
                                inputs[key] = value.to(device).to(torch.bfloat16)
                        else:
                            inputs[key] = value
                    
                    generate_kwargs = {
                        "max_new_tokens": max_new_tokens,
                        "do_sample": True,
                        "temperature": temperature,
                        "top_p": top_p,
                    }
                    
                    if tokenizer.eos_token_id is not None:
                        generate_kwargs["eos_token_id"] = tokenizer.eos_token_id
                    
                    with torch.inference_mode():
                        outputs = model.generate(**inputs, **generate_kwargs)
                    
                    outputs_decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
                    full_text = outputs_decoded[0] if outputs_decoded else ""
                    
                    inputs_decoded = tokenizer.batch_decode(
                        inputs['input_ids'] if isinstance(inputs, dict) else inputs, 
                        skip_special_tokens=True
                    )
                    input_text = inputs_decoded[0] if inputs_decoded else ""
                    
                    if full_text.startswith(input_text):
                        response = full_text[len(input_text):].strip()
                    else:
                        response = full_text
                else:
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
                    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
                    
                    with torch.no_grad():
                        output_ids = model.generate(
                            input_ids,
                            max_new_tokens=max_new_tokens,
                            do_sample=True,
                            temperature=temperature,
                            top_p=top_p,
                            num_return_sequences=1,
                            pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,
                            eos_token_id=tokenizer.eos_token_id
                        )
                    
                    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
                    input_length = input_ids.shape[1]
                    generated_ids = output_ids[0][input_length:]
                    response = tokenizer.decode(generated_ids, skip_special_tokens=True)
                    
                    # –ï—Å–ª–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–∞–ª–æ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –ø—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å –æ—Ç–≤–µ—Ç
                    if not response.strip():
                        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
                        if generated_text.startswith(prompt):
                            response = generated_text[len(prompt):].strip()
                        else:
                            response = generated_text.strip()
            
            responses.append(response)
        except Exception as e:
            warnings.warn(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞ {i+1} –¥–ª—è —Ç–µ–∫—Å—Ç–∞: {e}")
            responses.append("")  # –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    
    return responses


def extract_few_shot_examples(
    unlabeled_corpus_path: str,
    labeled_dataset_path: str,
    n_examples: int,
    generate_func: callable,
    model: Any,
    tokenizer: Any,
    max_new_tokens: int = 1024,
    k: int = 3,
    hyperparameters: Dict[str, Any] = None,
    temperature: float = 0.7,
    top_p: float = 0.95,
    text_column: str = "text",
    n_clusters: int = 100,
    alpha: float = 0.33,  # –≤–µ—Å –¥–ª—è Generation Disagreement (ùí∞_d)
    beta: float = 0.33,   # –≤–µ—Å –¥–ª—è Format Uncertainty (ùí∞_f)
    gamma: float = 0.34,  # –≤–µ—Å –¥–ª—è Content Uncertainty (ùí∞_c)
    verbose: bool = False
) -> pd.DataFrame:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç few-shot –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
    Dual-Level Introspective Uncertainty.
    
    –ê–ª–≥–æ—Ä–∏—Ç–º –≤—ã—á–∏—Å–ª—è–µ—Ç —Ç—Ä–∏ —Ç–∏–ø–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏:
    1. Generation Disagreement (ùí∞_d): —Å—Ä–µ–¥–Ω–µ–µ –ø–æ–ø–∞—Ä–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –º–µ–∂–¥—É k –æ—Ç–≤–µ—Ç–∞–º–∏
    2. Format-Level Uncertainty (ùí∞_f): R_fail (parsing failure rate) + Structural Disagreement
    3. Content-Level Uncertainty (ùí∞_c): 1 - —Å—Ä–µ–¥–Ω—è—è Jaccard similarity –º–µ–∂–¥—É –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    
    –û–±—â–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å: ùí∞_total = Œ±¬∑ùí∞_d + Œ≤¬∑ùí∞_f + Œ≥¬∑ùí∞_c
    
    Args:
        unlabeled_corpus_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º –∫–æ—Ä–ø—É—Å–æ–º (Excel)
        labeled_dataset_path: –ø—É—Ç—å –∫ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É (Excel)
        n_examples: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –≤ –∏—Ç–æ–≥–æ–≤–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ (—Ç–æ–ø-N –ø–æ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏).
                   –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –≤—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (–ø–æ –æ–¥–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –∏–∑ –∫–∞–∂–¥–æ–≥–æ), –Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–æ–ø-n_examples
        generate_func: —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
        model: –º–æ–¥–µ–ª—å
        tokenizer: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        max_new_tokens: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        hyperparameters: –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        text_column: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–∞–º–∏
        n_clusters: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤ –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ)
        alpha: –≤–µ—Å –¥–ª—è Generation Disagreement (ùí∞_d)
        beta: –≤–µ—Å –¥–ª—è Format Uncertainty (ùí∞_f)
        gamma: –≤–µ—Å –¥–ª—è Content Uncertainty (ùí∞_c)
        verbose: –ø–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥
    
    Returns:
        DataFrame —Å —Ç–æ–ø-n_examples –ø—Ä–∏–º–µ—Ä–∞–º–∏, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ –æ–±—â–µ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
    """
    print(f"\n{'='*80}")
    print(f"–ò–ó–í–õ–ï–ß–ï–ù–ò–ï FEW-SHOT –ü–†–ò–ú–ï–†–û–í")
    print(f"{'='*80}\n")
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    print(f"   –ù–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å: {unlabeled_corpus_path}")
    unlabeled_df = pd.read_excel(unlabeled_corpus_path)
    print(f"   ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(unlabeled_df)} –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤")
    
    print(f"   –†–∞–∑–º–µ—á–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {labeled_dataset_path}")
    labeled_df = pd.read_excel(labeled_dataset_path)
    print(f"   ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(labeled_df)} —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤")
    
    # 2. –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
    print(f"\nüîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤...")
    filtered_df = filter_unlabeled_texts(unlabeled_df, labeled_df, text_column)
    
    if len(filtered_df) == 0:
        print("   ‚ö†Ô∏è –ù–µ—Ç –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏!")
        return pd.DataFrame()
    
    # 3. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    print(f"\nüìä –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤...")
    clustered_df = cluster_russian_texts(filtered_df, text_column, n_clusters=n_clusters)
    
    # –í—ã–±–∏—Ä–∞–µ–º –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–º–µ—Ä—É –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã)
    # n_examples –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Å–∫–æ–ª—å–∫–æ –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –≤–µ—Ä–Ω—É—Ç—å –≤ –∏—Ç–æ–≥–µ (—Ç–æ–ø-N –ø–æ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏)
    unique_clusters = clustered_df["–ö–ª–∞—Å—Ç–µ—Ä"].unique()
    selected_indices = []
    
    # –í—ã–±–∏—Ä–∞–µ–º –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–º–µ—Ä—É –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
    for cluster_id in unique_clusters:
        cluster_texts = clustered_df[clustered_df["–ö–ª–∞—Å—Ç–µ—Ä"] == cluster_id]
        if len(cluster_texts) > 0:
            selected_indices.append(cluster_texts.index[0])
    
    candidate_df = clustered_df.loc[selected_indices].reset_index(drop=True)
    print(f"   ‚úì –í—ã–±—Ä–∞–Ω–æ {len(candidate_df)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ {len(unique_clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
    print(f"   ‚ÑπÔ∏è  –í –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—É–¥–µ—Ç –≤–æ–∑–≤—Ä–∞—â–µ–Ω–æ —Ç–æ–ø-{n_examples} –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏")
    
    # 4. –û—Ü–µ–Ω–∫–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
    print(f"\nü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {k} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞...")
    results = []
    
    for idx, row in candidate_df.iterrows():
        text = row[text_column]
        print(f"   [{idx+1}/{len(candidate_df)}] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞ {row.get('–ö–ª–∞—Å—Ç–µ—Ä', -1)}...")
        if verbose:
            print(f"      –¢–µ–∫—Å—Ç: {text[:100]}...")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º k –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞ —Å sampling
        responses = generate_multiple_responses(
            text, generate_func, model, tokenizer, max_new_tokens, k, hyperparameters,
            temperature=temperature, top_p=top_p
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
        generation_disagreement = calculate_generation_disagreement(responses)
        
        R_fail, structural_disagreement = calculate_format_uncertainty(responses)
        format_uncertainty = (R_fail + structural_disagreement) / 2.0
        
        content_uncertainty = calculate_content_uncertainty(responses)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –∫ [0, 1] (–æ–Ω–∏ —É–∂–µ –≤ —ç—Ç–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ)
        # –û–±—â–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å —Å–æ–≥–ª–∞—Å–Ω–æ —Ñ–æ—Ä–º—É–ª–µ (5)
        total_uncertainty = alpha * generation_disagreement + beta * format_uncertainty + gamma * content_uncertainty
        
        results.append({
            "text": text,
            "cluster": row.get("–ö–ª–∞—Å—Ç–µ—Ä", -1),
            "generation_disagreement": generation_disagreement,
            "R_fail": R_fail,
            "structural_disagreement": structural_disagreement,
            "format_uncertainty": format_uncertainty,
            "content_uncertainty": content_uncertainty,
            "total_uncertainty": total_uncertainty,
            "responses": responses
        })
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        print(f"      ‚úì Total Uncertainty: {total_uncertainty:.3f} "
              f"(Disagreement: {generation_disagreement:.3f}, "
              f"Format: {format_uncertainty:.3f}, "
              f"Content: {content_uncertainty:.3f})")
        
        if verbose:
            print(f"      R_fail: {R_fail:.3f}, Structural disagreement: {structural_disagreement:.3f}")
    
    # 5. –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    results_df = pd.DataFrame(results)
    
    # 6. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ–±—â–µ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
    results_df = results_df.sort_values("total_uncertainty", ascending=False).reset_index(drop=True)
    
    print(f"\n‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"   –¢–æ–ø-{min(n_examples, len(results_df))} –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –Ω–∞–∏–≤—ã—Å—à–µ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å—é:\n")
    for idx, row in results_df.head(n_examples).iterrows():
        print(f"{'='*80}")
        print(f"–ü—Ä–∏–º–µ—Ä {idx+1} (Total Uncertainty: {row['total_uncertainty']:.3f})")
        print(f"  Disagreement: {row['generation_disagreement']:.3f}, "
              f"Format: {row['format_uncertainty']:.3f}, "
              f"Content: {row['content_uncertainty']:.3f}")
        print(f"{'-'*80}")
        print(f"–¢–µ–∫—Å—Ç:")
        print(f"{row['text']}")
        print(f"\n–û–¢–í–ï–¢:")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–µ—Ä–≤—ã–π —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π JSON –∏–∑ responses
        best_response = None
        responses = row.get('responses', [])
        
        for response in responses:
            if response and response.strip():
                json_part = extract_json_from_response(response)
                parsed = parse_json_safe(json_part)
                if parsed is not None and isinstance(parsed, dict):
                    best_response = json_part
                    break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π JSON, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –Ω–µ–ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç
        if best_response is None:
            for response in responses:
                if response and response.strip():
                    best_response = extract_json_from_response(response)
                    if best_response:
                        break
        
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ JSON, –≤—ã–≤–æ–¥–∏–º –µ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ markdown
        if best_response:
            print(f"```json")
            print(f"{best_response}")
            print(f"```")
        else:
            # –ï—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–π –æ—Ç–≤–µ—Ç –∫–∞–∫ –µ—Å—Ç—å
            if responses and responses[0]:
                print(responses[0])
            else:
                print("(–û—Ç–≤–µ—Ç –Ω–µ –ø–æ–ª—É—á–µ–Ω)")
        print()
    
    return results_df


if __name__ == "__main__":
    import argparse
    import sys
    from config import UNLABELED_CORPUS_PATH
    from utils import find_dataset_path, find_file_path
    import model_loaders as ml
    
    parser = argparse.ArgumentParser(
        description="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ few-shot –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ Dual-Level Introspective Uncertainty"
    )
    parser.add_argument(
        "model_key",
        type=str,
        help="–ö–ª—é—á –º–æ–¥–µ–ª–∏ –∏–∑ MODEL_CONFIGS (–Ω–∞–ø—Ä–∏–º–µ—Ä, gemma-3-4b, qwen-2.5-3b)"
    )
    parser.add_argument(
        "--n-examples",
        type=int,
        default=10,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)"
    )
    parser.add_argument(
        "--n-clusters",
        type=int,
        default=100,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 100)"
    )
    parser.add_argument(
        "--k",
        type=int,
        default=3,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 3)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.8,
        help="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è sampling (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.7)"
    )
    parser.add_argument(
        "--top-p",
        type=float,
        default=0.95,
        help="Top-p –¥–ª—è sampling (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.95)"
    )
    parser.add_argument(
        "--alpha",
        type=float,
        default=0.33,
        help="–í–µ—Å –¥–ª—è Generation Disagreement (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.33)"
    )
    parser.add_argument(
        "--beta",
        type=float,
        default=0.33,
        help="–í–µ—Å –¥–ª—è Format Uncertainty (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.33)"
    )
    parser.add_argument(
        "--gamma",
        type=float,
        default=0.34,
        help="–í–µ—Å –¥–ª—è Content Uncertainty (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.34)"
    )
    parser.add_argument(
        "--text-column",
        type=str,
        default="text",
        help="–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: text)"
    )
    parser.add_argument(
        "--max-new-tokens",
        type=int,
        default=1024,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1024)"
    )
    parser.add_argument(
        "--unlabeled-corpus",
        type=str,
        default=None,
        help="–ü—É—Ç—å –∫ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º—É –∫–æ—Ä–ø—É—Å—É (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ config.py)"
    )
    parser.add_argument(
        "--labeled-dataset",
        type=str,
        default=None,
        help="–ü—É—Ç—å –∫ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —á–µ—Ä–µ–∑ find_dataset_path())"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ CSV (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"
    )
    
    args = parser.parse_args()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –∫ –∫–æ—Ä–ø—É—Å–∞–º
    # –†–∞–∑–º–µ—á–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ—Ç –∂–µ —Ñ–∞–π–ª, —á—Ç–æ –∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π
    labeled_dataset_path = args.labeled_dataset or find_dataset_path()
    
    # –î–ª—è –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∏—Å–∫ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö
    if args.unlabeled_corpus:
        unlabeled_corpus_path = args.unlabeled_corpus
        if not os.path.exists(unlabeled_corpus_path):
            print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {unlabeled_corpus_path}")
            sys.exit(1)
    elif UNLABELED_CORPUS_PATH:
        try:
            unlabeled_corpus_path = find_file_path(UNLABELED_CORPUS_PATH)
        except FileNotFoundError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            print("   –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –æ–¥–Ω–∏–º –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤:")
            print("   1. –í config.py: UNLABELED_CORPUS_PATH = 'data/udobrenia.xlsx'")
            print("   2. –ß–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç: --unlabeled-corpus path/to/unlabeled_corpus.xlsx")
            sys.exit(1)
    else:
        print("‚ùå –û—à–∏–±–∫–∞: –ø—É—Ç—å –∫ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º—É –∫–æ—Ä–ø—É—Å—É –Ω–µ —É–∫–∞–∑–∞–Ω!")
        print("   –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –æ–¥–Ω–∏–º –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤:")
        print("   1. –í config.py: UNLABELED_CORPUS_PATH = 'data/udobrenia.xlsx'")
        print("   2. –ß–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç: --unlabeled-corpus path/to/unlabeled_corpus.xlsx")
        sys.exit(1)
    
    if not os.path.exists(labeled_dataset_path):
        print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {labeled_dataset_path}")
        sys.exit(1)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
    from main import MODEL_CONFIGS
    
    if args.model_key not in MODEL_CONFIGS:
        print(f"‚ùå –û—à–∏–±–∫–∞: –º–æ–¥–µ–ª—å '{args.model_key}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ MODEL_CONFIGS")
        print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {', '.join(MODEL_CONFIGS.keys())}")
        sys.exit(1)
    
    model_config = MODEL_CONFIGS[args.model_key]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ API –º–æ–¥–µ–ª—å—é
    is_api_model = model_config["hyperparameters"].get("api_model", False)
    
    if is_api_model:
        print("‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: API –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å sampling –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞–ø—Ä—è–º—É—é")
        print("   –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
    
    print(f"\n{'='*80}")
    print(f"–ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò: {model_config['name']}")
    print(f"{'='*80}\n")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    try:
        model, tokenizer = model_config["load_func"]()
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
    try:
        results_df = extract_few_shot_examples(
            unlabeled_corpus_path=unlabeled_corpus_path,
            labeled_dataset_path=labeled_dataset_path,
            n_examples=args.n_examples,
            generate_func=model_config["generate_func"],
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=args.max_new_tokens,
            k=args.k,
            hyperparameters=model_config["hyperparameters"],
            temperature=args.temperature,
            top_p=args.top_p,
            text_column=args.text_column,
            n_clusters=args.n_clusters,
            alpha=args.alpha,
            beta=args.beta,
            gamma=args.gamma,
            verbose=args.verbose
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å
        if args.output:
            results_df.to_csv(args.output, index=False, encoding='utf-8')
            print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {args.output}")
        else:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ results/ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            from config import OUTPUT_DIR
            from datetime import datetime
            
            os.makedirs(OUTPUT_DIR, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = os.path.join(OUTPUT_DIR, f"few_shot_examples_{args.model_key}_{timestamp}.csv")
            results_df.to_csv(output_path, index=False, encoding='utf-8')
            print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –ø—Ä–∏–º–µ—Ä–æ–≤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

