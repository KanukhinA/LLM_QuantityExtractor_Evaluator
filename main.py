"""
–ì–ª–∞–≤–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π
"""
import os
import sys
import logging
from datetime import datetime
from model_evaluator import ModelEvaluator
import model_loaders as ml
import model_loaders_api as ml_api
from gemini_analyzer import analyze_errors_with_gemini, check_gemini_api
from config import DATASET_PATH, GROUND_TRUTH_PATH, OUTPUT_DIR, GEMINI_API_KEY

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log_file = os.path.join(OUTPUT_DIR, "model_errors.log")
os.makedirs(OUTPUT_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.ERROR,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler(sys.stderr)
    ]
)


def run_evaluation(model_config: dict, use_gemini: bool = True, verbose: bool = False):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏
    
    Args:
        model_config: —Å–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –º–æ–¥–µ–ª–∏:
            - name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            - load_func: —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
            - generate_func: —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            - hyperparameters: –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å multi_agent_mode)
        use_gemini: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Gemini API
        verbose: –µ—Å–ª–∏ True, –≤—ã–≤–æ–¥–∏—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (—Ç–µ–∫—Å—Ç –∏ –æ—Ç–≤–µ—Ç—ã) –≤ –∫–æ–Ω—Å–æ–ª—å
    """
    evaluator = ModelEvaluator(
        dataset_path=DATASET_PATH,
        ground_truth_path=GROUND_TRUTH_PATH,
        output_dir=OUTPUT_DIR
    )
    
    # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    evaluator.clear_memory()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É
    # –î–ª—è API –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ –ø–æ–ø—ã—Ç–æ–∫ (10 –≤–º–µ—Å—Ç–æ 2)
    num_retries = 10 if model_config["hyperparameters"].get("api_model", False) else 2
    result = evaluator.evaluate_model(
        model_name=model_config["name"],
        load_model_func=model_config["load_func"],
        generate_func=model_config["generate_func"],
        hyperparameters=model_config["hyperparameters"],
        num_retries=num_retries,
        verbose=verbose,  # –ü–µ—Ä–µ–¥–∞–µ–º —Ñ–ª–∞–≥ verbose
        use_gemini_analysis=use_gemini,
        gemini_api_key=GEMINI_API_KEY if use_gemini else None
    )
    
    if result.get("status") == "error":
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏: {result.get('error')}")
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –≤ log —Ñ–∞–π–ª
        error_msg = f"\n{'='*80}\n"
        error_msg += f"–û–®–ò–ë–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò\n"
        error_msg += f"{'='*80}\n"
        error_msg += f"–í—Ä–µ–º—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        error_msg += f"–ú–æ–¥–µ–ª—å: {model_config['name']}\n"
        error_msg += f"–û—à–∏–±–∫–∞: {result.get('error')}\n"
        error_msg += f"\n–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {model_config.get('hyperparameters', {})}\n"
        error_msg += f"\n–ü–æ–ª–Ω—ã–π traceback:\n{result.get('error_traceback', '–ù–µ —É–∫–∞–∑–∞–Ω')}\n"
        error_msg += f"{'='*80}\n"
        
        logging.error(error_msg)
        print(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∞–Ω–∞ –≤ log —Ñ–∞–π–ª: {log_file}")
        return result
    
    # –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Gemini (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
    if use_gemini:
        print(f"\n{'='*80}")
        print(f"–ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö –ß–ï–†–ï–ó GEMINI API")
        print(f"{'='*80}")
        
        parsing_errors = result.get("parsing_errors", [])
        quality_metrics = result.get("quality_metrics", {})
        hyperparameters = result.get("hyperparameters", {})
        
        print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:")
        print(f"   ‚Ä¢ –û—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞: {len(parsing_errors)}")
        if quality_metrics:
            mass_errors = len(quality_metrics.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {}).get('–æ—à–∏–±–∫–∏', []))
            prochee_errors = len(quality_metrics.get('–ø—Ä–æ—á–µ–µ', {}).get('–æ—à–∏–±–∫–∏', []))
            print(f"   ‚Ä¢ –û—à–∏–±–æ–∫ –∫–∞—á–µ—Å—Ç–≤–∞ '–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è': {mass_errors}")
            print(f"   ‚Ä¢ –û—à–∏–±–æ–∫ –∫–∞—á–µ—Å—Ç–≤–∞ '–ø—Ä–æ—á–µ–µ': {prochee_errors}")
        print(f"   ‚Ä¢ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {len(hyperparameters)} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        print()
        
        # –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Gemini —Ç–µ–ø–µ—Ä—å –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ evaluate_model
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –≤ JSON, –µ—Å–ª–∏ –æ–Ω –±—ã–ª –≤—ã–ø–æ–ª–Ω–µ–Ω
        gemini_analysis = result.get("gemini_analysis")
        if gemini_analysis and gemini_analysis.get("status") == "success":
            timestamp = result.get("timestamp", datetime.now().strftime("%Y%m%d_%H%M%S"))
            model_name_safe = model_config["name"].replace("/", "_").replace("\\", "_")
            analysis_path = os.path.join(OUTPUT_DIR, f"gemini_analysis_{model_name_safe}_{timestamp}.json")
            
            analysis_text = gemini_analysis.get("analysis", "")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ—Ü–µ–Ω–∫–∏
            hyperparameters = result.get("hyperparameters", {})
            gpu_info = result.get("gpu_info", {})
            average_response_time = result.get("average_response_time_seconds", 0)
            gpu_memory_during_inference = result.get("gpu_memory_during_inference_gb", 0)
            api_model = result.get("api_model", False)
            multi_agent_mode = result.get("multi_agent_mode")
            
            analysis_data = {
                "model_name": model_config["name"],
                "timestamp": timestamp,
                "analysis": analysis_text,
                "model_used": gemini_analysis.get("model_used", "gemini-2.5-flash"),
                "parsing_errors_count": len(parsing_errors),
                "hyperparameters": hyperparameters,
                "system_info": {
                    "api_model": api_model,
                    "multi_agent_mode": multi_agent_mode,
                    "gpu_info": gpu_info,
                    "gpu_memory_during_inference_gb": gpu_memory_during_inference,
                    "average_response_time_seconds": average_response_time
                },
                "quality_metrics_summary": {
                    "–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è": {
                        "accuracy": quality_metrics.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {}).get('—Å—Ä–µ–¥–Ω—è—è_—Ç–æ—á–Ω–æ—Å—Ç—å', 0) if quality_metrics else 0,
                        "precision": quality_metrics.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {}).get('precision', 0) if quality_metrics else 0,
                        "recall": quality_metrics.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {}).get('recall', 0) if quality_metrics else 0,
                        "f1": quality_metrics.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {}).get('f1', 0) if quality_metrics else 0
                    },
                    "–ø—Ä–æ—á–µ–µ": {
                        "accuracy": quality_metrics.get('–ø—Ä–æ—á–µ–µ', {}).get('—Å—Ä–µ–¥–Ω—è—è_—Ç–æ—á–Ω–æ—Å—Ç—å', 0) if quality_metrics else 0,
                        "precision": quality_metrics.get('–ø—Ä–æ—á–µ–µ', {}).get('precision', 0) if quality_metrics else 0,
                        "recall": quality_metrics.get('–ø—Ä–æ—á–µ–µ', {}).get('recall', 0) if quality_metrics else 0,
                        "f1": quality_metrics.get('–ø—Ä–æ—á–µ–µ', {}).get('f1', 0) if quality_metrics else 0
                    }
                } if quality_metrics else None
            }
            
            import json
            with open(analysis_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2)
            print(f"üíæ –ê–Ω–∞–ª–∏–∑ Gemini —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ JSON: {analysis_path}\n")
    else:
        if not use_gemini:
            print(f"\n–ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Gemini API –ø—Ä–æ–ø—É—â–µ–Ω (–æ—Ç–∫–ª—é—á–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º)\n")
    
    return result


# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
MODEL_CONFIGS = {
    "gemma-2-2b": {
        "name": "google/gemma-2-2b-it",
        "load_func": ml.load_gemma_2_2b,
        "generate_func": ml.generate_standard,
        "hyperparameters": {
            "max_new_tokens": 512,
            "do_sample": False,
            "torch_dtype": "bfloat16"
        }
    },
    "qwen-2.5-1.5b": {
        "name": "Qwen/Qwen2.5-1.5B-Instruct",
        "load_func": ml.load_qwen_2_5_1_5b,
        "generate_func": ml.generate_qwen,
        "hyperparameters": {
            "max_new_tokens": 512,
            "do_sample": False,
            "torch_dtype": "bfloat16"
        }
    },
    "qwen-2.5-3b": {
        "name": "Qwen/Qwen2.5-3B-Instruct",
        "load_func": ml.load_qwen_2_5_3b,
        "generate_func": ml.generate_qwen,
        "hyperparameters": {
            "max_new_tokens": 1024,
            "do_sample": False,
            "dtype": "bfloat16"
        }
    },
    "qwen-2.5-4b": {
        "name": "Qwen/Qwen2.5-4B-Instruct",
        "load_func": ml.load_qwen_2_5_4b,
        "generate_func": ml.generate_qwen,
        "hyperparameters": {
            "max_new_tokens": 512,
            "do_sample": False,
            "dtype": "bfloat16"
        }
    },
    "gemma-3-1b": {
        "name": "google/gemma-3-1b-it",
        "load_func": ml.load_gemma_3_1b,
        "generate_func": ml.generate_gemma,
        "hyperparameters": {
            "max_new_tokens": 512,
            "do_sample": False,
            "torch_dtype": "bfloat16"
        }
    },
    "gemma-3-4b": {
        "name": "google/gemma-3-4b-it",
        "load_func": ml.load_gemma_3_4b,
        "generate_func": ml.generate_gemma,
        "hyperparameters": {
            "max_new_tokens": 512,
            "do_sample": False,
            "torch_dtype": "bfloat16"
        }
    },
    "gemma-3-4b-api": {
        "name": "gemma-3-4b-it",
        "load_func": ml_api.load_gemma_3_4b_api,
        "generate_func": ml_api.generate_gemma_api,
        "hyperparameters": {
            "max_new_tokens": 512,
            "model_name": "gemma-3-4b-it",
            "api_model": True
        }
    },
    "gemma-3-12b-api": {
        "name": "gemma-3-12b-it",
        "load_func": ml_api.load_gemma_3_12b_api,
        "generate_func": ml_api.generate_gemma_api,
        "hyperparameters": {
            "max_new_tokens": 512,
            "model_name": "gemma-3-12b-it",
            "api_model": True
        }
    },
    "gemma-3-27b-api": {
        "name": "gemma-3-27b-it",
        "load_func": ml_api.load_gemma_3_27b_api,
        "generate_func": ml_api.generate_gemma_api,
        "hyperparameters": {
            "max_new_tokens": 512,
            "model_name": "gemma-3-27b-it",
            "api_model": True
        }
    },
    "Ministral-3-3B-Reasoning-2512": {
        "name": "mistralai/Ministral-3-3B-Reasoning-2512",
        "load_func": ml.load_ministral_3_3b_reasoning_2512,
        "generate_func": ml.generate_standard,
        "hyperparameters": {
            "max_new_tokens": 512,
            "do_sample": False,
            "torch_dtype": "bfloat16"
        }
    },
    "CHEMLLM-2b-1_5": {
        "name": "AI4Chem/CHEMLLM-2b-1_5",
        "load_func": ml.load_chemllm_2b_1_5,
        "generate_func": ml.generate_standard,
        "hyperparameters": {
            "max_new_tokens": 1024,
            "do_sample": False,
            "torch_dtype": "bfloat16"
        }
    },
    "Phi-3.5-mini-instruct": {
        "name": "microsoft/Phi-3.5-mini-instruct",
        "load_func": ml.load_phi_3_5_mini_instruct,
        "generate_func": ml.generate_phi_3_5,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è Phi-3.5
        "hyperparameters": {
            "max_new_tokens": 1024,
            "do_sample": False,
            "torch_dtype": "bfloat16"
        }
    },
    "phi-4-mini-instruct": {
        "name": "microsoft/Phi-4-mini-instruct",
        "load_func": ml.load_phi_4_mini_instruct,
        "generate_func": ml.generate_standard,
        "hyperparameters": {
            "max_new_tokens": 1024,
            "do_sample": False,
            "dtype": "bfloat16"
        }
    },
    "mistral-7b-v0.3-bnb-4bit": {
        "name": "unsloth/mistral-7b-v0.3-bnb-4bit",
        "load_func": ml.load_mistral_7b_v0_3_bnb_4bit,
        "generate_func": ml.generate_standard,
        "hyperparameters": {
            "max_new_tokens": 1024,
            "do_sample": False,
            "quantization": "4-bit (pre-quantized)"
        }
    }
}


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å Gemini API –≤ —Å–∞–º–æ–º –Ω–∞—á–∞–ª–µ
    print(f"\n{'='*80}")
    print(f"–ü–†–û–í–ï–†–ö–ê –°–ò–°–¢–ï–ú–´")
    print(f"{'='*80}")
    # GEMINI_API_KEY –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ config.py (–∫–æ—Ç–æ—Ä—ã–π –±–µ—Ä–µ—Ç –µ–≥–æ –∏–∑ config_secrets.py –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è)
    
    if GEMINI_API_KEY:
        print(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ Gemini API...")
        gemini_working, gemini_message = check_gemini_api(GEMINI_API_KEY)
        print(f"   {gemini_message}\n")
    else:
        print(f"GEMINI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É API")
        gemini_working = False
        print()
    
    use_gemini = True
    if not gemini_working:
        print(f"{'='*80}")
        print(f"–í–ù–ò–ú–ê–ù–ò–ï: Gemini API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        print(f"{'='*80}")
        print(f"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞, –Ω–æ –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ —á–µ—Ä–µ–∑ Gemini –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω.")
        print(f"–í—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –æ—à–∏–±–æ–∫ –∏–ª–∏ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∑–∞–Ω–æ–≤–æ.\n")
        
        while True:
            response = input("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –æ—à–∏–±–æ–∫? (y/n): ").strip().lower()
            if response in ['y', 'yes', '–¥–∞', '–¥']:
                use_gemini = False
                print("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –æ—à–∏–±–æ–∫...\n")
                break
            elif response in ['n', 'no', '–Ω–µ—Ç', '–Ω']:
                print("–ó–∞–ø—É—Å–∫ –æ—Ç–º–µ–Ω—ë–Ω. –ò—Å–ø—Ä–∞–≤—å—Ç–µ –ø—Ä–æ–±–ª–µ–º—É —Å Gemini API –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
                return
            else:
                print("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ 'y' (–¥–∞) –∏–ª–∏ 'n' (–Ω–µ—Ç)")
    
    # –¢–µ–ø–µ—Ä—å –ø—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python main.py <model_name> [--multi-agent MODE]")
        print("\n–ê—Ä–≥—É–º–µ–Ω—Ç—ã:")
        print("  <model_name>     - –∫–ª—é—á –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
        print("  --multi-agent     - (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Ä–µ–∂–∏–º –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞")
        print("                      –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ–∂–∏–º—ã: simple_4agents")
        print("\n–ü—Ä–∏–º–µ—Ä—ã:")
        print("  python main.py qwen-2.5-3b")
        print("  python main.py qwen-2.5-3b --multi-agent simple_4agents")
        print("\n–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
        for key in MODEL_CONFIGS.keys():
            print(f"  - {key}")
        return
    
    model_key = sys.argv[1]
    
    if model_key not in MODEL_CONFIGS:
        print(f"–ú–æ–¥–µ–ª—å '{model_key}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:", ", ".join(MODEL_CONFIGS.keys()))
        return
    
    # –ü–∞—Ä—Å–∏–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
    multi_agent_mode = None
    if len(sys.argv) > 2:
        if "--multi-agent" in sys.argv:
            idx = sys.argv.index("--multi-agent")
            if idx + 1 < len(sys.argv):
                multi_agent_mode = sys.argv[idx + 1]
            else:
                print("–û—à–∏–±–∫–∞: –ø–æ—Å–ª–µ --multi-agent –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–∫–∞–∑–∞–Ω —Ä–µ–∂–∏–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, simple_4agents)")
                return
        else:
            print(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç: {sys.argv[2]}")
            print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python main.py <model_name> [--multi-agent MODE]")
            return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    if not os.path.exists(DATASET_PATH):
        print(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {DATASET_PATH}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª results_var3.xlsx –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø–∞–ø–∫–µ data/")
        return
    
    print(f"\n{'='*80}")
    print(f"–ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ò")
    print(f"{'='*80}")
    print(f"üìå –ú–æ–¥–µ–ª—å: {model_key}")
    print(f"üìå –ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ: {MODEL_CONFIGS[model_key]['name']}")
    if multi_agent_mode:
        print(f"üìå –†–µ–∂–∏–º: –ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–π ({multi_agent_mode})")
    else:
        print(f"üìå –†–µ–∂–∏–º: –û–¥–Ω–æ–∞–≥–µ–Ω—Ç–Ω—ã–π")
    print(f"üìÅ –î–∞—Ç–∞—Å–µ—Ç: {DATASET_PATH}")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {OUTPUT_DIR}")
    print(f"üìÖ –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}\n")
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º multi_agent_mode –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    import copy
    config = copy.deepcopy(MODEL_CONFIGS[model_key])
    if multi_agent_mode:
        config["hyperparameters"]["multi_agent_mode"] = multi_agent_mode
    
    result = run_evaluation(config, use_gemini=use_gemini, verbose=True)  # –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è main.py
    
    if result.get("status") != "error":
        print(f"\n{'='*80}")
        print(f"üéâ –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê")
        print(f"{'='*80}")
        print(f"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ '{model_key}' –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"\n–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print(f"   ‚Ä¢ –ú–æ–¥–µ–ª—å: {result.get('model_name', 'N/A')}")
        print(f"   ‚Ä¢ –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {result.get('average_response_time_seconds', 0) * result.get('total_samples', 0) / 60:.2f} –º–∏–Ω—É—Ç")
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {result.get('average_response_time_seconds', 0):.3f} —Å–µ–∫/–æ—Ç–≤–µ—Ç")
        print(f"   ‚Ä¢ –û—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞: {result.get('parsing_error_rate', 0):.2%} ({result.get('invalid_json_count', 0)}/{result.get('total_samples', 0)})")
        print(f"   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {result.get('gpu_memory_during_inference_gb', 0):.2f} GB")
        
        quality = result.get('quality_metrics')
        if quality:
            print(f"\nüéØ –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:")
            mass = quality.get('–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è', {})
            prochee = quality.get('–ø—Ä–æ—á–µ–µ', {})
            print(f"   ‚Ä¢ '–º–∞—Å—Å–æ–≤–∞—è –¥–æ–ª—è':")
            print(f"     - Accuracy: {mass.get('—Å—Ä–µ–¥–Ω—è—è_—Ç–æ—á–Ω–æ—Å—Ç—å', 0):.2%}")
            print(f"     - Precision: {mass.get('precision', 0):.2%}, Recall: {mass.get('recall', 0):.2%}, F1: {mass.get('f1', 0):.2%}")
            print(f"   ‚Ä¢ '–ø—Ä–æ—á–µ–µ':")
            print(f"     - Accuracy: {prochee.get('—Å—Ä–µ–¥–Ω—è—è_—Ç–æ—á–Ω–æ—Å—Ç—å', 0):.2%}")
            print(f"     - Precision: {prochee.get('precision', 0):.2%}, Recall: {prochee.get('recall', 0):.2%}, F1: {prochee.get('f1', 0):.2%}")
        
        print(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {OUTPUT_DIR}")
        print(f"{'='*80}\n")


if __name__ == "__main__":
    main()

